---
title: 浅谈 boost、Adaboost 和 提升树
date: 2017-10-15 11:31:35
categories: ML
tags:
     - Ensemble
     - Boost
     - GBDT/GBRT
description: 简述从 boost 到 Adaboost 和提升树的一些关系和自己的见解。
---

# 三个公式

1. boost = 加法模型 + 前向分布算法
2. Adaboost = boost + 损失函数是指数函数（基函数任意）
3. 提升树 = boost + 基函数是决策树（损失函数任意）

由此可以看出：

1. boost 是一种算法框架，该框架是由加法模型和前向分布算法组成的。
2. Adaboost 和提升树都是 boost 算法的一个特殊情况，分别限定了 boost 算法的某一部分而得到。

# 加法模型

前向分布算法：每次学习一个基函数的参数作为下一个基函数。

下一个基函数的选择标准：在已经学习到的加法模型中，选择一个使损失函数最小的基函数。

# 常见损失函数

指数损失函数：决定了 Adaboost 必须进行加权取样（权重由错误率决定），最终模型也是加权累计。

平方误差损失函数：决定了 BRT 的下一个模型应该学习前一个模型的残差。

一般损失函数：决定了 GBDT/GBRT 的下一个模型应该学习一个模型的梯度（残差近似）。

# 各种提升树

BDT：二叉分类树 + 指数损失（加权学习）。（其实就是 Adaboost 要求基函数是二叉分类树）。

BRT：二叉回归树 + 平方误差损失（残差）。

GBDT：二叉分类树 + 普通损失函数（计算负梯度，近似残差）。

GBRT：二叉回归树 + 普通损失函数（计算负梯度，近似残差）。

其中，如果把 GBRT 的损失函数选为平方误差损失，则退化为 BRT，因为平方损失函数的负梯度就是残差。