<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[一个简单的垃圾回收代码]]></title>
    <url>%2F2021%2F07%2F20%2Fbaby-gc%2F</url>
    <content type="text"><![CDATA[再利用垃圾回收（Garbage Collection，GC）的基本思想是，编程语言通过底层的操作让你感觉内存是无穷无尽的。但我们都知道内存是有限的，所以要达到这一点就需要将不再被使用的内存（至少看起来是这样）回收成未分配状态。在此之中，最重要的就是安全地辨认所谓的“不再被使用”，因为如果你的程序能有机会访问到被抹除为随机值的内存块，那将是很危险的事情。 为了识别能否回收，最简单的思路就是判断一个内存块是否还被至少一个引用指向它。基于此观点，“仍被使用”就可以简单地被认为是： 正在被某变量引用的对象是“仍被使用”的。 被其它对象引用的对象是“仍被使用”的。 当然，第二条意味着它是一个递归，即，如果某变量指向对象 A ，且对象 A 引用了对象 B，那么 B 也是“被使用的”，因为 B 可以从 A 那里被找到。 标记-清除（Marking and Sweeping）垃圾回收有很多算法，其中最简单的就是标记-清除算法，该算法由 John McCarthy 发明（此人也发明了 Lisp ）。它的细节是这样的： 从根节点开始，遍历整个对象依赖图。对每一个能到达的对象，设定一个标记 bit 为 true。 遍历后，删除所有未被标记的对象。 可能会有人说准确的垃圾回收需要更复杂的算法，比如至少要分代回收等，不过作为一个示例我想仅仅从最简单的情况开始。 对象对儿我不想为这个垃圾回收示例写一个复杂的虚拟机（Virtual Machine，VM），所以我们采用一个不碰触语法解析、字节码等这些麻烦事情的方法，定义一个仅含两种类型对象的枚举： 1234typedef enum &#123; OBJ_INT, OBJ_PAIR&#125; ObjectType; 其中，这个 OBJ_PAIR 可以包含一对儿任意类型，两个 INT ，一个 INT 一个 PAIR ，或者两个 PAIR 都可以。 细心的朋友们（或接触过函数式编程朋友们）可以发现， PAIR 类型其实是一个可以被定制成很多东西的类型，比如 LIST 类型。事实上，一个 LIST 就可以看做是一个递归 PAIR： 12LIST = (LIST 首项, LIST 剩余项)LIST 剩余项 = (LIST 剩余项首项, LIST 剩余项的剩余项) 等等。所以能够处理 PAIR 类型其实就意味着你有能力处理很多其它类型。 根据这两种类型，我们定义一个结构体，让两种类型共享一个值，从而达到模拟 VM 中一个变量只能对类型进行二选一的特性。 1234567891011121314typedef struct sObject &#123; ObjectType type; union &#123; /* OBJ_INT */ int value; /* OBJ_PAIR */ struct &#123; struct sObject* head; struct sObject* tail; &#125;; &#125;;&#125; Object; 一个小型 VM有了定义对象的方法，我们就可以做一个小型 VM 了，这个 VM 只包含一个有限长的对象栈。 123456#define STACK_MAX 256typedef struct &#123; Object* stack[STACK_MAX]; int stackSize;&#125; VM; 紧接着，我们需要定义 new 函数、push 和 pop 函数，这些都是基本操作。 123456789101112131415VM* newVM() &#123; VM* vm = malloc(sizeof(VM)); vm-&gt;stackSize = 0; return vm;&#125;void push(VM* vm, Object* value) &#123; assert(vm-&gt;stackSize &lt; STACK_MAX, "Stack overflow!"); vm-&gt;stack[vm-&gt;stackSize++] = value;&#125;Object* pop(VM* vm) &#123; assert(vm-&gt;stackSize &gt; 0, "Stack underflow!"); return vm-&gt;stack[--vm-&gt;stackSize];&#125; 定义如何在 VM 里分配空间给一个对象（“变量”）。 12345Object* newObject(VM* vm, ObjectType type) &#123; Object* object = malloc(sizeof(Object)); object-&gt;type = type; return object;&#125; 注意到 PAIR 类型是一种复合对象，所以我们采用比较栈风格的方法来创造 PAIR 对象，即 PAIR 对象是由弹出栈顶两个对象再怼进来一个新 PAIR 对象做的。 1234567891011121314void pushInt(VM* vm, int intValue) &#123; Object* object = newObject(vm, OBJ_INT); object-&gt;value = intValue; push(vm, object);&#125;Object* pushPair(VM* vm) &#123; Object* object = newObject(vm, OBJ_PAIR); object-&gt;tail = pop(vm); object-&gt;head = pop(vm); push(vm, object); return object;&#125; 到此为止，VM 就大功告成了。 标记为了标记，我们需要在对象中放置一个标志位。 1234typedef struct sObject &#123; unsigned char marked; /* Previous stuff... */&#125; Object; 而刚刚所说的标记-清理的第一步，就是遍历 VM 中栈的所有对象并进行标记。注意，PAIR 是个复合对象，如果一个 PAIR 对象被标记了，其引用的对象也应该被标记。 123456789101112131415void mark(Object* object) &#123; object-&gt;marked = 1; if (object-&gt;type == OBJ_PAIR) &#123; mark(object-&gt;head); mark(object-&gt;tail); &#125;&#125;void markAll(VM* vm)&#123; for (int i = 0; i &lt; vm-&gt;stackSize; i++) &#123; mark(vm-&gt;stack[i]); &#125;&#125; 但是这个代码有个问题，如果对象的引用图上有圈，那么就会造成死循环，所以我们改一下 mark 函数为： 123456789101112void mark(Object* object) &#123; /* If already marked, we're done. Check this first to avoid recursing on cycles in the object graph. */ if (object-&gt;marked) return; object-&gt;marked = 1; if (object-&gt;type == OBJ_PAIR) &#123; mark(object-&gt;head); mark(object-&gt;tail); &#125;&#125; 清理标记-清理的第二步就是清理，这里一上来就有一个问题，即按照定义，未标记的对象应该是不可达的，那既然是不可达的我们也没法知道它在哪儿，从而无法回收它。 所以在这里我们就需要 C 语言里最强也是最恶心的东西了——指针。 我们用一个小技巧就是在每一个对象中记录下一个被分配的对象的指针，同时在 VM 里记住第一个被分配的对象是谁： 12345678910111213typedef struct sObject &#123; /* The next object in the list of all objects. */ struct sObject* next; /* Previous stuff... */&#125; Object;typedef struct &#123; /* The first object in the list of all objects. */ Object* firstObject; /* Previous stuff... */&#125; VM; 同时我们稍稍修改 VM 的 new 函数，初始化时让 firstObject 指向 NULL。 1234567891011Object* newObject(VM* vm, ObjectType type) &#123; Object* object = malloc(sizeof(Object)); object-&gt;type = type; object-&gt;marked = 0; /* Insert it into the list of allocated objects. */ object-&gt;next = vm-&gt;firstObject; vm-&gt;firstObject = object; return object;&#125; 这样做我们就可以用指针记住要被删除的对象，还记得 Linux 内核里链表结构中指针的指针这一技巧吗？ 12345678910111213141516171819void sweep(VM* vm)&#123; Object** object = &amp;vm-&gt;firstObject; while (*object) &#123; if (!(*object)-&gt;marked) &#123; /* This object wasn't reached, so remove it from the list and free it. */ Object* unreached = *object; *object = unreached-&gt;next; free(unreached); &#125; else &#123; /* This object was reached, so unmark it (for the next GC) and move on to the next. */ (*object)-&gt;marked = 0; object = &amp;(*object)-&gt;next; &#125; &#125;&#125; 现在我们可以完成垃圾回收了，先标记再清除即可： 1234void gc(VM* vm) &#123; markAll(vm); sweep(vm);&#125; 更多的问题怎么判断 VM 低内存呢？如果计算垃圾回收的时点？ 这些问题没有特别固定的解决办法，在我的例子里，添加一个对对象的计数并给定一个垃圾回收底线即可： 1234567891011121314151617181920212223242526272829303132333435363738394041typedef struct &#123; /* The total number of currently allocated objects. */ int numObjects; /* The number of objects required to trigger a GC. */ int maxObjects; /* Previous stuff... */&#125; VM;--------------------VM* newVM() &#123; /* Previous stuff... */ vm-&gt;numObjects = 0; vm-&gt;maxObjects = INITIAL_GC_THRESHOLD; return vm;&#125;--------------------Object* newObject(VM* vm, ObjectType type) &#123; if (vm-&gt;numObjects == vm-&gt;maxObjects) gc(vm); /* Create object... */ vm-&gt;numObjects++; return object;&#125;--------------------void gc(VM* vm) &#123; int numObjects = vm-&gt;numObjects; markAll(vm); sweep(vm); vm-&gt;maxObjects = vm-&gt;numObjects * 2;&#125; 简单地说，我用一个不断扩大成 2 倍的数字来记录，比如：第一次有 8 个对象就垃圾回收；第二次的话，我就在判断“系统中很轻易的就能达到 8 个对象”的前提下，16 个对象再做回收。以此类推。 最后这样就 OK 了，我们完成了一个简单的垃圾回收模块。它很简单，但是足以说明问题。 全部的代码请参考我的 GitHub：SakigamiYang/baby-gc]]></content>
      <categories>
        <category>Develop</category>
      </categories>
      <tags>
        <tag>GC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[变与不变]]></title>
    <url>%2F2020%2F01%2F12%2Falterable-vs-permanent%2F</url>
    <content type="text"><![CDATA[序有人说：“世界上唯一不变的，就是变化本身。”我敢说这个人不是一个现代哲学家，至少他不是数学家。没有一种相对的概念，何谈另一种相对概念呢？不过是玩一点文字游戏，给人一碗励志鸡汤，让自己的书大卖罢了。 我自己对于变和不变有一些自己的理解，于32岁时记于本文之中，只希望10年后心境有所变化时，回忆起此文多些笑料罢了。 变化的市场 不变的本欲市场的变是周期性的变。 周期性变化告诉我们，世界上有许多种不同的确定性。四季交替、生老病死、服饰搭配、物价高低、经济顺逆都呈现出一定的周期性变化。对于周期性变化，我们只能做出推测，而很难预料出现的时间。 然而我们该如何把握这种周期性变化并做到不被时代的列车抛弃呢？答案就是：掌握人性。 巴菲特曾说过：“在别人贪婪时要恐惧，在别人恐惧时要贪婪。”为什么一位投资大师会使用对人类情感的剖析来告诫他人，而不是单纯的说“在别人抛售时要购买，在别人购买时要抛售”这种蠢话呢？（当然了，大师也不会说出这样的蠢话。）我们猜想其原因就是，市场是随人而动的，人的本欲是内因，不断试图满足自己的本欲并同时满足别人的本欲，就成为了推动市场发展的重要因素。 因此我们总结，在市场上，很多东西都在变，但至少有两件事是不变的： 第一，人性不变。 第二，客户受益，自己才能受益。 显然，第一点是第二点的基础，所以首先我们要了解什么是人性。 人性是人的欲望、贪婪、嫉妒、执着、恐惧……用佛教的话说，就是贪嗔痴慢疑。 当你了解了人性，就了解了人们内心中最柔软的那一部分，然后依据此设计出他们“不要不行”的产品、服务和定位。人们想要的东西其实比你想象的要简单的多，并不是一些“我希望有一个机器人能代替我做完一切”这种不切实际的愿望。大多数时候，他们只是希望“我就要好看的”、“我只是不想一直站着”、“我就不想按两下”、“我的一定要比别人的好”、“别人都拥有它我不能没有”这样简单粗暴又直接的东西。 做产品，是做一种瘾品，一种能上瘾的东西，而不是合乎逻辑的东西。 根据这两个不变，我们就该站在用户角度思考商业问题、站在人性的角度做出判断，而不是迷恋于各种“自嗨”，想出很多刁钻的高级需求，却创造不出能一把掐住人性的弱点的东西。 变化的方式 不变的能力方式的变是直线性的变。 我们总在谈一个观点，那就是人要不断成长，不断提升自己的能力，顺应时代改变不同的方式。这令人很迷茫，难道我要一直学习到老死？我一个人的力量又不可能赶得上世界整体的发展，我该怎么做？因此我们在寻找，对于一个人来讲，有没有什么是不用变的呢。 这就要提到一个概念：可迁移能力。 可迁移能力，是指不管从哪个岗位转到哪个岗位，从哪个行业跨到哪个行业，那些依然能被不断重复使用、能无障碍迁移的能力。它包括三个层次： 底层可迁移能力：思考能力（包括逻辑思维、本质思考力、升维思考力、结构化思考力、系统思考力、批判性思维、元认知等）。 中层可迁移能力：其它非技能能力（学习力、理解力、沟通力、领导力等）。 上层可迁移能力：技能（写作技能、外语听说读写技能、数据分析技能、计算机使用技能等）。 当今社会不存在“铁饭碗”，而我们可以培养铁一般的可迁移能力，让它成为你的“金饭碗”。掌握了可迁移能力，才能让你真正拥有了一些“不变”的东西，也就拥有了足够多、足够广泛的职业选择。 也许大家注意到了，我没有提到“人脉”。所谓人脉，有时候脆弱的很，古语“人以类聚，物以群分”，当你自身能力不足时，你结交的人脉也必定能力平平。所以你需要的，并不是人脉，并不是平台，而是随时随地能帮你长出人脉的能力。这种追随你能力而来的人脉必定也是与你相配的人脉，是能给予你辅助的人脉。 变化的心境 不变的初心心境的变是拉锯式的变。 时光荏苒，春秋更易。致于学时，我们突然发现，世界充满了复杂的问题，前景似乎希望渺茫；而立之年，我们的身上又同时承担了太多的重担；知天命之年，我们又不堪回首，羡慕起新生代。 然而在这不可阻挡的变中，我们偶尔又能体会到许多不变。多年未见的朋友，变的是人情世故，不变的是亲近的情感和回忆；技术和商业，变的是内容和手段，不变的是服务大众的精神和发展生活的动力；社会现象、变的是大众舆论，不变的是以解决问题为目的，让社会变得更好的心情。 由此，我们有时候会忽然意识到，自己最初的想法到底是什么呢？是喜欢一件物品？拥有一个梦想？爱上一个人？面对这些问题，我们何尝不会萌生出在这个身不由己的社会中做回自己的冲动呢？这种冲动让我们的心灵在这俗世中不断受到一种拉扯——对未来的希望和对过去的惋惜这二者的不断拉扯。 要从这种痛苦中解脱出来，我们需要四个字：勿忘初心。勿忘初心，并不是一个虚幻的概念，而是一个原则。是一个能使人不被大众世俗的三观所绑架，从而摒弃利益的诱惑，踏下心来追求自己原初梦想的原则。有了这个不变的原则，我们才能在心境的变化过程中一直坚守自己，只有有了坚守自己的前提，才有余闲让自己不出格地寻找发展和进步。 一位前辈曾经告诫我，人总是要回到自己第一次萌发出“想做某事”的那件事情上去。现在的我也想把这句话讲给别人。这并非是什么“情怀”这种说起来很好听的东西，而是人都有追求自我的本能、都有对束缚的抗拒、都有对自由的向往。 跋我又想到那句在一开篇就提到的话：“世界上唯一不变的，就是变化本身。”老子也说：“道可道，非常道。名可名，非常名。”但那已经是几千年前的哲学了，我对这种形而上的哲学也已经不以为然很久了。 在我看来，变是连续的、整体的、长远发展的，不变是离散的、原子的、短期持续的。我们不应提倡过分的追求不变的部分或变的部分，也不能因世界的瞬息万变就随波逐流而失去了属于自己不变的本心。当我们既做到了在陌生的变化中恪守属于自己的底线，又能同时在变化中选择正确的道路不断提升自我时，我们就理解到这样一个道理—— 一直在变化的是在各微小阶段中百试不爽的“术”，永恒不变的是窥探、连接万物本质的“道”。 理解了这个道理，便能达“以不变应万变，万变不离其宗”。]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>Tips</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[提出好问题的 5 个步骤]]></title>
    <url>%2F2019%2F09%2F10%2F5-steps-for-asking-good-questions%2F</url>
    <content type="text"><![CDATA[学会提出好的问题与提高自主学习的能力息息相关。而在此之前，我们要明白一件事情，那就是在多数情况下，期待被提问者能够自主明白提问者的想法是非常不切实际的幻想。因此提问的方式需要提问者依靠自身的认知进行思考并改良。 以下就有 5 个步骤帮助你审视自己即将要提出的问题。（每个步骤均含有一些指导性问题可以帮助提问者考虑如何提出好问题。） 重点：我想知道什么特定信息？我丢失了什么信息？这个问题能比只停留在回答“是或否”的问题得到更多信息吗？此问题会将我引导至更深的知识领域吗？ 目的：我为什么要问这个？我想收集事实还是见解？我需要做简单的解释吗？我想提供一个不同的观点吗？ 意图：我希望让别人如何回答我？我是否希望答案同样对他人有所帮助？我问这个问题是希望开始一场辩论还是一次讨论？问题是不是肤浅的，而不是真的有用或重要吗？我是出于沮丧还是出于好奇心才问这个问题的？我真的关心答案是什么吗？我是否愿意对询问的对象表示尊敬或尊重？ 表达：我是否使用了易于理解的术语和措辞？我的问题是中立的，还是包含见解或偏见？我的问题是否太长或太短？我的问题是否包含了我想知道的重点？我的问题是否一次只专注一件事？我的问题是否与其它毫无关系的提问有所混淆？ 补充：我还有更具体的问题要补充吗？如果有需要，我所询问的人是否能解决我的其它问题？如果我依然没有得到我需要的答案，接下来该怎么做？如果我仍然不明白，接下来该怎么做？ 最后，记得不要打断对方，记得尊重对方的时间、意愿和思想。]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>Tips</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论判别模型和生成模型]]></title>
    <url>%2F2018%2F02%2F04%2Fdiscriminative-or-generative-model%2F</url>
    <content type="text"><![CDATA[一个粗略的解释判别模型（Discriminative model）：能够直接判断出输入的数据会输出什么东西的模型。比如对于一个输入，直接告诉你这个输入是 $+1$ 类，还是 $-1$ 类。 常见的判别模型有：线性回归（Linear Regression）、逻辑斯谛回归（Logistic Regression）、支持向量机（SVM）、传统神经网络（Traditional Neural Networks)、线性判别分析（Linear Discriminative Analysis）、条件随机场（Conditional Random Field）等。 生成模型（Generative model）：描述猜测目标的可能的分布的模型。比如给一个输入，告诉你输出为各个类别的概率。 常见的生成模型有：朴素贝叶斯（Naive Bayes）、隐马尔可夫模型（Hidden Markov Model）、贝叶斯网络（Bayes Networks）、隐含狄利克雷分布（Latent Dirichlet Allocation）。 理论解释定义： 训练数据 $(C, X)$ ，$C={c_1, c_2, \cdots, c_n}$ 是n个训练样本的标签，$X={x_1, x_2, \cdots, x_n}$ 是n个训练样本的特征。 单个测试数据 $(\tilde{c}, \tilde{x})$ ，$\tilde{c}$ 是测试数据的标签，$\tilde{x}$ 是测试数据的特征。 判别模型训练完毕后，输入测试数据，直接给出 $P(\tilde{c} | \tilde{x}) = P(\tilde{c} | \tilde{x}, C, X)$ 。我们认为这个条件分布是由模型的参数 $\theta$ 决定的，即 $P(\tilde{c} | \tilde{x},\theta)$ 。对于求解结果，我们有：$$\begin{aligned}&amp;P(\tilde{c} | \tilde{x}) \=&amp;P(\tilde{c} | \tilde{x}, C, X) \=&amp;\int P(\tilde{c}, \theta | \tilde{x}, C, X) \, d \theta \=&amp;\int P(\tilde{c} | \tilde{x}, \theta) \cdot P(\theta | C, X) \, d \theta \=&amp;\int P(\tilde{c} | \tilde{x}, \theta) \cdot \frac{P(C | X, \theta) \cdot P(\theta)}{\int P(C | X, \theta) \cdot P(\theta) \, d \theta} \, d \theta\end{aligned}$$实际上，求这个式子里的两个积分是相当复杂的，而且几乎不可能拿到全部的信息。 所以我们采用 variational inference 的方法来解决这个问题。如果训练样本足够多的话，可以使用 $\theta$ 的最大后验分布 $\theta_{MAP}$ 来对 $\theta$ 进行点估计（point estimate）。此时有， $P(\tilde{c} | \tilde{x}) = P(\tilde{c} | \tilde{x}, C, X) = P(\tilde{c} | \tilde{x},\theta_{MAP})$ 。 至于求最大后验分布，考虑到 $P(C|X) = \int P(C | X, \theta) \cdot P(\theta) \, d \theta$ 是一个常数，我们只要求 $P(C | X, \theta) \cdot P(\theta)$ 的最大值就可以，于是问题转变为求最大似然函数。 事实上，在假设噪声为高斯分布的前提下，最小误差平方和优化问题等价于求最大似然函数。（有兴趣的同学可以自己证明一下。） 总结一下，判别模型求解的思路是：条件分布→模型参数后验概率最大→似然函数/参数先验最大→最大似然。 生成模型直接求出联合分布 $P(\tilde{x}, \tilde{c})$ 。 以朴素贝叶斯为例：$P(\tilde{x}, \tilde{c}) = P(\tilde{x} | \tilde{c}) \cdot P(\tilde{c})$ 。 总结一下，生成模型求解的思路是：联合分布→求解类别先验概率和类别条件概率。 两种模型的优缺点判别模型优点： 节省计算资源，需要的样本数量少于生成模型。 准确率较生成模型高。 由于直接预测结果的概率，而不需要求解每一个类别的条件概率，所以允许对数据进行抽象（比如降维、构造等）。 缺点： 生成模型的优点它都没有。 生成模型优点： 因为结果给出的是联合分布，不仅能计算条件分布，还可以给出其它信息（比如边缘分布）。 收敛速度比较快，当样本数量较多时，可以更快地收敛于真实模型。 能够应付隐变量存在的情况，如高斯混合模型（Gaussian Mixture Model）。 缺点： 需要更多的样本和计算，尤其是为了更准确估计类别条件分布，需要增加样本的数目。]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>Discriminative model</tag>
        <tag>Generative model</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[考虑评分人数的用户评分模型]]></title>
    <url>%2F2017%2F12%2F27%2Frating-model-considering-user-count%2F</url>
    <content type="text"><![CDATA[应用场景在现实生活中我们会接触到很多评分系统，如豆瓣的书评、YouTube 的影评、StackOverflow 的回答评分等等。在这些评分中一个共同的问题是每个 item 的评分人数是不同的，因此 50000 个人打了 95 分似乎比只有 5 个人打了 95 分更能被相信该 item 是“95”分；而 50000 个人打了 90 分和 40000 个人打了 92 分的比较又如何呢？为了解决这个问题，引入了威尔逊区间法进行评分，并使用贝叶斯平滑对评分做了修正。 威尔逊区间法威尔逊区间法是基于二项分布的一种计算方法，想法很简单，如果 100 个人打分的平均分为 85 分，那么我们可以把这个平均分看作 100 个人中有 85% 的人给了满分，而另外 15% 的人打零分。就像硬币的正反面，要不就选满分，要不就选零分，保证平均数一样就可以。在这个条件下，威尔逊区间法考虑对一个 item ，有 85% 的人“愿意选择”这件事在置信水平 $\alpha$ 下的置信区间是多少（一般选 $\alpha = 95\%$ ），然后用这个置信区间的下限来当做这个 item 的评价。这样就在一定程度上平滑了人数对评价的影响。 定义： 最大评分：$S_{\mathrm{max}}$ 选择该 item 的人的比例：$p$ 评价总数：$n$ 统计量常数（使用置信水平计算）：$K = z_{1-\frac{\alpha}{2}}$ 则修正后的得分是：$$s = p_{\mathrm{min}} \cdot S_{\mathrm{max}}, \, p_{\mathrm{min}} = \frac{p + \frac{K^{2}}{2n} - K \sqrt{\frac{p(1 - p)}{n} + \frac{K^{2}}{4n^{2}}}}{1 + \frac{K^{2}}{n}}$$这个公式，有印象的同学可以发现，我在 之前的一篇博客 中提到过。 贝叶斯平滑贝叶斯平滑其实并不是评分模式，但是它可以解决一些边界问题，比如对于评分人数过少的 item ，我们可以假设有 $C$ 个不存在的人，这些人都打了全局平均分来给总分做一个平滑。 定义： 补偿人数：$C$ 补偿评分：$M$ 该 item 评分人数：$n$ 该 item 的得分：$s$ 则平滑后的得分为：$$\hat{s} = \frac{CM + ns}{C + n}$$ Python 实现123456789101112131415161718192021222324252627282930313233343536373839def Wilson(p, n): """ 威尔逊区间的下限 """ p = float(p) K = 1.96 # 95% confidence level _K2_div_n = (K ** 2) / n pmin = (p + _K2_div_n / 2.0 - K * ((p * (1 - p) / n + _K2_div_n / n / 4.0) ** 0.5)) / (1 + _K2_div_n) return pmindef WilsonAvgP(n): """ 这个分数是用来充当 Bayesian 中的 M 的，取法比较随意。 本方法中是取了 0.01~1 这 100 中情况的置信区间的平均值。 其实取所有数据的总平均分也可以，总之，是一个比较平均的合理的得分就好。 """ totalP = 0.0 totalN = 0 p = 0.01 while True: totalP += Wilson(p, n, 1); totalN += 1 p += 0.01 if p &gt;= 1: break return totalP / totalNdef Bayesian(C, M, n, s): """ 贝叶斯平滑 这里的 C 和 M 其实都随意取得，就像上面那个函数中说的， 只要合理并且是一个差不多平均的量就可以。 """ return (C * M + n * s) / (n + C)]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>Rating</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[梯度下降法家族]]></title>
    <url>%2F2017%2F12%2F23%2FGD-Series%2F</url>
    <content type="text"><![CDATA[用一个通式说明梯度下降梯度下降优化法经历了 SGD→SGDM→NAG→AdaGrad→AdaDelta→Adam→Nadam 这样的发展历程。之所以会不断地提出更加优化的方法，究其原因，是引入了动量（Momentum）这个概念。最初，人们引入一阶动量来给梯度下降法加入惯性（即，越陡的坡可以允许跑得更快些）。后来，在引入二阶动量之后，才真正意味着“自适应学习率”优化算法时代的到来。 我们用一个通式来描述梯度下降法，掌握了这个通式，你就也可以自己设计自己的优化算法了。 首先定义： 待优化参数：$w$ 目标函数：$f(w)$ 初始学习率：$\alpha$ 然后进行迭代优化。在每个 epoch $t$ ： 计算目标函数关于当前参数的梯度：$g_{t} = \nabla f(w_{t})$ 根据历史梯度计算一阶动量和二阶动量：$m_{t} = \phi(g_{1}, g_{2}, \cdots , g_{t}); \, V_{t} = \psi(g_{1}, g_{2}, \cdots , g_{t})$ 计算当前时刻的下降梯度：$\eta_{t} = \alpha \cdot m_{t} \Big/ \sqrt{V_{t}}$ 根据下降梯度进行更新：$w_{t+1} = w_{t} - \eta_{t}$ SGDSGD 没有动量的概念，即：$m_{t} = g_{t}; \, V_{t} = I^{2}$ 此时，$\eta_{t} = \alpha \cdot g_{t}$ SGD 最大的缺点是下降速度慢，而且可能会在沟壑的两边持续震荡，停留在一个局部最优点。 SGD with Momentum为了抑制 SGD 的震荡，SGDM 引入了惯性，即一阶动量。如果发现是陡坡，就用惯性跑得快一些。此时：$m_{t} = \beta_{1} \cdot m_{t-1} + (1 - \beta_{1}) \cdot g_{t}$ 我们把这个式子递归一下就可以看出，一阶动量其实是各个时刻梯度方向的指数移动平均值，约等于最近 $1 / (1 - \beta_{1})$ 个时刻的梯度向量和的平均值。 $\beta_{1}$ 的经验值为 0.9 ，也就是每次下降时更偏向于此前累积的下降方向，并稍微偏向于当前下降方向。（就像拐弯时刹不住车，或者说得好听点——漂移）。 SGD with Nesterov AccelerationSGD 还有一个问题是会被困在一个局部最优点里。就像被一个小盆地周围的矮山挡住了视野，看不到更远的更深的沟壑。 Nesterov 提出了一个方法是既然我们有了动量，那么我们可以在步骤 1 中先不考虑当前的梯度。每次决定下降方向的时候先按照一阶动量的方向走一步试试，然后在考虑这个新地方的梯度方向。此时的梯度就变成了：$g_{t} = \nabla f(w_{t} - \alpha \cdot m_{t-1})$ 。 我们用这个梯度带入 SGDM 中计算 $m_{t}$ 的式子里去，然后再计算当前时刻应有的梯度并更新这一次的参数。 AdaGrad终于引入了二阶动量。 想法是这样：神经网络中有大量的参数，对于经常更新的参数，我们已经积累了大量关于它们的知识，不希望它们被单个样本影响太大，希望学习速率慢一些；而对于不经常更新的参数，我们对于它们了解的信息太少，希望能从每个偶然出现的样本身上多学一些，学习速率大一些。 那么怎么样去度量历史更新频率呢？采用二阶动量——该维度上，所有梯度值的平方和：$V_{t} = \sum_{\tau = 1}^{t} g_{\tau}^{2}$ 。 回顾步骤 3 中的下降梯度：$\eta_{t} = \alpha \cdot m_{t} \Big/ \sqrt{V_{t}}$ 。我们发现引入二阶动量的意义上给了学习率一个缩放比例，从而达到了自适应学习率的效果（Ada = Adaptive）。（一般为了防止分母为 0 ，会对二阶动量做一个平滑。） 这个方法有一个问题是，因为二阶动量是单调递增的，所以学习率会很快减至 0 ，这样可能会使训练过程提前结束。 AdaDelta / RMSProp由于 AdaGrad 的学习率单调递减太快，我们考虑改变二阶动量的计算策略：不累计全部梯度，只关注过去某一窗口内的梯度。这个就是名字里 Delta 的来历。 修改的思路很直接，前面我们说过，指数移动平均值大约是过去一段时间的平均值，因此我们用这个方法来计算二阶累积动量：$V_{t} = \beta_{2} \cdot V_{t-1} + (1 - \beta_{2}) \cdot g_{t}^{2}$ 。 Adam / Nadam讲到这个 Adam 和 Nadam 就自然而然的出现了，既然有了一阶动量和二阶动量，干嘛不都用起来呢？所以： Adam = Adaptive + Momentum。 Nadam = Nesterov + Adam ※上文中提到的 $\beta_{1}$ 和 $\beta_{2}$ 就是在一些深度学习框架里提到一阶动量和二阶动量时要传递的两个系数。 不忘初心，为什么还要使用 SGD？既然 Adam 和 Nadam 这么厉害，为什么学术界在发论文时还有很多大牛只使用 SGD ？ 理由很简单：应用是应用，学术是学术。 SGD 虽然不那么“聪明”，但是它“单纯”。所有的控制都在你自己手中，用它才不会影响你对其它因素的研究。 而其它的算法实在太“快”了，所以它们有时不收敛，有时不能达到最优解。 最后的一句话算法没有好坏，最适合数据的才是最好的，永远记住：No free lunch theorem。]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>GD</tag>
        <tag>Momentum</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[神经网络中均方误差与交叉熵作为损失函数的共同点]]></title>
    <url>%2F2017%2F12%2F03%2Fcommon-ground-of-mse-and-cee-in-nn%2F</url>
    <content type="text"><![CDATA[先说结论在神经网络中有两种可以选择的损失函数，即均方误差和交叉熵误差。这两种损失函数在形式上很不一样，而且分别适用与不同的地方（回归或分类）。但是在适用梯度下降法学习最优参数时，它们是一致的。 考虑神经网络的 BP 算法，其最核心的一步是计算敏感度 $\delta$ ，采用不同损失函数和激活函数的神经网络在 BP 算法上的差异也主要存在于敏感度上。而下文中，我们就分别讨论了上述两种损失函数在不同场景下的情况，得到了它们的敏感度是相等的结论。 用到的符号： $net$ 或 $net_{i}$ ，净输出值，即 $net = w^{\mathsf{T}}x$ 。 $a$ 或 $a_{i}$ ，神经元的激活函数输出值，即 $a=f(net)$ 。 均方误差——线性回归线性回归使用均方误差作为损失函数。其激活函数为 identical ，即$a=net=w^{\mathsf{T}}x$ 。其损失：$$J(w) = \frac{1}{2} (a-y)^{2}=\frac{1}{2} (net-y)^{2}$$输出神经元的敏感度：$$\delta = \frac{\partial J}{\partial net} = net-y = a-y$$ 交叉熵——逻辑回归逻辑回归使用最大似然估计方法估计参数。 二分类逻辑回归其激活函数为 sigmoid 函数 ，即$a=\sigma(net)=\sigma(w^{\mathsf{T}}x)=\frac{1}{1+\exp(-w^{\mathsf{T}}x)}$ 。其损失：$$J(w) = -L(w) = -y \ln a - (1-y) \ln (1-a)$$其中：$$L(w) = y \ln a + (1-y) \ln (1-a)$$是所谓的 log-likelihood。 输出神经元的敏感度：$$\delta = \frac{\partial J}{\partial net} = \frac{\partial J}{\partial a} \frac{\partial a}{\partial net} = \frac{a-y}{(1-a)a}(1-a)a = a-y$$ 多分类逻辑回归其激活函数为 softmax 函数，即$a=\mathrm{softmax}(net)=\frac{\exp(net_{i})}{\sum_{j=1}^{N}\exp(net_{j})}$ 。 其损失：$$J(w) = -\sum_{j=1}^{N} y_j \ln a_j = \sum_{j=1}^{N} y_j (\ln \sum_{k=1}^{N} e^{net_{k}}-net_{j})$$第 $i$ 个输出神经元的敏感度：$$\delta_{i} = \frac{\partial J}{\partial net_{i}} = \sum_{j=1}^{N} y_j (\frac{\sum_{k=1}^{N} e^{net_{k}}\frac{\partial net_{k}}{\partial net_{i}}}{\sum_{k=1}^{N} e^{net_{k}}} - \frac{\partial net_{j}}{\partial net_{i}}) = \sum_{j=1}^{N} y_j \frac{e^{net_{i}}}{\sum_{k=1}^{N} e^{net_{k}}} - \sum_{j=1}^{N} y_j \frac{\partial net_{j}}{\partial net_{i}} = a_{i}-y_{i}$$]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>Deep learning</tag>
        <tag>Loss function</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生成对抗网络（二）—— Wasserstein GAN 简述及一个 TensorFlow 示例]]></title>
    <url>%2F2017%2F11%2F04%2FGAN-02%2F</url>
    <content type="text"><![CDATA[原始 GAN 到底出了什么问题在近似最优判别器下，最小化生成器的 loss 等价于最小化 $P_{\text{data}}$ 和 $P_g$ 之间的 JS 散度，而由于 $P_{\text{data}}$ 和 $P_g$ 几乎不可能有不可忽略的重叠，所以无论它们相距多远，JS 散度都是常数 $\log 2$ ，最终导致生成器的梯度近似为 $0$ ，梯度消失。 又因为最小化目标等价于最小化$$KL(P_{g} \Vert P_{\text{data}}) - 2 JS(P_{\text{data}} \Vert P_{g})$$这个目标要求同时最小化生成分布和真是分布的 KL 散度，并且最大化两者的 JS 散度。产生了数值上的不稳定。最终造成生成器宁可多生成一些重复但是很“安全”的样本，也不愿意去生成多样性的样本。就发生了所谓的 collapse mode。 Wasserstein GAN 改动了什么Wasserstein GAN 与 GAN 相比，只改了四点： 判别器最后一层去掉 sigmoid 生成器和判别器的 loss 不取 log 每次更新判别器的参数之后把它们的绝对值截断到不超过一个固定常数 c 不要用基于动量的优化方法（包括 momentum 和 Adam），推荐使用 RMSProp，SGD 也行。 PS：第四点是论文作者从实验中用玄学得到的。因为如果使用 Adam，判别器的 loss 有时候会崩掉，然后 Adam 给出的更新方向与梯度方向夹角的 cos 值就会变成负数，判别器的 loss 梯度就变得不稳定了。但是改用 RMSProp 以后就解决了上述问题。 PPS：推荐使用比较小的 learning rate，论文中使用的是 $\alpha=0.00005$ 。 一个 TensorFlow 示例（基于 MNIST 数据库）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_dataimport numpy as npimport matplotlib.pyplot as pltimport matplotlib.gridspec as gridspecimport osmb_size = 32X_dim = 784z_dim = 10h_dim = 128mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)def plot(samples): fig = plt.figure(figsize=(4, 4)) gs = gridspec.GridSpec(4, 4) gs.update(wspace=0.05, hspace=0.05) for i, sample in enumerate(samples): ax = plt.subplot(gs[i]) plt.axis('off') ax.set_xticklabels([]) ax.set_yticklabels([]) ax.set_aspect('equal') plt.imshow(sample.reshape(28, 28), cmap='Greys_r') return figdef xavier_init(size): in_dim = size[0] xavier_stddev = 1. / tf.sqrt(in_dim / 2.) return tf.random_normal(shape=size, stddev=xavier_stddev)X = tf.placeholder(tf.float32, shape=[None, X_dim])D_W1 = tf.Variable(xavier_init([X_dim, h_dim]))D_b1 = tf.Variable(tf.zeros(shape=[h_dim]))D_W2 = tf.Variable(xavier_init([h_dim, 1]))D_b2 = tf.Variable(tf.zeros(shape=[1]))theta_D = [D_W1, D_W2, D_b1, D_b2]z = tf.placeholder(tf.float32, shape=[None, z_dim])G_W1 = tf.Variable(xavier_init([z_dim, h_dim]))G_b1 = tf.Variable(tf.zeros(shape=[h_dim]))G_W2 = tf.Variable(xavier_init([h_dim, X_dim]))G_b2 = tf.Variable(tf.zeros(shape=[X_dim]))theta_G = [G_W1, G_W2, G_b1, G_b2]def sample_z(m, n): return np.random.uniform(-1., 1., size=[m, n])def generator(z): G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1) G_log_prob = tf.matmul(G_h1, G_W2) + G_b2 G_prob = tf.nn.sigmoid(G_log_prob) return G_probdef discriminator(x): D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1) out = tf.matmul(D_h1, D_W2) + D_b2 return outG_sample = generator(z)D_real = discriminator(X)D_fake = discriminator(G_sample)D_loss = tf.reduce_mean(D_real) - tf.reduce_mean(D_fake)G_loss = -tf.reduce_mean(D_fake)D_solver = (tf.train.RMSPropOptimizer(learning_rate=1e-4) .minimize(-D_loss, var_list=theta_D))G_solver = (tf.train.RMSPropOptimizer(learning_rate=1e-4) .minimize(G_loss, var_list=theta_G))clip_D = [p.assign(tf.clip_by_value(p, -0.01, 0.01)) for p in theta_D]sess = tf.Session()sess.run(tf.global_variables_initializer())if not os.path.exists('out/'): os.makedirs('out/')i = 0for it in range(1000000): for _ in range(5): X_mb, _ = mnist.train.next_batch(mb_size) _, D_loss_curr, _ = sess.run( [D_solver, D_loss, clip_D], feed_dict=&#123;X: X_mb, z: sample_z(mb_size, z_dim)&#125; ) _, G_loss_curr = sess.run( [G_solver, G_loss], feed_dict=&#123;z: sample_z(mb_size, z_dim)&#125; ) if it % 100 == 0: print('Iter: &#123;&#125;; D loss: &#123;:.4&#125;; G_loss: &#123;:.4&#125;'.format(it, D_loss_curr, G_loss_curr)) if it % 1000 == 0: samples = sess.run(G_sample, feed_dict=&#123;z: sample_z(16, z_dim)&#125;) fig = plot(samples) plt.savefig('out/&#123;&#125;.png'.format(str(i).zfill(3)), bbox_inches='tight') i += 1 plt.close(fig)]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>Deep learning</tag>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生成对抗网络（一）—— GAN 原理简述及一个 TensorFlow 示例]]></title>
    <url>%2F2017%2F11%2F01%2FGAN-01%2F</url>
    <content type="text"><![CDATA[生成对抗网络（GAN）的基本概念2014年 Goodfellow 等人提出了一个生成对抗网络（GAN）的概念后一直火爆至今，相比于其它生成模型，它有以下几个优点： 模型只用到了反向传播，而不需要马尔科夫链。 训练时不需要对隐变量做推断。 理论上，只要是可微分函数都可以用于构建 D（判别模型）和 G（生成模型），因为能够与深度神经网络结合做深度生成式模型。 G 的参数更新不是直接来自数据样本，而是使用来自 D 的反向传播。 它的主要思想来源于博弈论中的零和游戏。简单描述起来就是我们有一些真实的数据，也有一些随机生成的假数据。G 负责把这些数据拿过来拼命地模仿成真实数据并把它们藏在真实数据中，而 D 就拼命地要把伪造数据和真实数据分开。经过二者的博弈以后，G 的伪造技术越来越厉害，D 的鉴别技术也越来越厉害。直到 D 再也分不出数据是真实的还是 G 生成的数据的时候（D 判断正确的概率为 $1/2$ ，即随机猜测），我们就达到了目的。此时 G 可以用来模仿生成所谓的“真实数据”了。 为了学习到生成器在数据 $x$ 生的分布 $p_g(x)$ ，我们先定义一个先验的输入噪声变量 $p_z(z)$ ，然后根据 $G(z;\theta_g)$ 将其映射到数据空间中，其中 $G$ 为多层感知机所表征的可微函数。然后利用第二个多层感知机 $D(x;\theta_d)$ ，它的输出为单个标量，表示 $x$ 来源于真实数据的概率。我们训练 $D$ 来最大化正确分配真实样本和生成样本的概率，因此我们就可以通过最小化 $\log (1-D(G(z)))$ （G 生成的数据被分错的损失函数）而同时训练 $G$ 。即：$$\min_G \max_D V(D,G) = \mathbb{E}{x \sim p{\text{data}}(x)}[\log D(x)] + \mathbb{E}{z \sim p{z}(z)}[\log (1-D(G(z)))]$$ 最优判别器考虑到$$\mathbb{E}{z \sim p{z}(z)}[\log (1-D(G(z)))] = \mathbb{E}{x \sim p{g}(x)}[\log (1-D(x)]$$我们可以将价值函数 $V(D,G)$ 展开为在全体 $x$ 上的积分形式$$V(G, D) = \int_{x} p_{\text{data}}(x) \log (D(x)) + p_g(x) \log (1-D(x)) \, dx$$因为求积分最大值可以转化为求被积函数最大值，且 $p_{\text{data}}(x)$ 和 $p_g(x)$ 均为标量，因此我们讨论如下形式的式子的最大值：$$f(y) = a \log y + b \log (1-y)$$考虑到$$f’(y) = 0 \Rightarrow \frac{a}{y}-\frac{b}{1-y}=0 \Rightarrow y=\frac{a}{a+b}$$且$$f’’(y) \Big|{y=\frac{a}{a+b}} = -\frac{a}{\left( \frac{a}{a+b} \right)^{2}}-\frac{b}{1-\left( \frac{a}{a+b} \right)^{2}}&lt;0$$我们得到极大值在 $\frac{a}{a+b}$ 处取到，令 $a=p{\text{data}}(x), b=p_g(x), y=D(x)$ ，则容易对比出最优判别器$$D(x) = \frac{p_{\text{data}}(x)}{p_{\text{data}}(x) + p_g(x)}$$ 最优生成器显然，GAN 的目的是让 $p_{\text{data}}=p_g$ 。此时判别器已经完全分辨不出真实数据和生成数据的区别了。即$$D_G^{\ast}=\frac{p_{\text{data}}(x)}{p_{\text{data}}(x) + p_g(x)}=\frac{1}{2}$$Goodfellow 等人证明，这个最优生成器就是上面那个 min max 的式子。 一个 TensorFlow 示例（基于 MNIST 数据库）简单起见，没有用 CNN。主要是展示 G 与 D 的博弈过程。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112import tensorflow as tfimport numpy as npfrom tensorflow.examples.tutorials.mnist import input_data# from PIL import Imagemnist = input_data.read_data_sets("MNIST_data/")images = mnist.train.imagesdef xavier_initializer(shape): return tf.random_normal(shape=shape, stddev=1.0 / shape[0])# Generatorz_size = 100 # maybe largerg_w1_size = 400g_out_size = 28 * 28# Discriminatorx_size = 28 * 28d_w1_size = 400d_out_size = 1z = tf.placeholder('float', shape=(None, z_size))X = tf.placeholder('float', shape=(None, x_size))# use dict to share variablesg_weights = &#123; 'w1': tf.Variable(xavier_initializer(shape=(z_size, g_w1_size))), 'b1': tf.Variable(tf.zeros(shape=[g_w1_size])), 'out': tf.Variable(xavier_initializer(shape=(g_w1_size, g_out_size))), 'b2': tf.Variable(tf.zeros(shape=[g_out_size])),&#125;d_weights = &#123; 'w1': tf.Variable(xavier_initializer(shape=(x_size, d_w1_size))), 'b1': tf.Variable(tf.zeros(shape=[d_w1_size])), 'out': tf.Variable(xavier_initializer(shape=(d_w1_size, d_out_size))), 'b2': tf.Variable(tf.zeros(shape=[d_out_size])),&#125;def G(z, w=g_weights): # here tanh is better than relu h1 = tf.tanh(tf.matmul(z, w['w1']) + w['b1']) # pixel output is in range [0, 255] return tf.sigmoid(tf.matmul(h1, w['out']) + w['b2']) * 255def D(x, w=d_weights): # here tanh is better than relu h1 = tf.tanh(tf.matmul(x, w['w1']) + w['b1']) h2 = tf.matmul(h1, w['out']) + w['b2'] return h2 # use h2 to calculate logits lossdef generate_z(n=1): return np.random.normal(size=(n, z_size))sample = G(z)dout_real = D(X)dout_fake = D(G(z))G_obj = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits(logits=dout_fake, labels=tf.ones_like(dout_fake)))D_obj_real = tf.reduce_mean( # use single side smoothing tf.nn.sigmoid_cross_entropy_with_logits(logits=dout_real, labels=(tf.ones_like(dout_real) - 0.1)))D_obj_fake = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits(logits=dout_fake, labels=tf.zeros_like(dout_fake)))D_obj = D_obj_real + D_obj_fakeG_opt = tf.train.AdamOptimizer().minimize(G_obj, var_list=g_weights.values())D_opt = tf.train.AdamOptimizer().minimize(D_obj, var_list=d_weights.values())# Trainingbatch_size = 128with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for i in range(200): sess.run(D_opt, feed_dict=&#123; X: images[np.random.choice(range(len(images)), batch_size)].reshape(batch_size, x_size), z: generate_z(batch_size), &#125;) # run two phases of generator sess.run(G_opt, feed_dict=&#123; z: generate_z(batch_size) &#125;) sess.run(G_opt, feed_dict=&#123; z: generate_z(batch_size) &#125;) g_cost = sess.run(G_obj, feed_dict=&#123;z: generate_z(batch_size)&#125;) d_cost = sess.run(D_obj, feed_dict=&#123; X: images[np.random.choice(range(len(images)), batch_size)].reshape(batch_size, x_size), z: generate_z(batch_size), &#125;) image = sess.run(G(z), feed_dict=&#123;z: generate_z()&#125;) df = sess.run(tf.sigmoid(dout_fake), feed_dict=&#123;z: generate_z()&#125;) # print i, G cost, D cost, image max pixel, D output of fake print(i, g_cost, d_cost, image.max(), df[0][0]) # You may wish to save or plot the image generated # to see how it looks like image = sess.run(G(z), feed_dict=&#123;z: generate_z()&#125;) image1 = image[0].reshape([28, 28]) # print(image1) # im = Image.fromarray(image1) # im.show()]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>Deep learning</tag>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K-means 是 GMM 的一种特例]]></title>
    <url>%2F2017%2F10%2F23%2Fkmeans-is-a-gmm%2F</url>
    <content type="text"><![CDATA[对一个特定 GMM 模型的讨论考虑到在非监督学习 K-means 中，那些不存在的所谓的“标签”也可以被当成“不可见因素”的想法。本文通过一个特定的 GMM（高斯混合模型，Gaussian Mixture Model）模型讨论了 K-means 算法在该特定模型下可以看做一个 EM 算法。因此可以认为 K-means 算法与 EM 算法在某种先验条件下存在交集。 首先给出以下两个问题的描述： 聚类问题：给定数据点 $x_{1}, x_{2}, \cdots, x_{N} \in \mathbb{R}^{m}$ ，给定分类数目 $K$ ，求出 $K$ 个类中心 $\mu_{1}, \mu_{2}, \cdots, \mu_{K}$ ，使得所有点到距离该点最近的类中心的距离的平方和 $\sum\limits_{i=1}^{N} \min\limits_{1 \le k \le K} \lVert x_{i} - \mu_{k} \rVert_{2}^{2}$ 最小。 含隐变量的最大似然问题：给定数据点 $x_{1}, x_{2}, \cdots, x_{N} \in \mathbb{R}^{m}$ ，给定分类数目 $K$ ，考虑如下生成模型，$$p(x, z \mid \mu_{1}, \mu_{2}, \cdots, \mu_{K}) \propto\left {\begin{aligned}&amp;\exp \left ( -\lVert x-\mu_{z} \rVert_{2}^{2} \right ) &amp; \qquad \lVert x-\mu_{z} \rVert_{2} = \min_{1 \le k \le K} \lVert x-\mu_{k} \rVert_{2} \&amp;0 &amp; \qquad \lVert x-\mu_{z} \rVert_{2} &gt; \min_{1 \le k \le K} \lVert x-\mu_{k} \rVert_{2}\end{aligned}\right.$$模型中 $z \in {1, 2, \cdots, K}$ 为隐变量。 这个模型的意义是：先验的假设真的存在那么 $K$ 个中心，只不过看不到而已。而所有的数据确实也是根据这 $K$ 个中心生成的，且这个生成概率是以其最近的“隐形中心”为中心呈高斯分布，并且不认为（零概率）该数据点是由离其较远的“隐形中心”生成的。（因此这个模型的条件还是很苛刻的。） 结论：用 EM 算法解这个含隐变量的最大似然问题等价于用 K-means 算法解聚类问题。 E 步骤： 计算$$p(z_{i} \mid x_{i}, {\mu_{k}}^{(t)}) \propto\left {\begin{aligned}&amp;1 &amp; \qquad \lVert x_{i}-\mu_{z_{i}}^{(t)} \rVert_{2} = \min_{1 \le k \le K} \lVert x_{i}-\mu_{k}^{(t)} \rVert_{2} \&amp;0 &amp; \qquad \lVert x_{i}-\mu_{z_{i}}^{(t)} \rVert_{2} &gt; \min_{1 \le k \le K} \lVert x_{i}-\mu_{k}^{(t)} \rVert_{2}\end{aligned}\right.$$以及$$p(Z \mid X, { \mu_{k} }^{(t)}) = \prod_{i=1}^{N} p(z_{i} \mid x_{i}, { \mu_{k} }^{(t)})$$这里使用正比于符号是考虑离数据点最近的中心可能不止一个。 下面为了简单只讨论只有一个中心，且中心编号为 $y_{i}$ 的情况。 计算目标函数$$\begin{aligned}Q({ \mu_{k} } \mid { \mu_{k} }^{(t)}) &amp; = E_{Z \mid X, { \mu_{k} }^{(t)}} [\log p(X, Z \mid { \mu_{k} })] \&amp; = \sum_{i=1}^{N} E_{z_{i} \mid x_{i}, { \mu_{k} }^{(t)}} [\log p(x_{i}, z_{i} \mid { \mu_{k} })] \&amp; = \sum_{i=1}^{N} \log p(x_{i}, z_{i}=y_{i} \mid { \mu_{k} }) \&amp; = \mathrm{const} - \sum_{i=1}^{N} \lVert x_{i} - \mu_{y_{i}} \rVert_{2}^{2}\end{aligned}$$所以最大化目标函数就等价于最小化 $Q’({ \mu_{k} } \mid { \mu_{k} }^{(t)}) = \sum\limits_{i=1}^{N} \lVert x_{i} - \mu_{y_{i}} \rVert_{2}^{2}$ 。 M 步骤： 找寻 ${ \mu_{k} }$ 使得 $Q’$ 最小。 将指标集 ${1, 2, \cdots, K }$ 分割为 $K$ 个，$I_{k} = { i \mid y_{i}=k }$ ，则$$Q’({ \mu_{k} } \mid { \mu_{k} }^{(t)}) = \sum\limits_{i=1}^{K} \sum_{i \in I_{k}} \lVert x_{i} - \mu_{k} \rVert_{2}^{2}$$由于求和的每一项都是非负的，所以每一个内层求和 $\sum\limits_{i \in I_{k}} \lVert x_{i} - \mu_{k} \rVert_{2}^{2}$ 都达到最小时，总和最小。 这和 K-means 最小化二范数平方和是一样的。 进阶讨论以上的讨论是为了简单起见提出了一个简化的 GMM 模型，这个模型里没有提到高斯分布的协方差矩阵的问题。实际上，K-means 在简化 GMM 时，应该是限制协方差矩阵 $\Sigma = \sigma I$ ，再让 $\sigma \to 0$ 。更多的讨论在 这篇论文 的 2.1 节可以找到。]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>K-means</tag>
        <tag>GMM</tag>
        <tag>EM Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈 boost、Adaboost 和 提升树]]></title>
    <url>%2F2017%2F10%2F15%2Ffrom-boost-to-Adaboost-to-GBT%2F</url>
    <content type="text"><![CDATA[三个公式 boost = 加法模型 + 前向分布算法 Adaboost = boost + 损失函数是指数函数（基函数任意） 提升树 = boost + 基函数是决策树（损失函数任意） 由此可以看出： boost 是一种算法框架，该框架是由加法模型和前向分布算法组成的。 Adaboost 和提升树都是 boost 算法的一个特殊情况，分别限定了 boost 算法的某一部分而得到。 加法模型前向分布算法：每次学习一个基函数的参数作为下一个基函数。 下一个基函数的选择标准：在已经学习到的加法模型中，选择一个使损失函数最小的基函数。 常见损失函数指数损失函数：决定了 Adaboost 必须进行加权取样（权重由错误率决定），最终模型也是加权累计。 平方误差损失函数：决定了 BRT 的下一个模型应该学习前一个模型的残差。 一般损失函数：决定了 GBDT/GBRT 的下一个模型应该学习一个模型的梯度（残差近似）。 各种提升树BDT：二叉分类树 + 指数损失（加权学习）。（其实就是 Adaboost 要求基函数是二叉分类树）。 BRT：二叉回归树 + 平方误差损失（残差）。 GBDT：二叉分类树 + 普通损失函数（计算负梯度，近似残差）。 GBRT：二叉回归树 + 普通损失函数（计算负梯度，近似残差）。 其中，如果把 GBRT 的损失函数选为平方误差损失，则退化为 BRT，因为平方损失函数的负梯度就是残差。]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>Ensemble</tag>
        <tag>Boost</tag>
        <tag>GBDT/GBRT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[给程序员们看的统计公式]]></title>
    <url>%2F2017%2F10%2F12%2Fstatistical-formulars-for-programmers%2F</url>
    <content type="text"><![CDATA[与平均数有关的公式修正后的标准差标准差是一个反映出数据分散的多散的数字。它往往和平均数一起给出。$$s = \sqrt{\frac{1}{N-1}\sum_{i=1}^{N}(x_{i}-\bar{x})^{2}}$$其中 $N$ 是样本数 $x_{i}$ 是第 $i$ 个样本的值 $\bar{x}$ 是样本的平均值 标准误差从统计学的角度讲，“平均”只是一个对平均值的估计，这个估计的不确定性由标准误差来衡量。$$SE = \frac{s}{\sqrt{N}}$$ 由期望给出的置信区间置信区间反映了在某一置信水平下，统计假设不会被拒绝。所以由期望给出的置信区间反映了一系列可能的平均值，这些平均值作为对数据平均值的估计都不会被拒绝。$$CI = \bar{x} \pm t_{\alpha/2} SE$$其中 $\alpha$ 是显著性水平，一个典型的选择是 5% ，（ 1 减去置信水平） $t_{\alpha/2}$ 是自由度为 $N-1$ 的 t 分布的分位点 两样本 t 检验两样本的 t 检验可以告诉你两组样本的平均数是否一样。 检验统计量由下式给出$$t = \frac{\bar{x_{1}} - \bar{x_{2}}}{\sqrt{s_{1}^{2}/n_{1}+s_{2}^{2}/n_{2}}}$$平均数相同的假设将在 $\lvert t \rvert$ 大于自由度为$$df = \frac{(s_{1}^{2}/n_{1}+s_{2}^{2}/n_{2})^2}{(s_{1}^{2}/n_{1})^{2}/(n_{1}-1)+(s_{2}^{2}/n_{2})^{2}/(n_{2}-1)}$$的 t 分布的 $1-\alpha /2$ 分位点时被拒绝。 与比例有关的统计公式伯努利分布的参数的置信区间伯努利分布的参数是两分类结果中某个事件的比例。（比如抛硬币是出现正面的次数。）$$CI = \left( p + \frac{z_{\alpha / 2}^{2}}{2N} \pm z_{\alpha / 2}\sqrt{[p(1-p)+z_{\alpha / 2}/4N] / N} \right) / (1+z_{\alpha / 2}^{2}/N)$$其中 $p$ 是你所感兴趣的事件被观察到的比例 $z_{\alpha / 2}$ 是正态分布的 $1-\alpha / 2$ 分位点 多项分布的参数的置信区间$$CI = \left( p_{j} + \frac{z_{\alpha / 2}^{2}}{2N} \pm z_{\alpha / 2}\sqrt{[p_{j}(1-p_{j})+z_{\alpha / 2}/4N] / N} \right) / (1+z_{\alpha / 2}^{2}/N)$$ 卡方检验皮尔逊卡方检验可以检测出在一个表中每一行的样本频数是不是随着列的不同而变得不同（是否存在统计相关性）。 检验统计量$$X^{2} = \sum_{i=1}^{n} \sum_{j=1}^{m} \frac{(O_{i,j}-E_{i,j})^{2}}{E_{i,j}}$$其中 $n$ 是行数 $m$ 是列数 $O_{i,j}$ 是第 $i$ 行第 $j$ 列的观测频数 $E_{i,j}$ 是第 $i$ 行第 $j$ 列的期望频数 期望频数由下式给出$$E_{i,j} = \frac{\sum_{k=1}^{n} O_{k,j} \sum_{l=1}^{m} O_{i,l}}{N}$$其中 $N$ 是表中所有计数的总和 我们将在以下条件下认为存在统计相关性：$X^{2}$ 大于自由度为 $(m-1) \times (n-1)$ 的 $\chi^{2}$ 分布的 $1-\alpha$ 分位点。 与计数数据有关的统计公式泊松分布的标准差$$\sigma = \sqrt{\lambda}$$ 泊松分布参数的置信区间$$CI = \left( \frac{\gamma^{-1}(\alpha / 2,c)}{t},\frac{\gamma^{-1}(1 - \alpha / 2,c+1)}{t} \right)$$ 其中 $c$ 是在经过 $t$ 个时间区段后被观测到的事件数 $\gamma^{-1}(p, c)$ 是低阶不完全伽马函数的反函数 两个泊松分布参数的条件检验不要这样做：观测到 5 个，变化是 -2 个，所以减少了 28.57%。 从统计学的观点看，5 个与 7 个是没有明显区别的。在报告减少了很多之前，先做好两个泊松分布的平均值的条件检验。 p 值由下式给出$$p = 2 \times \frac{c!}{t^{c}} \times \min \left{ \sum_{i=0}^{c_{1}}\frac{t_{1}^{i}t_{2}^{c-i}}{i!(c-i!)}, \sum_{i=c_{1}}^{c}\frac{t_{1}^{i}t_{2}^{c-i}}{i!(c-i!)} \right}$$其中 观测结果 1 历经了 $t_{1}$ 个时间区段，由 $c_{1}$ 个事件组成 观测结果 2 历经了 $t_{2}$ 个时间区段，由 $c_{2}$ 个事件组成 $c = c_{1} + c_{2}$ ，$t = t_{1}+t_{2}$]]></content>
      <categories>
        <category>Mathematics</category>
      </categories>
      <tags>
        <tag>Statistics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅说范数规范化（附录）]]></title>
    <url>%2F2017%2F09%2F13%2Fnorm-regularization-appendix%2F</url>
    <content type="text"><![CDATA[附录 A：矩阵条件数 $\kappa({A})$在本系列第一篇里提到病态矩阵的时候，说到了矩阵条件数的一些性质，下面给一个简单的证明。 事实上由于条件数是描述线性方程系统 $AX = b$ 的变化敏感度的一个量，所以我们给这个系统的每个量一个微扰，然后解出 $x$ 的变化程度：$$\begin{aligned}&amp; \quad (A+\Delta A)(x + \Delta x) = b+\Delta b \&amp; \Rightarrow \Delta x = A^{-1}[\Delta b - (\Delta A) x - (\Delta A)(\Delta x)] \&amp; \Rightarrow \lVert \Delta x \rVert \le \lVert A^{-1} \rVert (\lVert \Delta b \rVert + \lVert \Delta A \rVert \lVert x \rVert + \lVert \Delta A\rVert \lVert \Delta x\rVert ) \&amp; \Rightarrow (1-\lVert A^{-1} \rVert \lVert \Delta A \rVert) \lVert \Delta x \rVert \le \lVert A^{-1} \rVert (\lVert \Delta b \rVert + \lVert \Delta A \rVert \lVert x \rVert) \&amp; \Rightarrow \lVert \Delta x \rVert \le \frac{\lVert A^{-1} \rVert}{1-\lVert A^{-1} \rVert \lVert \Delta A \rVert} \left( \lVert \Delta b \rVert + \lVert \Delta A \rVert \lVert x \rVert \right) \qquad (\because \, \lVert A^{-1} \rVert \lVert \Delta A \rVert \le 1) \&amp; \Rightarrow \frac {\lVert \Delta x \rVert}{\lVert x \rVert} \le \frac{\lVert A^{-1} \rVert \lVert A \rVert}{1-\lVert A^{-1} \rVert \lVert \Delta A \rVert} \left( \frac{\lVert \Delta b \rVert}{\lVert b \rVert} + \frac{\lVert \Delta A \rVert}{\lVert A \rVert} \right) \qquad (\because \, \lVert b \rVert \le \lVert A \rVert \lVert x \rVert)\end{aligned}$$当 $\Delta A = 0$ 时，有：$$\frac{\lVert \Delta x \rVert}{\lVert x \rVert} \le \kappa(A) \frac{\lVert \Delta b \rVert}{\lVert b \rVert}$$当 $\Delta b = 0$ 时，有：$$\frac{\lVert \Delta x \rVert}{\lVert x \rVert} \le \kappa(A) \frac{\lVert \Delta A \rVert}{\lVert A \rVert}(1+o(\lVert A^{-1} \rVert))$$一个常用的条件数是 2-条件数：$$\kappa_{2}(A) = \lVert A^{-1} \rVert _{2} \lVert A \rVert _{2} = \sqrt{\frac{\lambda _{max}}{\lambda {min}}}$$其中，$\lambda{max}$ 和 $\lambda_{min}$ 分别是 $A^{\mathsf{H}}A$ 的最大特征值和最小特征值。（从这个角度上来看，条件数也确实是衡量矩阵敏感度的一个值。）]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>Norm regularization</tag>
        <tag>Matrix theory</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅说范数规范化（二）—— 核范数]]></title>
    <url>%2F2017%2F09%2F09%2Fnorm-regularization-02%2F</url>
    <content type="text"><![CDATA[矩阵补全（matrix completion）问题这个问题最早火起来是因为 Netflix 公司悬赏 100 万美金的公开挑战，奖励给能够改进公司现行矩阵补全算法 10% 以上最优胜的队伍。最后的结果是 2009 年 9 月 BellKors Pragmatic 拿走了奖金。 什么是矩阵补全问题呢？用 Netflix 的数据集来作说明。简单的来说就是一个电影评分系统要根据非常稀疏的现有数据集（一个用户可能只 rate 了几十部电影）来推断整个用户群对不同电影的评分。 这个问题在推荐系统、图像处理等方面都有广泛的应用。接着，这类问题一般都有隐含的假设，即最终的矩阵应该是低秩（low rank）的。这其实也很好理解，因为我们一般会觉得：1、不同用户对于电影的偏好可以分成聚落（cliques），比如按照观众的年龄来分，年龄相仿的观众口味往往相近；2、电影也可以分成大致几种不同的题材（genres），如：爱情片、动作片、科幻片等。所以会有低秩的特性。简单来说，这个矩阵的行和列会有“协同”的特性，这也是这个问题的别名协同过滤（collaborative filtering）的得名原因。另外，低秩限制会比较实用，人们都比较喜欢得到稀疏解，使得整个问题更有可诠释性。 所以这个问题的目标函数被定义成下面的样子： \begin{aligned} &\mathrm{minimize} \quad \mathrm{rank}(Z) \\ &\text{subject to} \quad \sum_{(i,j):\text{Observed}} (Z_{ij} - X_{ij})^{2} \le \delta \end{aligned} \\ \text{Impute missing } X_{ij} \text{ with } Z_{ij}然而问题来了，这个问题是非凸的，而且这种问题的规模都非常大，没办法在如此庞大的 NP-hard 问题中找到全局最优解。 早先时候，人们使用了一些启发式算法。后来人们发现，核范数是矩阵秩的一个很好的凸近似。（原因有点类似与 L1 范数是 L0 范数的一个凸近似。）下面就介绍一下核范数的一些性质。 核范数定义矩阵 $X$ 的核范数定义为： \lVert X \rVert _{\ast} = \mathrm{tr}\left( \sqrt{X^{\mathsf{T}}X} \right)显而易见，核范数也可以等价地定义为矩阵特征值的和，考虑 $X$ 的特征值分解 $X=U \Sigma V^{\mathsf{T}}$ 显然有： \begin{aligned} \mathrm{tr}\left( \sqrt{X^{\mathsf{T}}X} \right) & = \mathrm{tr} \left( \sqrt{(U \Sigma V^{\mathsf{T}})^{\mathsf{T}}U \Sigma V^{\mathsf{T}}} \right) \\ & = \mathrm{tr} \left( \sqrt{V \Sigma^{\mathsf{T}} U^{\mathsf{T}} U \Sigma V^{\mathsf{T}}} \right) \\ & = \mathrm{tr} \left( \sqrt{V \Sigma^{2} V^{\mathsf{T}}} \right) \quad \left( \Sigma^{\mathsf{T}} = \Sigma \right) \\ & = \mathrm{tr} \left( \sqrt{V^{\mathsf{T}} V \Sigma^{2}} \right) \\ & = \mathrm{tr} (\Sigma) \end{aligned}凸性的证明首先，矩阵诱导范数是凸的，即：令 $ f_x (A) = \lVert Ax \rVert _{p} \quad (p \ge 1) $ ，则 $f_x$ 凸，故 $\lVert A \rVert _{p} = \sup\limits_{\Vert x \rVert _{p}=1} f_x (A)$ 凸。特别的，$\lVert A \rVert_{2}$ 凸。因为 $\lVert A \rVert_{\ast}$ 和 $\lVert A \rVert_{2}$ 是对偶范数，所以 $\lVert A \rVert_{\ast}$ 凸。（$\lVert A \rVert_{\ast}=\sup \limits_{\lVert X \rVert_{2}=1} \mathrm{tr} \left( A^{\mathsf{T}}X \right)$） 梯度的求解基于上述 S.V.D 的假设，我们可以轻易地得到， \frac{\partial \lVert X \rVert_{\ast}}{\partial X} = \frac{\partial \mathrm{tr} (\Sigma)}{\partial X} = \frac{\mathrm{tr} (\partial \Sigma)}{\partial X}所以我们需要解出 $\partial \Sigma$ ，考虑 $X=U \Sigma V^{\mathsf{T}}$ ，因此： \begin{aligned} & & \partial X & = (\partial U )\Sigma V^{\mathsf{T}} + U (\partial \Sigma) V^{\mathsf{T}} + U \Sigma (\partial V^{\mathsf{T}}) \\ &\Rightarrow \, & \partial \Sigma & = U^{\mathsf{T}} (\partial X) V - U^{\mathsf{T}} (\partial U) \Sigma - \Sigma (\partial V^{\mathsf{T}}) V \\ & & & = U^{\mathsf{T}} (\partial X) V \qquad (- U^{\mathsf{T}} (\partial U) \Sigma - \Sigma (\partial V^{\mathsf{T}}) V = 0) \end{aligned}所以： \frac{\partial \lVert X \rVert_{\ast}}{\partial X} = \frac{\mathrm{tr} (\partial \Sigma)}{\partial X} = \frac{\mathrm{tr} (U^{\mathsf{T}} (\partial X)V)}{\partial X} = \frac{\mathrm{tr} (V U^{\mathsf{T}} (\partial X))}{\partial X} = (V U^{\mathsf{T}})^{\mathsf{T}} = U V^{\mathsf{T}}低秩问题的近似定义了核范数以后，我们就可以将低秩优化问题近似成相应的核范数优化问题了。即： \begin{aligned} &\mathrm{minimize} \quad \lVert Z \rVert_{\ast} \\ &\text{subject to} \quad \sum_{(i,j):\text{Observed}} (Z_{ij} - X_{ij})^{2} \le \delta \end{aligned} \\ \text{Impute missing } X_{ij} \text{ with } Z_{ij}]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>Norm regularization</tag>
        <tag>Convex optimization</tag>
        <tag>Matrix theory</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅说范数规范化（一）—— L0 范数、L1 范数、L2 范数]]></title>
    <url>%2F2017%2F09%2F07%2Fnorm-regularization-01%2F</url>
    <content type="text"><![CDATA[我们需要范数规则化的原因监督机器学习用一句话总结就是：Minimize your error while regularizing your parameters。其中“minimize error”是目的，一般采用最小化损失函数的做法来达到。而“regularizing parameters”就是一种保障，它可以防止模型发生过拟合，让模型的参数规模尽量向着“简单”的方向进化。（根据奥卡姆剃刀原理 Occam’s Razor，简单的模型虽然不尽准确，却也有更好的泛化能力。） 所以监督机器学习一般可以用一个公式来代表：$$w^{\ast} = \mathop{\arg\min}{w} \sum{i} L(y_{i}, f(x_{i};w)) + \lambda\Omega(w)$$对于第一项的损失函数，我们不在本文中做过多讨论，大概说一下： Square loss —— 最小二乘 Hinge loss —— SVM Exp loss —— Boosting Log loss —— Logistic regression 等等。总之，不同的 loss 函数有不同的拟合特性。 下面我们重点说一说规则项 $ \Omega(w) $ 。 关于规则项大多时候规则项都是用来限制参数的复杂程度的。所以一般用参数 $w$ 的某些性质来约束，常见的就是几种范数：L0 范数、L1 范数、L2 范数、迹范数、Frobenius 范数和核范数。 本篇，着重说 L0 范数、L1 范数 和 L2 范数。 （稍微声明一下：一些大家都知道的知识我就不在此赘述了。） L0 范数和 L1 范数这两个范数都是保证参数稀疏性的所以放在一起说。 L0 范数是向量中非 0 元素的个数，这个想必大家都知道了。用它来限制稀疏性本身很好，可惜它不是凸的，求解它将成为一个 NP-hard 问题。所以我们用能够完全包络 L0 范数的一个凸包—— L1 范数来近似的代替它1。 特征稀疏的好处有以下两点： 便于特征选择。现实世界中，问题的特征的数量往往是很大的，而起决定性作用的往往是一小部分，所以我们在建立简单模型的时候，会先考虑舍弃权重快速收敛于 0 的特征。 更具可解释性。例如对于癌症预测问题，可能有上千个特征，如果主要特征只有 10 个，就可以解释为癌症的发生几乎更和这 10 个特征息息相关，其它的暂不考虑影响也不大。 L2 范数考虑到 L1 范数在顶点处是不可微的，人们又引入了 L2 范数。 关于 L2 范数如何提升模型泛化能力就不赘述了，在此讲一下它对优化计算所作出的贡献。 优化问题有两个难题，一：局部最小值，二：病态（ill-condition）问题。第一个问题很容易理解，那么第二个问题提到的病态又是什么呢？简单来说，假设有一个方程 $AX = b$ ，如果 $A$ 和 $b$ 稍微发生改变就会引起 $X$ 的巨大变化的话，我们就称这个方程组系统是病态（ill-condition）的。反之就是良态（well-condition）的。 举个例子，在人脸识别中，如果一个人粘了个假睫毛就不认识了，那就说明她的脸是“病态的”（笑）。 定义：方阵 $A$ 是非奇异的，那么 $A$ 的条件数（condition number）定义为$$\kappa(A) = \lVert A \rVert \lVert A^{-1} \rVert$$经过简单的证明（请参考本系列的附录 A），我们可以得到以下的结论$$\frac{\lVert \Delta x \rVert}{\lVert x \rVert} \le \kappa(A) \cdot \frac{\lVert \Delta b \rVert}{\lVert b \rVert} \\frac{\lVert \Delta x \rVert}{\lVert x + \Delta x \rVert} \le \kappa(A) \cdot \frac{\lVert \Delta A \rVert}{\lVert A \rVert}$$因此可以认为，condition number 描述的是一个矩阵（或它形成的线性系统）的稳定性（或敏感度）的度量。如果一个矩阵的 condition number 在 1 附近，那么它是 well-condition 的；反之，它是 ill-condition 的。 考虑线性回归的解析解$$w^{\ast} = (X^{\mathsf{T}}X)^{-1}X^{\mathsf{T}}y$$如果样本的数目比样本的维度还要小的时候，矩阵 $ X^{\mathsf{T}}X $ 将会不是满秩的，也就不可逆。但如果加上 L2 范数规则项，解就变成$$w^{\ast} = (X^{\mathsf{T}}X + \lambda I)^{-1}X^{\mathsf{T}}y$$此时，就可以直接求逆了。 另外，通常我们并不适用解析解求解，而是使用牛顿迭代法求解。此时，加入规则项的两一个好处就是它可以将目标函数变为 $ \lambda $ 强凸（$\lambda$ -strongly convex）的。 定义：当 $f$ 满足$$f(y) \ge f(x) + \langle \nabla f(x), y-x \rangle + \frac{\lambda}{2} \lVert y-x \rVert ^{2}$$时，我们称 $f$ 为 $ \lambda $ 强凸（$\lambda$ -strongly convex）的。 对比一阶泰勒展开$$f(y) \ge f(x) + \langle \nabla f(x), y-x \rangle + o (\lVert y-x \rVert)$$我们可以发现，这种强凸性质不仅仅要求函数在某条切线的上方，而且还要求函数整体在某个二次函数图像上方，即已经具备一定的“向上弯曲度”。所以这种函数的下降速度非常快，而且很稳定。 1.Ramirez, C. &amp; V. Kreinovich &amp; M. Argaez. Why l1 is a good approximation to l0: A geometric explanation [J]. Engineering, 2013, 7 (3): 203-207 ↩]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>Norm regularization</tag>
        <tag>Convex optimization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[说一说核方法（二）——数学角度简介（掉粉文）]]></title>
    <url>%2F2017%2F08%2F13%2Fabout-kernel-02%2F</url>
    <content type="text"><![CDATA[泛函分析（functional analysis）主要研究的是函数空间（function space），也就是说这个空间中所有的点（元素）都是函数。 先介绍几个概念，我们暂时只讨论由线性空间发展出来的概念，因为线性空间有很多很好的性质。 线性空间线性空间（linear space）是定义了数乘和加法的空间。所以我们可以找到一组基底（basis），然后通过这一组基底的线性组合来得到空间中的所有点。举个例子，二次函数空间，基底就可以定义为 $\lbrace 1, x, x^{2} \rbrace$ ，这个空间中任意的函数 $f(x)$ 都可以表示为 $f(x) = \alpha_{1} \cdot 1 + \alpha_{2} \cdot x + \alpha_{3} \cdot x^{2}$ ，所以我们可以说 $f(x)$ 在这组基底下的坐标是 $\lbrace \alpha_{1}, \alpha_{2}, \alpha_{3} \rbrace$ 。（请自行想象三维空间中，三个维度的坐标不是 $\lbrace x, y, z \rbrace$ 。）当然基底也可以定义为 $\lbrace 1, x+1, (x+1)^{2} \rbrace$ ，但是计算上会增大难度。所以我们可以看出来基底只要是线性不相关的就行了。那么基底中有 $n$ 个元素的话，我们就叫这个空间是 $n$ 维的。一般来讲，选用的基底都是正交基（orthogonal basis），也就是基底中的任意两个元素的内积为 0。 线性度量空间线性度量空间（metric linear space）是在线性空间中定义了距离（metric）的空间。距离的定义必须满足如下三个条件： 非负性： $d(x, y) \ge 0; d(x, y) = 0 \Leftrightarrow x = y$ 。 对称性： $d(x, y) = d(y, x)$ 。 三角不等式：$d(x, z) + d(z, y) \ge d(x, y)$ 。 线性赋范空间线性赋范空间（normed linear space）是定义了范数（norm）的线性度量空间。范数的定义必须满足： 非负性：$\lVert x \rVert \ge 0$ 。 齐次性：$ \lVert \alpha x \rVert = |\alpha| \lVert x \rVert $ 。 三角不等式：$\lVert x \rVert + \lVert y \rVert \ge \lVert x+y \rVert$ 。 由范数可以导出距离（定义 $d(x, y) = \lVert x - y \rVert$ ），但是不可以由距离导出范数。 巴拿赫空间巴拿赫空间（Banach space）是完备的赋范线性空间。完备性（completeness）：任一柯西序列（Cauchy sequence）都收敛（convergence）。 内积线性空间内积线性空间（inner product linear space）是定义了内积（inner product）的赋范线性空间。其中内积也叫标量积（scalar product）或点积（dot product）。这里要注意，内积的定义跟范数其实没关系，不过内积可以导出范数，所以一般内积空间都有范数。内积的定义必须满足： 对称性：$\langle x, y \rangle = \langle y, x \rangle$ 。 线性性： $\langle x, y \rangle + \langle x, z \rangle = \langle x, y + z \rangle$ 、 $\langle \alpha x, y \rangle = \alpha \langle x, y \rangle$ 。注意：数乘只对第一变元有效。 正定性：$\langle x, x \rangle \ge 0$ 。 由内积可以导出范数（定义 $\lVert x \rVert^{2} = \langle x, x \rangle$ ），但是范数不可以导出内积。 欧几里得空间欧几里得空间（Euclidean space）是有限维的实内积线性空间。 希尔伯特空间希尔伯特空间（Hilbert space）是完备的内积线性空间。 两个例子 泰勒级数展开（Taylor series）：将一个函数用 $\lbrace x^{i} \rbrace_{0}^{\infty}$ 作为基底表示的一个空间。 傅里叶级数展开（Fourier series）：将一个函数用 $\lbrace 1, \cos x, \sin x, \cos 2x, \sin 2x, \cdots \rbrace$ 作为基底表示的一个空间。 核函数终于到主题了，写的手都酸了…… 一般的欧式空间中，我们可以定义一个 $n \times n$ 矩阵的特征值和特征向量。 Ax=\lambda x考虑一个矩阵的列空间，当这个矩阵可以进行特征值分解的时候，其特征向量就构成了这个 $n$ 维空间的一组基底。 现在我们把这个概念推广到函数空间。 我们把每个函数 $f(x)$ 看作一个无穷维的向量，然后定义一个函数空间中无穷维的矩阵 $K(x, y)$ ，如果它满足： 正定性：$\forall f \rightarrow \iint f(x)K(x, y)f(y) \,\mathrm{d}x\,\mathrm{d}y \ge 0$ 。 对称性：$K(x, y) = K(y, x)$ 。 我们就把它称作核函数（kernel function）。 和特征值与特征向量的概念相似，存在特征值 $\lambda$ 和特征函数 $\psi(x)$ 。满足： \int K(x, y)\psi(x) \mathrm{d}x = \lambda \psi (y)对于不同的特征值 $\lambda_{1}$ 、 $\lambda_{2}$ ，对应不同的特征函数 $\psi_{1}(x)$ 、 $\psi_{2}(x)$ ，很容易得到： \begin{aligned} \int \lambda_1 \psi_{1}(x) \psi_{2}(x) \mathrm{d}x &= \iint K(y, x) \psi_{1}(y) \mathrm{d}y\,\psi_{2}(x)\mathrm{d}x \\ &= \iint K(y, x) \psi_{2}(x) \mathrm{d}x\,\psi_{1}(y)\mathrm{d}y \\ &= \int \lambda_2 \psi_{2}(y) \psi_{1}(y) \mathrm{d}y \\ &= \int \lambda_2 \psi_{2}(x) \psi_{1}(x) \mathrm{d}x \end{aligned}因此， \langle \psi_{1}, \psi_{2} \rangle = \int \psi_{1}(x) \psi_{2}(x) \mathrm{d}x = 0所以我们找到了一个可以生成这个空间的矩阵 $K$，一组无穷多个特征值 $\lbrace \lambda_{i} \rbrace_{i=1}^{\infty}$ ，和一组无穷多个元素的正交基 $\lbrace \psi_{i} \rbrace_{i=1}^{\infty}$ 。 再生核希尔伯特空间如果我们把 $\lbrace \sqrt{\lambda_{i}}\psi_{i} \rbrace_{i=1}^{\infty}$ 当成一组正交基来生成一个希尔伯特空间 $\mathcal{H}$ 。则该空间中的所有函数都能表示为这组正交基的线性组合。 f = \sum_{i=1}^{\infty} f_{i}\sqrt{\lambda_{i}}\psi_{i}于是我们就可以把函数 $f$ ，看作 $\mathcal{H}$ 中的一个向量 $f = (f_{1}, f_{2}, \cdots)_{\mathcal{H}}^{\mathsf{T}}$ 。对于另外一个函数 $g = (g_{1}, g_{2}, \cdots)_{\mathcal{H}}^{\mathsf{T}}$ ，我们有： \langle f, g \rangle_{\mathcal{H}} = \sum_{i=1}^{\infty}f_{i}g_{i}有了这个内积，我们就可以把核函数看成一种内积形式了，即： K(x, \cdot) = \sum_{i=0}^{\infty} \lambda_{i}\psi_{i}(x)\psi_{i}(\cdot)如果把 $\psi_{i}$ 当成一个算子来看的话，我们就取函数名的一个形式：$K(x, \cdot) = \sum_{i=0}^{\infty} \lambda_{i}\psi_{i}(x)\psi_{i}$ 。所以我们就可以把 $K$ 当作一个向量来看了。 K(x, \cdot) = (\sqrt{\lambda_{1}}\psi_{1}(x), \sqrt{\lambda_{2}}\psi_{2}(x),\cdots)_{\mathcal{H}}^{\mathsf{T}}因此， \langle K(x, \cdot), K(y, \cdot) \rangle_{\mathcal{H}} = \sum_{i=0}^{\infty} \lambda_{i} \psi_{i}(x) \psi_{i}(y) = K(x, y)这个性质就叫再生性（reproducing），这个 $\mathcal{H}$ 就叫做再生核希尔伯特空间（reproducing kernel Hilbert space，RKHS）。 回到我们最初的问题，怎么把一个点映射到一个特征空间上呢？ 定义一个映射： \Phi(x) = K(x, \cdot) = (\sqrt{\lambda_{1}}\psi_{1}(x), \sqrt{\lambda_{2}}\psi_{2}(x),\cdots)_{\mathcal{H}}^{\mathsf{T}}则 \langle \Phi(x), \Phi(y) \rangle_{\mathcal{H}} = \langle K(x, \cdot), K(y, \cdot) \rangle_{\mathcal{H}} = K(x, y)虽然我们不知道这个映射的具体形式是什么，但是我们可以知道对于一个对称的正定函数（矩阵） $K$ ，一定存在一个映射 $\Phi$ 和一个特征空间 $\mathcal{H}$ ，使得 \langle \Phi(x), \Phi(y) \rangle_{\mathcal{H}} = K(x, y)这就叫做核方法（kernel trick）。 所以为什么一个核函数都对应一个正定矩阵呢，就是因为它把核函数看成张成某个 RKHS 的空间的一组基底的线性组合。 在 SVM 中的应用简单说几句，公式太难写了（笑）。 我们在使用原始数据 $x$ 的时候发现数据并不可分，所以就寄希望于一个映射 $\Phi(x)$ ，这个映射把低维空间上的数据映射到高维空间，这样数据集就有可能变得可分了。 但是在考虑优化问题的对偶问题时，需要计算 $\langle x_{i}, x_{j} \rangle$ ，请注意到，我们已经把所有的 $x$ 换成了 $\Phi(x)$ ，所以就变成需要计算 $\langle \Phi(x_{i}), \Phi(x_{j}) \rangle$ 。为了不让计算变得很困难，我们就可以找到一个核函数 $K$ ，满足 $K$ 可以生成 $\Phi$ 所形成的高维空间，这样 $\langle \Phi(x_{i}), \Phi(x_{j}) \rangle$ 就可以简单的用 $K(x_{i}, x_{j})$ 代替了。而 $K$ 往往定义成和 $x$ 的内积有关的式子，这样在低维空间中计算内积就很简单。 如：径向基函数里有 $\lVert x - y \rVert^{2}$ ，展开以后其实就含有两个范数项（注意范数就是内积）和一个内积项。]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>Kernel</tag>
        <tag>RKHS</tag>
        <tag>Functional analysis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[说一说核方法（一）——核方法与核函数简介]]></title>
    <url>%2F2017%2F08%2F13%2Fabout-kernel-01%2F</url>
    <content type="text"><![CDATA[本文由好友史博士点播写。 昨日史博士忽然来问我博客可以点播主题吗？大家都知道我这个人一沾数学就是兴奋的，于是就同意了。因此史博士就提到了对于核函数（kernel function）这个东西不容易说明白的话题，还有关于核函数是否就是将一个低维映射到高维的问题。 相信很多人都对这个存在一些问题，原因是很多大牛在讲课的时候都是只讲果不讲因的，所以让初学者吃了很多亏，不知道核（kernel）这个东西是一个独立概念。这个东西大概的确（这里使用了鲁迅体）也没法在机器学习课程中讲解，因为需要涉及到一些泛函知识。那么，这些知识将在本系列的第二篇——“掉粉”文中进行讲解（因为是纯数文）。 这篇文章的话，说以下几个事情： kernel 这个东西在 SVM 中真的只是一少部分，为了面试的话大概了解一下就可以了，面试官也很少有懂的。并不是说大家不爱学习，而是做 kernel 的人根本不关心 SVM 是什么，做 SVM 的人也根本不用关心 kernel 是个什么鬼。 kernel 和 SVM 是两个完全没有关系的概念。实际上在 SVM 提出以前，人们就提出了再生核希尔伯特空间（reproducing kernel Hilbert space，RKHS）这个概念，并且把它应用在信号处理中。如：在信号检测（signal detection）问题中，对于一条时间序列（time series），我如何知道它是一个随机步行（random walk）的噪音序列呢？还是有一个特定的模式（pattern）在里面呢？在这个情景下，RKHS 理论就给出了一个通过求解似然率（likelihood ratio）的假设检验方案，其中的 kernel 是某个随机过程在两个不同时间点的相关性（correlation）。 另外，核方法可以用在 逻辑斯谛回归（logistic regression）、最小二乘法（least square）、降维（dimension reduction）等多处地方，也不是只和 SVM 这个概念绑定的。 很多人觉得 kernel 定义了一个从低维到高维的映射，这是不准确的。首先不是所有空间都有维度定义，比如高斯核（Gaussian kernel）——也称径向基函数（radial basis function，RBF 核）就把低维映射到了无穷维，无穷维实际上是不知道多少维的（虽然确实也是映射到了高维），所以如果强调不同的维度就是不同的空间的话，无穷维时就无法区分不同的 RKHS 之间有什么不同了。 那么这个映射是什么呢？它其实描述的是一个跟内积有关的东西。有点像是在说：如果我有一个维度很高的内积空间，那么我能找到一个映射 $\Phi : X \to \mathcal{H}, \Phi(x) = K(x, \cdot)$ （其中 $\mathcal{H}$ 是某个 RKHS 空间），它可以把这个空间中的点 $x$ 映射成为一个函数（请想象这个 RKHS 空间是由函数们组成的空间，里面的每一个点，或者说每一个元素，都是一个函数），这样，在计算高维内积时就有 $&lt;\Phi(x), \Phi(y)&gt;_{\mathcal{H}} = K(x, y)$ ，就转变成了计算核函数的值了。（我仿佛已经听到了掉粉的声音，我本打算在第二篇再写数学的，可是不写不清楚。写了更不清楚。） 正确的想法是什么？事实上，我们一开始要的不是核函数，而是一个简单的映射。这个映射负责把低维映射到高维，原因是我们的数据在低维上可能是不可分的，而到了高维中就可以。 但是我们在选这个映射时有一个条件就是“我不想算高维空间中很复杂的内积”。这个时候我们才看中了核函数，因为有一些核函数可以把低维映射到高维，并且高维的内积可以很简单的用低维的内积表示。]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>Kernel</tag>
        <tag>SVM</tag>
        <tag>RKHS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[由 Softmax 想到的（三）—— 从 Sigmoid 到 ReLU，一些常见的激活函数]]></title>
    <url>%2F2017%2F08%2F11%2Fthinking-from-softmax-03%2F</url>
    <content type="text"><![CDATA[承接上文，提到 Logistic 模型，就会想到那个典型的 Sigmoid 函数。其实 Sigmoid 本来就是“ S 形”的意思，不过似乎大家一说到 Sigmoid 函数就想起 Logistic 模型的那个函数。该函数一般被记做 $\sigma(x)$ ：$$\sigma(x) = \frac{1}{1+e^{-x}}$$这个函数有一些很好的性质，比如： 它是连续并且光滑的，因此是可导的。这意味它在每一个点都可以求梯度。 它的定义域是 $[-\infty, +\infty]$ 。 它的值域是 $(0, 1)$ ，虽然取不到两边的端点，但是可以在无穷远处无限趋近。把实数集映射到 $(0, 1)$ ，刚好满足概率的定义。 “完美”的函数。 但在被引入神经网络中担负起激活函数的职责后，似乎出了一些问题。 为什么需要激活函数？非常遗憾的是并不是所有的优化问题都是凸优化问题，乃至于是否是线性问题就更谈不上了。相反，人类所遇到的大部分优化问题都是非线性、非凸的。于是人们模拟神经元的样子发明了神经网络（neural nets）。而在神经网络的各个隐藏层间担任粘合作用的就是激活函数（activation function）。 究竟激活函数要设计成什么样子呢？ 可微性。因为我们想要求导、想要梯度。特别是在向后传播神经网络中一定是要可导的。 单调性。激活函数如果是单调的，就能保证单层网络是凸的。凸的很好，有全局最值。 输出值范围有限。基于梯度的优化方法会更加稳定。当激活函数的输出是无限的时候，模型的训练会更加高效，但是一般需要小心翼翼地设定合适的更小的学习率（learning rate）。 如果不考虑输出值范围。满足这些条件的一个平凡例子是 $f(x) = x$ ，但是这样不能让两层网络逼近非线性函数。于是人们想到了 Sigmoid 函数。 Sigmoid 函数的问题最近几年，使用 Sigmoid 函数的人越来越少了，是因为它有一个致命的缺点，即当输入非常大或者非常小的时候（也称达到饱和，saturation），这些神经元的梯度是接近 0 的。所以这需要尤其注意参数的初始值设定，因为如果初始值很大，大部分神经元可能都会处在饱和的状态，从而“杀死”梯度（kill gradient）。这会导致神经网络变得很难学习。事实上，由于：$$\sigma(x)’ = \sigma(x)(1 - \sigma(x))$$我们可以了解到，Sigmoid 函数的梯度的最大值也才是 $1/4$ 。如果放在向后传导网络中，运用链式法则多乘几层隐藏层，那么得到的偏导数大概要趋近于 0 了。 另外一个小问题是，Sigmoid 函数并不是 0 均值的，所以正输入所计算出的的梯度也一定是正的（就是 1）。当然，如果使用批训练（batch），能缓解一下这个问题。比 kill gradient 要好对付一点。 不很科学的说，我们似乎也不需要在神经网络的每一层都产生一个类似于概率的东西。神经网络的每一层应该起到为下一层提供新的抽象特征（feature）的作用，特别是这些特征如果能有一定的稀疏性就好了。（这里稍微啰嗦一句，神经网络本身就是一个交织在一起的结构，再加入 Dropout ，其实是采用了随机森林（random forest）的 Bagging 思想的优点。） 其它的激活函数呢？ tanh 这个函数解决了 0 均值的问题。不过看图像就知道了，它是 Sigmoid 函数的变体。 ReLU 数学表达式：$f(x) = \max(0, x)$ 。 近几年，ReLU作为激活函数越来越火，因为： 它是线性的，而且是不饱和的。 运算简单，计算一个阈值即可。 梯度形式简单，0 或者 1。原点处的话定义一个次梯度就好。 虽然训练时很脆弱，非常大的梯度流过一个 ReLU 单元后很轻易就会“die”掉。但是反过来想，这个问题只要设定一个合适的比较小的学习率就可以让它发生的不太频繁。同时它还可以为每一层的特征提供一定的稀疏性（负输入将得到 0）。 Leaky-ReLU、P-ReLU、R-ReLU、Maxout 请自行 wiki。这些函数其实都是 ReLU 的变体，看名字就知道。它们被设计出来解决“dying ReLU”的问题。但具体效果却未有定论。总之，都是一些实验表明它们表现的很好，一些实验又不是这样。大概是天下没有免费的午餐（No free lunch theoren）吧。 毕竟神经网络就是炼丹。大家都懂的。 这里要提一句，Maxout 函数的拟合能力还是很强的，两个 Maxout 节点就可以拟合任意的凸函数了（相减），前提是“隐”隐藏层的节点个数可以任意多。所以 Maxout 函数是又有 ReLU 的优点，又没有 ReLU 的缺点。如果你没有见过别人用这个大概是因为它唯一的缺点：它会把参数个数翻倍。（所以一般在使用的时候，都和 Dropout 结合。） 还有很多的激活函数可供选择就不一一赘述了。 怎么选择激活函数呢？首先说这种问题不可能有标答的，我的实战经验也不是很多，算是说一点个人看法。 如果使用 ReLU ，那么一定要小心设置学习率。如果“dying ReLU”的问题不好解决，试一试 Leaky ReLU、PReLU 或者 Maxout。 另外，个人认为：最好不要用 Sigmoid，但可以试试 tanh。 还有，很少有把各种激活函数串起来放在同一个网络中的，那其实并没有多大意义。]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>Deep learning</tag>
        <tag>Activation function</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[由 Softmax 想到的（二）—— Logistic 回归的 Loss 函数为什么长成那个样子？]]></title>
    <url>%2F2017%2F08%2F10%2Fthinking-from-softmax-02%2F</url>
    <content type="text"><![CDATA[Logistic 回归的 Loss 函数为什么长成那个样子？一般人大概都能想得到，Logistic 回归的 Loss 函数应该是下面的样子。（以下是正常的版本。） \begin{aligned} Loss(h_{\theta}, \, y) &=\frac{1}{n} \sum_{i=1}^{n} \frac{1}{2}[h_{\theta}(x_{i})-y_{i}]^{2} \\ & =\frac{1}{2}\left(\frac{1}{1+e^{-\theta^{\mathsf{T}}x}}-y\right)^{2} \end{aligned}这里 $1/2$ 是为了求导后方便计算的系数，不重要。$n$ 是样本数量。括号里面就是分错的部分了，很容易理解。 但实际上，它的 Loss 函数长成了这个样子： Loss = \left \{ \begin{aligned} & -\log (h_{\theta}(x)) & \qquad \text{if }y=1 \\ & -\log (1-h_{\theta}(x)) & \qquad \text{if }y=0 \end{aligned} \right.为什么呢？ 答案是：保证凸性。第一个 Loss 函数不是凸函数，而取过了 $-\log$ 并且分段以后就变成凸函数了。 凸优化 及 梯度下降法几乎每一个机器学习问题都是一个优化问题。而在所有优化问题当中，有一类优化问题是人们非常关心的，并希望每一个优化问题都能转化为这样的优化问题的，那就是凸优化问题（convex optimization）。 一类常见的凸优化问题就是目标函数是凸函数，并且边界条件是仿射集的优化问题。如：支持向量机（SVM） 、 感知机（Perceptron）等等。为什么要研究凸优化呢？因为凸函数有一个优点——全局最小值。它可以让人们明确一个信念，即：所研究的问题一定是可学习的，而且一定在全局有一个固定结果，并不会踩到某个局部极值从而达不到最优。 那么如何求得这个最值呢，一般的方法是利用梯度下降法（gradient descent，GD）。由于凸函数有一个一阶条件：$f(y) \ge f(x) + \nabla f(x)^{\mathsf{T}}(y-x)$ ，所以我们很容易看到，沿着目标函数 $f$ 的梯度方向一直下降的话（其实是梯度方向的反向，因为要下降），终究会像从沙滩上滑下一般到达最小值。（顺便一说凸函数还有一个二阶条件：$\nabla^{2}f(x) \ge 0$ 。） 那么，问题来了，如果目标函数不是光滑的怎么办？比如 $y=|x|$ ，这很明显是一个凸锥。再比如，神经网络（neural net，NN）中最常见的激活函数（activation Function）：ReLU，这也是一个凸锥。 解决方法是提出次梯度（subgradient）的概念。所谓次梯度，是一个很像梯度的，但是是个常量的 $g$ ，它满足：$f(y) \ge f(x) + g^{\mathsf{T}}(y-x)$ 。形式上是不是和凸函数的一阶条件很像？因此我们实际上是找了一个很像梯度的替代品。例如：在 $y=|x|$ 的条件下，我们可以把原点的次梯度定义为 0。在ReLU的条件下，我们把原点的次梯度定义为 $1/2$​​ 等等。样子如下图中的红线的斜率： （原图来自 百度百科：次导数） 扩展：Softmax 模型的 Loss 函数—— Log-Sum-Exp 函数这里就不给出函数的形式了，大家请自行 wiki。总之来说，是 Logistic 模型的 Loss 函数的高阶扩展。也是一个带有一堆 $\log$ 、$\exp$ 还要求加和的式子。 它的凸性可以证明如下： 首先，我们考虑函数 $f(x) = \log (\sum \alpha_{i} e^{x_{i}})$ ，我们证明它是凸的。然后根据凸函数的性质 Affine composition 来导出 Log-Sum-Exp 函数是凸的。（Affine composition 指的是：$f(x)$ 是凸的 $\Leftrightarrow f(Ax+b)$ 也是凸的，即对自变量做仿射变换不影响凸性。） 考虑： \begin{aligned} \nabla_{i} f & = e^{x_{i}}\Big/\sum_{k} e^{x_{k}} & \\ \nabla_{i}^{2} f & = \mathrm{diag}(z)-zz^{\mathsf{T}}, \, z_{i} = e^{x_{i}}\Big/\sum_{k} e^{x_{k}} \end{aligned}显而易见，$\nabla_{i}^{2} f$ 是对角优势矩阵（diagonally dominant matrix）。（对角优势矩阵：对矩阵 $A$ 的所有对角元，有 $A_{ii} \ge \sum_{i \ne j} |A_{ij}|$ 。）因此，$\nabla_{i}^{2} f$ 是半正定的（positive semidefinite matrix，p.s.d）。所以得到 Log-Sum-Exp 函数是凸的。]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>Convex optimization</tag>
        <tag>Logistic</tag>
        <tag>Softmax</tag>
        <tag>Gradient</tag>
        <tag>Subgradient</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[由 Softmax 想到的（一）——广义线性模型]]></title>
    <url>%2F2017%2F08%2F10%2Fthinking-from-softmax-01%2F</url>
    <content type="text"><![CDATA[昨日被好友史博士问了个问题：为什么是 Softmax？Softmax到底是怎么来的？于是脑洞大开，写几篇博客把广义线性模型、梯度、次梯度、激活函数这几块有一些关联的概念大概串一下。 广义线性模型 及 指数分布族本质上，线性模型、Logistic 模型和 Softmax 模型都是由一个东西推理出来的。就是广义线性模型（Generalized Linear Model）。这些分布之所以长成这个样子，是因为我们对标签的分布进行了假设。 标签是连续的（正态分布，由中心极限定律所得）——线性模型。 标签是二分类（多次观察可看做是二项分布）—— Logistic 模型。 标签是多分类（多次观察可看做是多项分布）—— Softmax 模型。 另外，只要 $y$ 的分布是指数分布族（exponential family distribution）的，都可以用一种通用的方法推导出 $h(x)$ 。 广义线性模型的定义： 给定特征属性 $x$ 和参数 $\theta$ 后，$y$ 的条件概率 $P(y \mid x; \, \theta)$ 服从指数分布族。 预测 $T(y)$ 的期望，即计算 $E[T(y) \mid x]$ 。 $\eta$ 与 $x$ 之间是线性的，即 $\eta=\theta^{\mathsf{T}}x$ 。 指数分布的形式如下：$$P(y; \, \eta) = b(y) \ast \exp(\eta^{\mathsf{T}}T(y)-a(\eta))$$ 二项分布—— Logistic 模型的意义伯努利分布（Bernoulli distribution），也叫 0-1 分布，如果实验多次就变成了二项分布（binomial distribution）。所以二项分布也叫 $n$ 重伯努利分布，或者 $n$ 重伯努利实验。概率密度函数为：$P(y; \, \phi) = \phi^{y}(1-\phi)^{1-y}$ 。把它变为指数分布族的形式：$P(y; \, \phi) = \exp(y\ln \frac{\phi}{1-\phi}+\ln(1-\phi))$ 。对比指数分布族，有：$\eta=\ln \frac{\phi}{1-\phi} \Rightarrow \phi=\frac{1}{1+e^{-\eta}}$ 。根据广义线性模型的第三点的线性关系。可得：$$\phi = \frac{1}{1+e^{-\theta^{\mathsf{T}}x}}$$所以我们用 Logistic 函数来估计伯努利分布的参数。直观的想就是我们估计一个东西的表现更像 0 还是更像 1，而不是准确的等于哪一个。 多项分布—— Softmax 模型的意义多项分布（multinomial distribution）也是 $n$ 重实验，只不过每次不是伯努利分布，而是有 $k$ 个选择的“伯努利分布”。跟推导 Logistic 模型的手法相似，将多项分布化为指数分布族，在考虑线性关系，可得：$$P(y^{(i)}=j \mid x^{(i)}; \, \theta)=\frac{e^{\theta_{j}^{\mathsf{T}}x^{(i)}}}{\sum_k e^{\theta_{k}^{\mathsf{T}}x^{(i)}}}$$注意在推导中有一个小技巧，就是 Softmax 模型总是有一个“冗余”的参数，道理很简单，如果我们有 $k$ 个分类，并且一个东西的表现不太像某 $k-1$ 类的话，那必然猜测它属于第 $k$ 类。 另外，当 $k=2$ 时，Softmax 模型退化为 Logistic 模型。]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>Generalized Linear Model</tag>
        <tag>Exponential family distribution</tag>
        <tag>Logistic</tag>
        <tag>Softmax</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈 PCA 和 LFM 的异同]]></title>
    <url>%2F2017%2F08%2F08%2Fcompare-between-PCA-and-LFM%2F</url>
    <content type="text"><![CDATA[在机器学习中有两种算法非常相似，却又很不同。它们就是“PCA（Principal Component Analysis）”和“LFM（Latent Factor Model）”。许多人都知道，PCA 常用于数据的降维，而接触到 LFM 最多的地方就是大家都知道的推荐系统。事实上，这两种方法都涉及到矩阵的分解，也都是为了得到数据中隐含的信息。（说到矩阵分解， PCA 是特征值分解；而 LFM 经常是被分解为两个矩阵的乘积，如 user-item 矩阵分解。） 下面详细讲一下这两个算法的不同点。 PCA PCA的目的是尽可能简化数据，即当数据维度 $p​$ 很大时，我们希望能够以一个更小维度的 $q​$ 维数据来代替它，而又不损失原始数据的随机程度，或者说是想要保持原始数据的分散程度。即，希望找到能够代替原始数据 $\lbrace X_{1}, X_{2}, \cdots, X_{p} \rbrace$ 的一组数据 $\lbrace Y_{1}, Y_{2}, \cdots, Y_{q} \rbrace$ ，使得$$Y_{i} = \sum_{j=1}^{p} \alpha_{j}^{(i)} X_{j}$$由上式的线性关系可知，PCA可以看做是相关性（correlation） 的推广。我们从视觉上给随机变量所展示出来的数据一个分散程度的定义——“波动”。那么 correlation 就可以说，某两个随机变量的涨落相似，如果它们的相关性系数很高。 同样，如果将某个 $Y_{i}$ 分解为 $X_{j}$ 的线性组合时，某些 $\alpha_{j}^{(i)}$ 很大（而其他的 $\alpha_{j}^{(i)}$ 接近 $0$ 甚至等于 $0$ ），那么我们就可以认为 $Y_{i}$ 与这些 $X_{j}$ 共沉浮，即相关性很高。当我们尽可能的排除掉那些 $\alpha_{j}^{(i)} $ 接近 $0$ 的维度时，对于剩下的维度，我们就可以说：原始的 $p$ 维数据的“波动”，大部分都反映在这个 $q$ 维的子空间里。 因此，PCA 所给出的 $q$ 个特征向量（eigenvector）就是描述这个“波动”的 $q$ 个主方向，而 $q$ 个特征值（eigenvalue）就是描述这个“波动”在这 $q$ 个主方向上波动的幅度。 所以，PCA 才被用来降维，也就是我们用它来关注数据“最乱”的几个方向，其它的方向变化不大，不重要。 LFM 目标：解释 $\lbrace X_{1}, X_{2}, \cdots, X_{p} \rbrace$ 之间的相关性，即希望找到一些 $Z_{k}$，有$$\forall X_{i}, \text{ 能找到一系列 } \beta_{k}^{(i)}, \text{ 使得 } X_{i} = \sum_{k=1}^{r} \beta_{k}^{(i)} Z_{k}$$一般，假设 $\lbrace Z_{1}, Z_{2}, \cdots, Z_{r} \rbrace$ 是零均值、单位方差且不相关的。（注意这里，所有的 $\beta^{\mathsf{T}}$ 就有点像 user 矩阵，所有的 $Z$ 就有点像 item 矩阵。所以 user-item 矩阵分解是LFM的一种应用。） 同时，对于不同的 $X$ ，他们的那些 $\beta$ 越相似，这些 $X$ 就越相似。这也是我们能用它来判定有哪些用户相似，或者有哪些商品相似的原因。 ​ 结论PCA：归纳。找出分散的根源。LFM：分解。将随机变量分解为同一组变量的线性组合，来试图解释它们相关性的来源。]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>PCA</tag>
        <tag>LFM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【机器不太会学习】的第一篇博文]]></title>
    <url>%2F2017%2F08%2F07%2Finit-blog%2F</url>
    <content type="text"><![CDATA[我的博客【机器不太会学习】也要开始慢慢写作了。 最早被带入机器学习领域，还是要感谢我的好友——史博士。（他的博客都是干货，很干，百年大旱那种干。）进入这一领域以后，又把自己的看家本领——数学，给拾起来了。让我对自己喜欢的东西更增添了一份热爱。 在学习机器学习的过程中，我有幸遇到了我女朋友。她是一个极好的女人，是一个我以后想娶她的女人。正是她在我想换做这方面的工作的时候，支持我说：“只要是你喜欢做的事情，不管怎么样，我都支持你。”作为一个男人遇到这样的女人还能怎么样呢？当然是娶她，然后给她幸福。 以后的博文想来可能会偏向理论吧，我的应用做的并不是很好。想来估计也没有什么人看，就算是自己的日记吧，通往AI之国的某条道路上“旅行日记”。]]></content>
      <categories>
        <category>Life</category>
      </categories>
  </entry>
</search>
