{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/README.md","path":"README.md","modified":0,"renderable":0},{"_id":"source/CNAME","path":"CNAME","modified":0,"renderable":0},{"_id":"source/baidu_verify_aX3upgcPux.html","path":"baidu_verify_aX3upgcPux.html","modified":0,"renderable":0},{"_id":"source/robots.txt","path":"robots.txt","modified":0,"renderable":0},{"_id":"source/images/linear_space.eddx","path":"images/linear_space.eddx","modified":0,"renderable":0},{"_id":"source/uploads/me.jpeg","path":"uploads/me.jpeg","modified":0,"renderable":0},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/bower.json","path":"lib/fastclick/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","path":"lib/canvas-ribbon/canvas-ribbon.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/LICENSE","path":"lib/fastclick/LICENSE","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/README.md","path":"lib/fastclick/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","path":"lib/jquery_lazyload/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","path":"lib/jquery_lazyload/CONTRIBUTING.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","path":"lib/jquery_lazyload/jquery.lazyload.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","path":"lib/jquery_lazyload/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","path":"lib/jquery_lazyload/jquery.scrollstop.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","path":"lib/pace/pace-theme-big-counter.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","path":"lib/pace/pace-theme-bounce.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","path":"lib/pace/pace-theme-barber-shop.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","path":"lib/pace/pace-theme-center-atom.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","path":"lib/pace/pace-theme-center-circle.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","path":"lib/pace/pace-theme-center-radar.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","path":"lib/pace/pace-theme-center-simple.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","path":"lib/pace/pace-theme-corner-indicator.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","path":"lib/pace/pace-theme-fill-left.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","path":"lib/pace/pace-theme-flash.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","path":"lib/pace/pace-theme-loading-bar.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","path":"lib/pace/pace-theme-minimal.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace.min.js","path":"lib/pace/pace.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","path":"lib/pace/pace-theme-mac-osx.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","path":"lib/three/canvas_lines.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","path":"lib/three/canvas_sphere.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/three-waves.min.js","path":"lib/three/three-waves.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/bower.json","path":"lib/velocity/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","path":"lib/fancybox/source/blank.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","path":"lib/fancybox/source/fancybox_loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","path":"lib/fancybox/source/fancybox_overlay.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","path":"lib/fancybox/source/fancybox_loading@2x.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","path":"lib/fancybox/source/fancybox_sprite.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","path":"lib/fancybox/source/fancybox_sprite@2x.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.css","path":"lib/Han/dist/han.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","path":"lib/fastclick/lib/fastclick.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","path":"lib/fastclick/lib/fastclick.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.css","path":"lib/Han/dist/han.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.js","path":"lib/Han/dist/han.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.js","path":"lib/Han/dist/han.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","path":"lib/fancybox/source/helpers/fancybox_buttons.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","path":"lib/fancybox/source/helpers/jquery.fancybox-media.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","path":"lib/Han/dist/font/han-space.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","path":"lib/Han/dist/font/han-space.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","path":"lib/Han/dist/font/han.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","path":"lib/Han/dist/font/han.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/three.min.js","path":"lib/three/three.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":0,"renderable":1}],"Cache":[{"_id":"source/README.md","hash":"876c607060f4a864761fd1d8cc982edba012f251","modified":1626699811695},{"_id":"source/CNAME","hash":"685ff10641d5fffd71eec3979dab6fd0f10c9ea7","modified":1626699811695},{"_id":"source/baidu_verify_aX3upgcPux.html","hash":"344502cc0cb9646f0643ff932d5c7adb1438cd5f","modified":1626699811699},{"_id":"source/robots.txt","hash":"7858025995a22727d626cdda854f061df5ba8fb9","modified":1626699811699},{"_id":"themes/next/.bowerrc","hash":"3228a58ed0ece9f85e1e3136352094080b8dece1","modified":1626699811721},{"_id":"themes/next/.gitattributes","hash":"44bd4729c74ccb88110804f41746fec07bf487d4","modified":1626699811721},{"_id":"themes/next/.gitignore","hash":"32ea93f21d8693d5d8fa4eef1c51a21ad0670047","modified":1626699811722},{"_id":"themes/next/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1626699811721},{"_id":"themes/next/.hound.yml","hash":"b76daa84c9ca3ad292c78412603370a367cc2bc3","modified":1626699811722},{"_id":"themes/next/.javascript_ignore","hash":"8a224b381155f10e6eb132a4d815c5b52962a9d1","modified":1626699811723},{"_id":"themes/next/.jshintrc","hash":"9928f81bd822f6a8d67fdbc909b517178533bca9","modified":1626699811723},{"_id":"themes/next/.stylintrc","hash":"b28e24704a5d8de08346c45286574c8e76cc109f","modified":1626699811723},{"_id":"themes/next/LICENSE","hash":"f293bcfcdc06c0b77ba13570bb8af55eb5c059fd","modified":1626699811723},{"_id":"themes/next/.travis.yml","hash":"c42d9608c8c7fe90de7b1581a8dc3886e90c179e","modified":1626699811723},{"_id":"themes/next/README.en.md","hash":"32d6cdfec1447f54aae1d7f1365ce6733dfcec8f","modified":1626699811723},{"_id":"themes/next/README.md","hash":"500b5606eb6a09c979d16128f8b00f4bf9bc95ac","modified":1626699811724},{"_id":"themes/next/gulpfile.coffee","hash":"031bffc483e417b20e90eceb6cf358e7596d2e69","modified":1626699811724},{"_id":"themes/next/package.json","hash":"193dad6f59a588908fac082cc46fe067dac1b84d","modified":1626699811740},{"_id":"themes/next/bower.json","hash":"7d7938f9da896fe710aa0e9120140e528bf058df","modified":1626699811724},{"_id":"themes/next/_config.yml","hash":"a42a99eb9d8a522e19e9eac34e15109f1a4162f1","modified":1638608678975},{"_id":"source/_posts/5-steps-for-asking-good-questions.md","hash":"efa55e28d49e4fdf225049831531d2d8c0b74b1a","modified":1626699811695},{"_id":"source/_posts/GAN-01.md","hash":"0e85b1c8c4c1ee9252b919c46446de5926c22535","modified":1626699811695},{"_id":"source/_posts/GAN-02.md","hash":"dcae32a27b5391dc22a6b24f17e9db14b291cfe7","modified":1626699811695},{"_id":"source/_posts/GD-Series.md","hash":"ea008fd53dff233ccbf385e91796d273525679a9","modified":1626699811695},{"_id":"source/_posts/about-kernel-01.md","hash":"2661ad052f1bc224bb37a3bdcc48db3caddfa4f9","modified":1626699811696},{"_id":"source/_posts/alterable-vs-permanent.md","hash":"eabae0c31231edc870b3a7a2cb0e5e2b07a784bb","modified":1626699811696},{"_id":"source/_posts/common-ground-of-mse-and-cee-in-nn.md","hash":"49be8210766564efa21a4943cf3ed6c27cf391c5","modified":1626699811696},{"_id":"source/_posts/compare-between-PCA-and-LFM.md","hash":"9fc369ce6dcbee1c3277392a2a44b53168cc878e","modified":1626699811696},{"_id":"source/_posts/discriminative-or-generative-model.md","hash":"d46e0751660e31f94e5471485fef2b0512f72395","modified":1626699811696},{"_id":"source/_posts/from-boost-to-Adaboost-to-GBT.md","hash":"b5eed46a2eac5907759b948534af50b227c0ed77","modified":1626699811697},{"_id":"source/_posts/init-blog.md","hash":"a601cdb073a44c5a819e5e9670641223ff4c07cd","modified":1626699811697},{"_id":"source/_posts/kmeans-is-a-gmm.md","hash":"badb8938705da8a504e13bbcebdc24cc25db6367","modified":1626699811697},{"_id":"source/_posts/norm-regularization-01.md","hash":"568be0ec8bcc3a353a03eeb1b9d2e09182c74d4c","modified":1626699811697},{"_id":"source/_posts/baby-gc.md","hash":"9b4868378ccafd9c6e5da48f88543014af696bc3","modified":1626791353794},{"_id":"source/_posts/norm-regularization-appendix.md","hash":"d5e2422500ffdf93bec04b4e55169a173ab60637","modified":1626699811697},{"_id":"source/_posts/about-kernel-02.md","hash":"fc0685b7b1bb5ca11efa1208387b3490e7bf20eb","modified":1626871199772},{"_id":"source/_posts/rating-model-considering-user-count.md","hash":"d6bc256e093c6f25312ff67d0e3821e3efaa0e30","modified":1626699811698},{"_id":"source/_posts/statistical-formulars-for-programmers.md","hash":"dbb6411bf38d79a79fffc99eeeae1474a94098db","modified":1626699811698},{"_id":"source/_posts/thinking-from-softmax-01.md","hash":"a29c7daef5bc9d2fba01ce29e79347d3713d008f","modified":1626699811698},{"_id":"source/_posts/norm-regularization-02.md","hash":"1a5c45319762c05f6feef23f6a4d03463f2534f3","modified":1626793654647},{"_id":"source/_posts/thinking-from-softmax-03.md","hash":"eb3cde57768cb935b36ac3b32dee386d5d7fdc81","modified":1626699811699},{"_id":"source/_posts/thinking-from-softmax-02.md","hash":"3794abddf3e8846af140b3f48da3d19058c69174","modified":1626871157075},{"_id":"source/tags/index.md","hash":"2cf9b26cbef3a6bbdf6a70647fe3974a83615d82","modified":1626699811699},{"_id":"source/categories/index.md","hash":"e0146dd9fb3b393359217a035341b8f26423420a","modified":1626699811699},{"_id":"source/images/linear_space.eddx","hash":"8910cfe98f066b604fbbb11ddc2b972c17164796","modified":1626699811699},{"_id":"source/_posts/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1626788335818},{"_id":"source/uploads/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1626700446399},{"_id":"source/uploads/me.jpeg","hash":"38ab8de612b4c83a8be656af7ca2b87b670c4c57","modified":1610246127375},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"b56c01cdfc6ee7ffea8a8a9fa149263f368caef6","modified":1626699811722},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"37bd0ec1d655c601946fc5f5ac2fe8ed1e529b77","modified":1626699811722},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"3b5eafd32abb718e56ccf8d1cee0607ad8ce611d","modified":1626699811721},{"_id":"themes/next/layout/_layout.swig","hash":"06b1eab2e00273e0b94bd32dc682bd92c1e0a747","modified":1626699811727},{"_id":"themes/next/layout/archive.swig","hash":"383f64deab105724fd5512371963bd9e9aafbffd","modified":1626699811738},{"_id":"themes/next/layout/page.swig","hash":"37c874cd720acf0eda8d26e063278f2b6ae8d3a6","modified":1626699811739},{"_id":"themes/next/layout/index.swig","hash":"c3762a6028d8ca79f9a07e9520ef9c612e7e193c","modified":1626699811739},{"_id":"themes/next/layout/category.swig","hash":"4472255f4a3e3dd6d79201523a9526dcabdfbf18","modified":1626699811739},{"_id":"themes/next/layout/post.swig","hash":"2d5f8d7f0a96b611e2d5a5e4d111fc17726a990f","modified":1626699811739},{"_id":"themes/next/layout/schedule.swig","hash":"d86f8de4e118f8c4d778b285c140474084a271db","modified":1626699811739},{"_id":"themes/next/scripts/merge-configs.js","hash":"13c8b3a2d9fce06c2488820d9248d190c8100e0a","modified":1626699811740},{"_id":"themes/next/scripts/merge.js","hash":"9130dabe6a674c54b535f322b17d75fe6081472f","modified":1626699811740},{"_id":"themes/next/layout/tag.swig","hash":"7e0a7d7d832883eddb1297483ad22c184e4368de","modified":1626699811740},{"_id":"themes/next/languages/en.yml","hash":"e7def07a709ef55684490b700a06998c67f35f39","modified":1626699811725},{"_id":"themes/next/languages/default.yml","hash":"4cc6aeb1ac09a58330e494c8771773758ab354af","modified":1626699811725},{"_id":"themes/next/languages/de.yml","hash":"306db8c865630f32c6b6260ade9d3209fbec8011","modified":1626699811725},{"_id":"themes/next/languages/id.yml","hash":"2835ea80dadf093fcf47edd957680973f1fb6b85","modified":1626699811725},{"_id":"themes/next/languages/ja.yml","hash":"1c3a05ab80a6f8be63268b66da6f19da7aa2c638","modified":1626699811726},{"_id":"themes/next/languages/fr-FR.yml","hash":"24180322c83587a153cea110e74e96eacc3355ad","modified":1626699811725},{"_id":"themes/next/languages/ko.yml","hash":"be150543379150f78329815af427bf152c0e9431","modified":1626699811726},{"_id":"themes/next/languages/ru.yml","hash":"1549a7c2fe23caa7cbedcd0aa2b77c46e57caf27","modified":1626699811726},{"_id":"themes/next/languages/pt-BR.yml","hash":"958e49571818a34fdf4af3232a07a024050f8f4e","modified":1626699811726},{"_id":"themes/next/languages/zh-Hans.yml","hash":"3c0c7dfd0256457ee24df9e9879226c58cb084b5","modified":1626699811726},{"_id":"themes/next/languages/zh-hk.yml","hash":"1c917997413bf566cb79e0975789f3c9c9128ccd","modified":1626699811726},{"_id":"themes/next/languages/pt.yml","hash":"36c8f60dacbe5d27d84d0e0d6974d7679f928da0","modified":1626699811726},{"_id":"themes/next/languages/zh-tw.yml","hash":"0b2c18aa76570364003c8d1cd429fa158ae89022","modified":1626699811726},{"_id":"themes/next/test/.jshintrc","hash":"19f93d13d1689fe033c82eb2d5f3ce30b6543cc0","modified":1626699811795},{"_id":"themes/next/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1626699811796},{"_id":"themes/next/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1626699811796},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1626699811762},{"_id":"source/_posts/about-kernel-02/linear_space.png","hash":"3fba384aa3bb9b905e3e70d11013dc4a0cfeb0a9","modified":1626699811696},{"_id":"source/_posts/thinking-from-softmax-02/subgradient.png","hash":"62fddffc69ca5fb2184acbbf60771a666e5e8514","modified":1626699811698},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"31322a7f57936cf2dc62e824af5490da5354cf02","modified":1626699811727},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"53d4f83b2b7fb4387dfc9fe81519abd56fbce4ae","modified":1626699811727},{"_id":"themes/next/layout/_partials/comments.swig","hash":"ce7094ee05878161e7568a6dfae5b56ff3fbd6e1","modified":1626699811728},{"_id":"themes/next/layout/_macro/post.swig","hash":"911363776867d9523a3e322cdf591d49cd166403","modified":1626699811727},{"_id":"themes/next/layout/_macro/reward.swig","hash":"5d5f70deb6074cb4dd0438463e14ccf89213c282","modified":1626699811727},{"_id":"themes/next/layout/_partials/footer.swig","hash":"51d1dd1258e0525da5d846f7827e5ae942d02ab3","modified":1626699811728},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"39852700e4084ecccffa6d4669168e5cc0514c9e","modified":1626699811728},{"_id":"themes/next/layout/_partials/head.swig","hash":"1f14d3f494b2dbbcee802fd6f6d1abd5b7e2304c","modified":1626699811728},{"_id":"themes/next/layout/_partials/header.swig","hash":"a1ffbb691dfad3eaf2832a11766e58a179003b8b","modified":1626699811729},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"98962d59c744ea6b0bbac9be0b02033ed39964aa","modified":1626699811728},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"9e8e21d194ef44d271b1cca0bc1448c14d7edf4f","modified":1626699811729},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"1efd925d34a5d4ba2dc0838d9c86ba911e705fc9","modified":1626699811729},{"_id":"themes/next/layout/_partials/search.swig","hash":"9dbd378e94abfcb3f864a5b8dbbf18d212ca2ee0","modified":1626699811729},{"_id":"themes/next/layout/_third-party/duoshuo-hot-articles.swig","hash":"5d4638c46aef65bf32a01681495b62416ccc98db","modified":1626699811736},{"_id":"themes/next/layout/_third-party/mathjax.swig","hash":"6d25596d6a7c57700d37b607f8d9a62d89708683","modified":1626699811736},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"fc93b1a7e6aed0dddb1f3910142b48d8ab61174e","modified":1626699811737},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"7c04a42319d728be356746363aff8ea247791d24","modified":1626699811736},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"22369026c87fc23893c35a7f250b42f3bb1b60f1","modified":1626699811737},{"_id":"themes/next/layout/_third-party/scroll-cookie.swig","hash":"1ddb2336a1a19b47af3017047012c01ec5f54529","modified":1626699811737},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"766b2bdda29523ed6cd8d7aa197f996022f8fd94","modified":1626699811731},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1626699811727},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"03aaebe9d50f6acb007ec38cc04acd1cfceb404d","modified":1626699811731},{"_id":"themes/next/layout/_custom/header.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1626699811727},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"9baf90f7c40b3b10f288e9268c3191e895890cea","modified":1626699811732},{"_id":"themes/next/scripts/tags/button.js","hash":"62e6dbeb53d07627a048132c79630b45d9a8f2cc","modified":1626699811741},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"535fc542781021c4326dec24d8495cbb1387634a","modified":1626699811741},{"_id":"themes/next/scripts/tags/exturl.js","hash":"8d7e60f60779bde050d20fd76f6fdc36fc85e06d","modified":1626699811741},{"_id":"themes/next/scripts/tags/full-image.js","hash":"8eeb3fb89540299bdbb799edfdfdac3743b50596","modified":1626699811741},{"_id":"themes/next/scripts/tags/label.js","hash":"2f8f41a7316372f0d1ed6b51190dc4acd3e16fff","modified":1626699811742},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"49252824cd53184dc9b97b2f2d87ff28e1b3ef27","modified":1626699811741},{"_id":"themes/next/scripts/tags/lazy-image.js","hash":"eeeabede68cf263de9e6593ecf682f620da16f0a","modified":1626699811742},{"_id":"themes/next/scripts/tags/note.js","hash":"64de4e9d01cf3b491ffc7d53afdf148ee5ad9779","modified":1626699811742},{"_id":"themes/next/scripts/tags/tabs.js","hash":"5786545d51c38e8ca38d1bfc7dd9e946fc70a316","modified":1626699811742},{"_id":"themes/next/source/css/main.styl","hash":"20702c48d6053c92c5bcdbc68e8d0ef1369848a0","modified":1626699811762},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1626699811762},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1626699811763},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1626699811763},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1626699811763},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1626699811763},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1626699811764},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1626699811764},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1626699811764},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1626699811764},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1626699811764},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1626699811765},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1626699811764},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1626699811765},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1626699811765},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1626699811732},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1626699811732},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1626699811756},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1626699811756},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1626699811757},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1626699811761},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1626699811762},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"bd07518060a73795d1250d93a74186444b292a9f","modified":1626699811729},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"9e1b9666efa77f4cf8d8261bcfa445a9ac608e53","modified":1626699811729},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"957701729b85fb0c5bfcf2fb99c19d54582f91ed","modified":1626699811730},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"959b7e04a96a5596056e4009b73b6489c117597e","modified":1626699811730},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"eefe2388ff3d424694045eda21346989b123977c","modified":1626699811730},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"23e23dc0f76ef3c631f24c65277adf7ea517b383","modified":1626699811730},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"1f1107468aaf03f7d0dcd7eb2b653e2813a675b4","modified":1626699811730},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1626699811730},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"4aa55cd424389cf5626aa019c15ef6f3e4da09f2","modified":1638607850611},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"63315fcf210799f894208c9f512737096df84962","modified":1626699811731},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"60426bf73f8a89ba61fb1be2df3ad5398e32c4ef","modified":1626699811733},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"8160b27bee0aa372c7dc7c8476c05bae57f58d0f","modified":1626699811733},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"deda6a814ed48debc694c4e0c466f06c127163d0","modified":1626699811733},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"3358d11b9a26185a2d36c96049e4340e701646e4","modified":1626699811733},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"a234c5cd1f75ca5731e814d0dbb92fdcf9240d1b","modified":1626699811733},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"3658414379e0e8a34c45c40feadc3edc8dc55f88","modified":1626699811734},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"fc65b9c98a0a8ab43a5e7aabff6c5f03838e09c8","modified":1626699811734},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"5d9943d74cc2e0a91badcf4f755c6de77eab193a","modified":1626699811733},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"c3971fd154d781088e1cc665035f8561a4098f4c","modified":1626699811734},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"0e3378f7c39b2b0f69638290873ede6b6b6825c0","modified":1626699811735},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"0ddc94ed4ba0c19627765fdf1abc4d8efbe53d5a","modified":1626699811734},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"c316758546dc9ba6c60cb4d852c17ca6bb6d6724","modified":1626699811735},{"_id":"themes/next/layout/_third-party/comments/duoshuo.swig","hash":"a356b2185d40914447fde817eb3d358ab6b3e4c3","modified":1626699811735},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"ee63aa2e49507b884a2d56778479cf01c723d751","modified":1626699811735},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"3e8dc5c6c912628a37e3b5f886bec7b2e5ed14ea","modified":1626699811735},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"8b6650f77fe0a824c8075b2659e0403e0c78a705","modified":1626699811736},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"8a2e393d2e49f7bf560766d8a07cd461bf3fce4f","modified":1626699811736},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"c747fb5c6b1f500e8f0c583e44195878b66e4e29","modified":1626699811738},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"385c066af96bee30be2459dbec8aae1f15d382f5","modified":1626699811738},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1626699811738},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"c057b17f79e8261680fbae8dc4e81317a127c799","modified":1626699811738},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"069d1357c717572256e5cdee09574ebce529cbae","modified":1626699811731},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1626699811732},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1626699811732},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"2aa5b7166a85a8aa34b17792ae4f58a5a96df6cc","modified":1626699811756},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"eaedfaf06dae94ba77a8f4893e2e434bf8859bac","modified":1626699811756},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"82f9055955920ed88a2ab6a20ab02169abb2c634","modified":1626699811757},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"328d9a9696cc2ccf59c67d3c26000d569f46344c","modified":1626699811756},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"49b5210fa62d6cbc6a98f57d89d5067a06ab3561","modified":1626699811761},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"c8d35a6b9e3bff6d8fdb66de853065af9d37562d","modified":1626699811761},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"cfee25d790e4f9b7d57f0dc7e2ea9c1649f08f11","modified":1626699811762},{"_id":"themes/next/source/css/_variables/base.styl","hash":"d477196c5699c8261b08e993a77ef67054d86166","modified":1626699811762},{"_id":"themes/next/source/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1626699811766},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"6117f97b4984b8e33f21c726132da64ba678e4ed","modified":1626699811766},{"_id":"themes/next/source/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1626699811765},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1626699811765},{"_id":"themes/next/source/js/src/motion.js","hash":"dc0365b2fb315a8b43d3ef19b59d3a82a366fcc1","modified":1626699811766},{"_id":"themes/next/source/js/src/post-details.js","hash":"0693695a9512641daff63d99da772625a058ab18","modified":1626699811767},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1626699811766},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1626699811766},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1626699811767},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1626699811767},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1626699811774},{"_id":"themes/next/source/js/src/utils.js","hash":"2917c39c75b14b6dab7e1c46ab4d87b4df9fcd5d","modified":1626699811767},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1626699811771},{"_id":"themes/next/source/lib/fastclick/bower.json","hash":"13379463c7463b4b96d13556b46faa4cc38d81e6","modified":1626699811778},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","hash":"7fd2f3e2773555392ef40df40cae3bedb884f17a","modified":1626699811774},{"_id":"themes/next/source/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1626699811777},{"_id":"themes/next/source/lib/fastclick/README.md","hash":"1decd8e1adad2cd6db0ab50cf56de6035156f4ea","modified":1626699811778},{"_id":"themes/next/source/lib/fastclick/.bower.json","hash":"93ebd5b35e632f714dcf1753e1f6db77ec74449b","modified":1626699811777},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"a2aaaf12378db56bd10596ba3daae30950eac051","modified":1626699811779},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1626699811779},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1626699811779},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1626699811779},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1626699811779},{"_id":"themes/next/source/lib/jquery/.bower.json","hash":"91745c2cc6c946c7275f952b2b0760b880cea69e","modified":1626699811784},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","hash":"895d50fa29759af7835256522e9dd7dac597765c","modified":1626699811786},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","hash":"4891864c24c28efecd81a6a8d3f261145190f901","modified":1626699811786},{"_id":"themes/next/source/lib/jquery_lazyload/.bower.json","hash":"b7638afc93e9cd350d0783565ee9a7da6805ad8e","modified":1626699811785},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1626699811786},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","hash":"65bc85d12197e71c40a55c0cd7f6823995a05222","modified":1626699811786},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1626699811786},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1626699811787},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1626699811787},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1626699811787},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1626699811787},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1626699811787},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1626699811787},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1626699811787},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1626699811788},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1626699811788},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1626699811788},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1626699811788},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1626699811788},{"_id":"themes/next/source/lib/pace/pace.min.js","hash":"9944dfb7814b911090e96446cea4d36e2b487234","modified":1626699811789},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1626699811788},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1626699811789},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1626699811789},{"_id":"themes/next/source/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1626699811789},{"_id":"themes/next/source/lib/velocity/.bower.json","hash":"05f960846f1c7a93dab1d3f9a1121e86812e8c88","modified":1626699811793},{"_id":"themes/next/source/lib/velocity/bower.json","hash":"2ec99573e84c7117368beccb9e94b6bf35d2db03","modified":1626699811793},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1626699811795},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1626699811795},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1626699811795},{"_id":"themes/next/source/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1626699811785},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"ba698f49dd3a868c95b240d802f5b1b24ff287e4","modified":1626699811737},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"28ff4ed6714c59124569ffcbd10f1173d53ca923","modified":1626699811737},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"0dfb4b3ba3180d7285e66f270e1d3fa0f132c3d2","modified":1626699811743},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1626699811743},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"31050fc7a25784805b4843550151c93bfa55c9c8","modified":1626699811743},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"d026c8489f66ab6c12ad04bd37f1d5b6f2f3f0d1","modified":1626699811743},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"a6bb5256be6195e76addbda12f4ed7c662d65e7a","modified":1626699811743},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"711c8830886619d4f4a0598b0cde5499dce50c62","modified":1626699811746},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"2186be20e317505cd31886f1291429cc21f76703","modified":1626699811754},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"9c25c75311e1bd4d68df031d3f2ae6d141a90766","modified":1626699811755},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"2915df7152ea095a6290ef69157fd67669e0e793","modified":1626699811755},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1626699811751},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"86b6fd7f1b1be3ae98f8af6b23a6b1299c670ce9","modified":1626699811755},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a280a583b7615e939aaddbf778f5c108ef8a2a6c","modified":1626699811755},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1626699811755},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"64f5d56c08d74a338813df1265580ca0cbf0190b","modified":1626699811756},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"bc8c388553bbcf95897459a466ba35bffd5ec5f0","modified":1626699811757},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"5ae7906dc7c1d9468c7f4b4a6feddddc555797a1","modified":1626699811758},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"c2d079788d6fc2e9a191ccdae94e50d55bf849dc","modified":1626699811757},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1626699811758},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"b0dcca862cd0cc6e732e33d975b476d744911742","modified":1626699811758},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"fda14bc35be2e1b332809b55b3d07155a833dbf4","modified":1626699811758},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1626699811758},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9a5581a770af8964064fef7afd3e16963e45547f","modified":1626699811758},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"c4ed249798296f60bda02351fe6404fb3ef2126f","modified":1626699811760},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"5b93958239d3d2bf9aeaede44eced2434d784462","modified":1626699811760},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"215de948be49bcf14f06d500cef9f7035e406a43","modified":1626699811760},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"2f878213cb24c5ddc18877f6d15ec5c5f57745ac","modified":1626699811761},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"0efa036a15c18f5abb058b7c0fad1dd9ac5eed4c","modified":1626699811759},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"e3e23751d4ad24e8714b425d768cf68e37de7ded","modified":1626699811761},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"69ecd6c97e7cdfd822ac8102b45ad0ede85050db","modified":1626699811761},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"82bbaa6322764779a1ac2e2c8390ce901c7972e2","modified":1626699811759},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"8829bc556ca38bfec4add4f15a2f028092ac6d46","modified":1626699811759},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1626699811759},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"a0e2030a606c934fb2c5c7373aaae04a1caac4c5","modified":1626699811760},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"79da92119bc246fe05d1626ac98426a83ec90a94","modified":1626699811767},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1626699811774},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1626699811774},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1626699811775},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1626699811775},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1626699811775},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1626699811775},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1626699811777},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1626699811777},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1626699811777},{"_id":"themes/next/source/lib/Han/dist/han.css","hash":"bd40da3fba8735df5850956814e312bd7b3193d7","modified":1626699811769},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1626699811778},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1626699811778},{"_id":"themes/next/source/lib/Han/dist/han.min.css","hash":"a0c9e32549a8b8cf327ab9227b037f323cdb60ee","modified":1626699811770},{"_id":"themes/next/source/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1626699811771},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1626699811780},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1626699811780},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1626699811780},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1626699811793},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1626699811793},{"_id":"themes/next/source/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1626699811770},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1626699811784},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1626699811784},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"ae1ca14e51de67b07dba8f61ec79ee0e2e344574","modified":1626699811744},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"8994ffcce84deac0471532f270f97c44fea54dc0","modified":1626699811744},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"6c00f6e0978f4d8f9a846a15579963728aaa6a17","modified":1626699811744},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1626699811744},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"49c2b2c14a1e7fcc810c6be4b632975d0204c281","modified":1626699811745},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"8a2421cb9005352905fae9d41a847ae56957247e","modified":1626699811744},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"a82afbb72d83ee394aedc7b37ac0008a9823b4f4","modified":1626699811746},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"2039590632bba3943c39319d80ef630af7928185","modified":1626699811746},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"7778920dd105fa4de3a7ab206eeba30b1a7bac45","modified":1626699811745},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1626699811745},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1626699811746},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"96f32ea6c3265a3889e6abe57587f6e2a2a40dfb","modified":1626699811745},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"b76387934fb6bb75212b23c1a194486892cc495e","modified":1626699811745},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"740d37f428b8f4574a76fc95cc25e50e0565f45e","modified":1626699811745},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"beccb53dcd658136fb91a0c5678dea8f37d6e0b6","modified":1626699811746},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"f54367c0feda6986c030cc4d15a0ca6ceea14bcb","modified":1626699811747},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"0f7f522cc6bfb3401d5afd62b0fcdf48bb2d604b","modified":1626699811746},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"2cdc094ecf907a02fce25ad4a607cd5c40da0f2b","modified":1626699811747},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"88c7d75646b66b168213190ee4cd874609afd5e3","modified":1626699811747},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1626699811747},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"017074ef58166e2d69c53bb7590a0e7a8947a1ed","modified":1626699811748},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"ed88c8b51d0517759c777e71a6bfbe2907bcd994","modified":1626699811747},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"ee554b1031ef0070a5916477939021800e3c9d27","modified":1626699811747},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"a5d8617a24d7cb6c5ad91ea621183ca2c0917331","modified":1626699811747},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"963105a531403d7aad6d9e5e23e3bfabb8ec065a","modified":1626699811748},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"08a500b2984f109b751f3697ca33172d1340591a","modified":1626699811748},{"_id":"themes/next/source/css/_common/components/post/post-wordcount.styl","hash":"4fda5d38c6c8d910e3bf5c74a48a8d4a3f3dc73d","modified":1626699811748},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1626699811748},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"10251257aceecb117233c9554dcf8ecfef8e2104","modified":1626699811748},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"65a64d5662637b66e2f039a5f58217afe7a6e800","modified":1626699811749},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"51eca243220cf57133a4becae9b78514bcfdc723","modified":1626699811749},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"5f6ea57aabfa30a437059bf8352f1ad829dbd4ff","modified":1626699811749},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"920343e41c124221a17f050bbb989494d44f7a24","modified":1626699811749},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"45fa7193435a8eae9960267438750b4c9fa9587f","modified":1626699811750},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"efa5e5022e205b52786ce495d4879f5e7b8f84b2","modified":1626699811750},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"77c92a449ce84d558d26d052681f2e0dd77c70c9","modified":1626699811750},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"57d2c8a060f5e4e1a0aef9aae11a0016cf7ac5ba","modified":1626699811750},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"9486ddd2cb255227db102d09a7df4cae0fabad72","modified":1626699811750},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"3623e7fa4324ec1307370f33d8f287a9e20a5578","modified":1626699811750},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"c2abe4d87148e23e15d49ee225bc650de60baf46","modified":1626699811751},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"1b3cc9f4e5a7f6e05b4100e9990b37b20d4a2005","modified":1626699811751},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"b8969e1654eec89a0fd10d88b337fee9cb03cd44","modified":1626699811751},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"f7784aba0c1cd20d824c918c120012d57a5eaa2a","modified":1626699811750},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"4a457d265d62f287c63d48764ce45d9bcfc9ec5a","modified":1626699811752},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1626699811752},{"_id":"themes/next/source/css/_common/components/tags/note-modern.styl","hash":"45df0cf4c97b47e05573bcd41028ee50f3fdf432","modified":1626699811752},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"32c9156bea5bac9e9ad0b4c08ffbca8b3d9aac4b","modified":1626699811752},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"4ab5deed8c3b0c338212380f678f8382672e1bcb","modified":1626699811752},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"ead0d0f2321dc71505788c7f689f92257cf14947","modified":1626699811752},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"93b08815c4d17e2b96fef8530ec1f1064dede6ef","modified":1626699811753},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"fd42777b9125fd8969dc39d4f15473e2b91b4142","modified":1626699811753},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"d4e6d8d7b34dc69994593c208f875ae8f7e8a3ae","modified":1626699811753},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"2340dd9b3202c61d73cc708b790fac5adddbfc7f","modified":1626699811753},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1626699811754},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"cce6772e2cdb4db85d35486ae4c6c59367fbdd40","modified":1626699811754},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"d89c4b562b528e4746696b2ad8935764d133bdae","modified":1626699811754},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"aeff0e6e23725e8baea27c890ccbbf466024f767","modified":1626699811754},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1626699811759},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1626699811759},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1626699811760},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1626699811776},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1626699811776},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1626699811776},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1626699811776},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1626699811768},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1626699811776},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1626699811776},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1626699811768},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1626699811769},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1626699811769},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1626699811780},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1626699811794},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1626699811781},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1626699811783},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1626699811773},{"_id":"themes/next/source/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1626699811792},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1626699811782},{"_id":"public/baidusitemap.xml","hash":"0544658f64dde790b72d9e80e748a95dd411dcd6","modified":1638607877001},{"_id":"public/search.xml","hash":"436a85059384c143e18ca450d375ea9cdf023fcf","modified":1638607877004},{"_id":"public/sitemap.xml","hash":"f8d35988432c28a160a038817c4307e84f16c2d1","modified":1638607877005},{"_id":"public/tags/index.html","hash":"9c4c4d5496065799dcaeb315b7768c3285db616b","modified":1638608719118},{"_id":"public/categories/index.html","hash":"9436f48b50f1776d4498c4555bfe583494765754","modified":1638608719118},{"_id":"public/2021/07/20/baby-gc/index.html","hash":"1241c27fafda47d9b33ea159e40649d939ac1f71","modified":1638608719118},{"_id":"public/2020/01/12/alterable-vs-permanent/index.html","hash":"bc64b02dfc04093e27b2b9608ff59b656acfdd7a","modified":1638608719118},{"_id":"public/2019/09/10/5-steps-for-asking-good-questions/index.html","hash":"cedd7042f80d1691fb42444d7374b02f8fd72e45","modified":1638608719118},{"_id":"public/2017/12/27/rating-model-considering-user-count/index.html","hash":"168180b781b6e91cbf50285a3f0b925f1ac6ce71","modified":1638608719118},{"_id":"public/2018/02/04/discriminative-or-generative-model/index.html","hash":"0b10eedb25b82af37d4cf26ea2fc838b4245767a","modified":1638608719118},{"_id":"public/2017/12/23/GD-Series/index.html","hash":"60409b5541c0f89ed922796a22eda14bbd47b4d9","modified":1638608719119},{"_id":"public/2017/12/03/common-ground-of-mse-and-cee-in-nn/index.html","hash":"4853e0ef7788bab126b6bf4848f65c08c367975e","modified":1638608719119},{"_id":"public/2017/11/04/GAN-02/index.html","hash":"f44ce8c24354797dad0c930d3a64c3fe21d7f55e","modified":1638608719119},{"_id":"public/2017/11/01/GAN-01/index.html","hash":"afe85e4f966333f19f1a099cc8b3b0dcf5b39ac0","modified":1638608719119},{"_id":"public/2017/10/12/statistical-formulars-for-programmers/index.html","hash":"90df1e82a42c84f228dd57ce4086611cd808fdb8","modified":1638608719119},{"_id":"public/2017/10/15/from-boost-to-Adaboost-to-GBT/index.html","hash":"642996adbf9faee8f922c4627fdca9fce14450be","modified":1638608719119},{"_id":"public/2017/10/23/kmeans-is-a-gmm/index.html","hash":"ccf303e9d7b76a339f1967f11e9c9ea8073e0181","modified":1638608719119},{"_id":"public/2017/09/09/norm-regularization-02/index.html","hash":"73d8ad560eacb5189ab1220e52e09da149a74de5","modified":1638608719119},{"_id":"public/2017/09/13/norm-regularization-appendix/index.html","hash":"e4c65ae5bba56bd746404fe9e9c6961d6e80437e","modified":1638608719119},{"_id":"public/2017/09/07/norm-regularization-01/index.html","hash":"83ef88eda3f76c9d19920206578afb6ba8d89cd2","modified":1638608719119},{"_id":"public/2017/08/13/about-kernel-02/index.html","hash":"4f065289fd7d638ec653f84c4affd98f080734c3","modified":1638608719120},{"_id":"public/2017/08/13/about-kernel-01/index.html","hash":"30ba9ec5ce9ae2421e259eac9b5f5646e4538492","modified":1638608719120},{"_id":"public/2017/08/10/thinking-from-softmax-02/index.html","hash":"7b48b67d8cfbe68580b1cc6674391ca547d2f4da","modified":1638608719120},{"_id":"public/2017/08/11/thinking-from-softmax-03/index.html","hash":"ecc2d538f1255e2faab6b8ad509be9b2f6460c9b","modified":1638608719120},{"_id":"public/categories/Life/index.html","hash":"bfd6fccd14a122990917362d037c195958c8ad5b","modified":1638608719125},{"_id":"public/2017/08/08/compare-between-PCA-and-LFM/index.html","hash":"159870938cb3b7afc8127e2553a236f4cd4a67e4","modified":1638608719120},{"_id":"public/2017/08/10/thinking-from-softmax-01/index.html","hash":"56acea9408d7db14535778b01f3a9ddd0560b822","modified":1638608719120},{"_id":"public/2017/08/07/init-blog/index.html","hash":"0922a42bafa85734dc6559d6bc38cd31a52bbc62","modified":1638608719120},{"_id":"public/categories/ML/index.html","hash":"9a2f92cfcf45b06e2d39290405779d0ec3396963","modified":1638608719125},{"_id":"public/categories/Mathematics/index.html","hash":"3aef56d54e716e4edebf50e6cef309ecf0967261","modified":1638608719126},{"_id":"public/archives/index.html","hash":"2347e1d62f4c50e5e5b80175c42de51a0138575d","modified":1638608719121},{"_id":"public/categories/Develop/index.html","hash":"1013eea672b7a06f0d82c27c923a34942e794fb0","modified":1638608719126},{"_id":"public/categories/ML/page/2/index.html","hash":"957b8fe3c1fee75e5ee478a8e8f68cb2c2a6f901","modified":1638608719125},{"_id":"public/archives/page/3/index.html","hash":"ae34b7065d3e579e1ea6a55a300a0924bd8c6d00","modified":1638608719121},{"_id":"public/archives/page/2/index.html","hash":"01e75babf9298313f2bcba122f6152a3ef320342","modified":1638608719120},{"_id":"public/archives/2017/index.html","hash":"422fb12569726975ad49e2024bf2c12517a3a645","modified":1638608719120},{"_id":"public/archives/2017/page/2/index.html","hash":"422b30aca20d96abfb19b7c10b3e506e11d1d5e1","modified":1638608719120},{"_id":"public/archives/2017/09/index.html","hash":"7baee6d20c411cb8be6682eb176920b907c0ccfd","modified":1638608719121},{"_id":"public/archives/2017/08/index.html","hash":"42c71db06b855023be9267324c8c5eb9edb4d2ae","modified":1638608719121},{"_id":"public/archives/2017/10/index.html","hash":"c3858b8d35621214f6b1d43b44bc8adf626daaa8","modified":1638608719121},{"_id":"public/archives/2017/11/index.html","hash":"14df0d222f33e5a844fcee1f35d87716c8d004ea","modified":1638608719121},{"_id":"public/archives/2018/index.html","hash":"bef2d76843204ebd7abf281a8e1dbf2ba7418a0c","modified":1638608719121},{"_id":"public/archives/2017/12/index.html","hash":"0bc4011519738aedb43a584540608a2d5619328a","modified":1638608719121},{"_id":"public/archives/2019/index.html","hash":"21b904d5bb235f7bd8e74cf7eace3e11bddbf3dd","modified":1638608719121},{"_id":"public/archives/2018/02/index.html","hash":"58855649ebd6bab57646abcfb9f30a3280d0697e","modified":1638608719121},{"_id":"public/archives/2020/index.html","hash":"1659dc5027e55ff683047a34979dbfa1e0148aa6","modified":1638608719122},{"_id":"public/archives/2020/01/index.html","hash":"eaf6cd610e018a6b630d34b7c31fc4f980c4dd19","modified":1638608719122},{"_id":"public/archives/2019/09/index.html","hash":"46ebc75fdd767b116ee0c309e2f4f41a93cf0a9d","modified":1638608719121},{"_id":"public/archives/2021/index.html","hash":"8f02ae9502f5988a4a462c06d211ca2a6577dcd5","modified":1638608719122},{"_id":"public/index.html","hash":"edc28cc78049093c5712e324b09ad1ef84b26710","modified":1638608719122},{"_id":"public/archives/2021/07/index.html","hash":"f6796a0f3762a4ac1c8d667749486319245bd7e3","modified":1638608719122},{"_id":"public/page/2/index.html","hash":"9e59ee20ceb061439ab62d7680a2d64c9a247ed3","modified":1638608719122},{"_id":"public/tags/Tips/index.html","hash":"800b47b1fccb650cdc57aee8fff159bba3d34441","modified":1638608719123},{"_id":"public/page/3/index.html","hash":"a1e967a2f1a872c91d2d70c06faf383c6a8ff1b9","modified":1638608719122},{"_id":"public/tags/Deep-learning/index.html","hash":"0d57b5f33240dcadedfd9ab52ff96ac1142f53fc","modified":1638608719123},{"_id":"public/tags/GAN/index.html","hash":"bbb6f4616befa5bb505a6079fa2cc1b1e59669da","modified":1638608719123},{"_id":"public/tags/GD/index.html","hash":"09a3d257f154efc0778093fff3fcfe03718241c3","modified":1638608719123},{"_id":"public/tags/SVM/index.html","hash":"026803d33124f880ca44726e030f63939a4c3fab","modified":1638608719123},{"_id":"public/tags/Kernel/index.html","hash":"e801087148b024de443bc9822ea208a69cd8b267","modified":1638608719123},{"_id":"public/tags/Momentum/index.html","hash":"274fd5282256f5d4968c561785b7505aa8efc109","modified":1638608719123},{"_id":"public/tags/RKHS/index.html","hash":"fa69e8441866ba4eeb5fb88652d3cbb94f562885","modified":1638608719123},{"_id":"public/tags/Loss-function/index.html","hash":"573a0e5c566c9497a63b33a444b789df1ee03c5a","modified":1638608719123},{"_id":"public/tags/PCA/index.html","hash":"4e77eb7c9bfb02711c78b394b4a00e4885ccbc71","modified":1638608719124},{"_id":"public/tags/Generative-model/index.html","hash":"362f1bbc7635023bc6d9de3b988fc2b2ed577793","modified":1638608719124},{"_id":"public/tags/LFM/index.html","hash":"31bd2efecd6c2f761035913f939a3da285715daf","modified":1638608719124},{"_id":"public/tags/Discriminative-model/index.html","hash":"0d169f59a9cc364243abc1185b7ace60051e4512","modified":1638608719124},{"_id":"public/tags/Boost/index.html","hash":"225a055455718c967b5fe7737c97cf370a8cab06","modified":1638608719124},{"_id":"public/tags/Ensemble/index.html","hash":"59afdce7ec0440658bf6bc7491684ff6ebb6e492","modified":1638608719124},{"_id":"public/tags/GBDT-GBRT/index.html","hash":"cda916e36e6b5fcb47f2bbca7777ef68cab00dcc","modified":1638608719124},{"_id":"public/tags/GMM/index.html","hash":"93ff4c2b1953bc939ee552b6d6eadbaeab5f85f2","modified":1638608719124},{"_id":"public/tags/EM-Algorithm/index.html","hash":"3c6bccb9f45fce7fb1ad28659f71a19d0edf99a8","modified":1638608719124},{"_id":"public/tags/Norm-regularization/index.html","hash":"d8b3eb877ce132fc3e237e253f5fdcc6e831a26b","modified":1638608719124},{"_id":"public/tags/GC/index.html","hash":"574cde346816c9841ebed9231ae95c957a92efa9","modified":1638608719125},{"_id":"public/tags/K-means/index.html","hash":"5681c97f5a2bd4513780e68d782054611b23a0ac","modified":1638608719124},{"_id":"public/tags/Matrix-theory/index.html","hash":"c16814d860bf736ed7ee934f245e467af15978ce","modified":1638608719125},{"_id":"public/tags/Convex-optimization/index.html","hash":"ba9224a8c1982c33b6a55fd5ed81c502814d79f0","modified":1638608719124},{"_id":"public/tags/Functional-analysis/index.html","hash":"ae4dfdc96776a949799ea7108ca93b8942d858cc","modified":1638608719125},{"_id":"public/tags/Generalized-Linear-Model/index.html","hash":"3e7cb71bca261fd583b3de58c2b2ef85cbd8b9d5","modified":1638608719125},{"_id":"public/tags/Exponential-family-distribution/index.html","hash":"f29ec8b9dea51224d61675e603ed7b947a25fc36","modified":1638608719125},{"_id":"public/tags/Rating/index.html","hash":"3b750c3a8d17c7edb465b7cdef64127863962d5d","modified":1638608719125},{"_id":"public/tags/Statistics/index.html","hash":"8e2c04364f22b7dd81a6331d6b3a05f18357f935","modified":1638608719125},{"_id":"public/tags/Softmax/index.html","hash":"e266c260979d3b48877a6e66aff1115f3ad3c6da","modified":1638608719125},{"_id":"public/tags/Activation-function/index.html","hash":"1151a83038ba0f123d71753f840601209bb331f6","modified":1638608719125},{"_id":"public/tags/Logistic/index.html","hash":"2b0a9a50c29e363a31980706b37de15b265cc968","modified":1638608719125},{"_id":"public/tags/Gradient/index.html","hash":"4eb69ed2bccf429e85f238a4d3231f0fe6258162","modified":1638608719125},{"_id":"public/tags/Subgradient/index.html","hash":"a1783496ebda807f38d41d9637fc29ae11fe0ccd","modified":1638608719125},{"_id":"public/baidu_verify_aX3upgcPux.html","hash":"344502cc0cb9646f0643ff932d5c7adb1438cd5f","modified":1638607877222},{"_id":"public/README.md","hash":"876c607060f4a864761fd1d8cc982edba012f251","modified":1638607877222},{"_id":"public/CNAME","hash":"685ff10641d5fffd71eec3979dab6fd0f10c9ea7","modified":1638607877222},{"_id":"public/robots.txt","hash":"7858025995a22727d626cdda854f061df5ba8fb9","modified":1638607877223},{"_id":"public/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1638607877223},{"_id":"public/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1638607877223},{"_id":"public/images/linear_space.eddx","hash":"8910cfe98f066b604fbbb11ddc2b972c17164796","modified":1638607877223},{"_id":"public/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1638607877223},{"_id":"public/uploads/me.jpeg","hash":"38ab8de612b4c83a8be656af7ca2b87b670c4c57","modified":1638607877223},{"_id":"public/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1638607877223},{"_id":"public/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1638607877223},{"_id":"public/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1638607877223},{"_id":"public/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1638607877223},{"_id":"public/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1638607877223},{"_id":"public/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1638607877223},{"_id":"public/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1638607877224},{"_id":"public/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1638607877224},{"_id":"public/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1638607877224},{"_id":"public/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1638607877224},{"_id":"public/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1638607877224},{"_id":"public/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1638607877224},{"_id":"public/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1638607877224},{"_id":"public/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1638607877224},{"_id":"public/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1638607877224},{"_id":"public/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1638607877224},{"_id":"public/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1638607877224},{"_id":"public/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1638607877224},{"_id":"public/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1638607877224},{"_id":"public/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1638607877224},{"_id":"public/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1638607877224},{"_id":"public/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1638607877225},{"_id":"public/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1638607877225},{"_id":"public/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1638607877225},{"_id":"public/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1638607877225},{"_id":"public/2017/08/13/about-kernel-02/linear_space.png","hash":"3fba384aa3bb9b905e3e70d11013dc4a0cfeb0a9","modified":1638607877225},{"_id":"public/2017/08/10/thinking-from-softmax-02/subgradient.png","hash":"62fddffc69ca5fb2184acbbf60771a666e5e8514","modified":1638607877225},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1638607877665},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1638607877665},{"_id":"public/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1638607877668},{"_id":"public/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1638607877668},{"_id":"public/js/src/bootstrap.js","hash":"6117f97b4984b8e33f21c726132da64ba678e4ed","modified":1638607877668},{"_id":"public/js/src/post-details.js","hash":"0693695a9512641daff63d99da772625a058ab18","modified":1638607877668},{"_id":"public/js/src/motion.js","hash":"dc0365b2fb315a8b43d3ef19b59d3a82a366fcc1","modified":1638607877668},{"_id":"public/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1638607877668},{"_id":"public/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1638607877668},{"_id":"public/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1638607877668},{"_id":"public/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1638607877668},{"_id":"public/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1638607877668},{"_id":"public/js/src/utils.js","hash":"2917c39c75b14b6dab7e1c46ab4d87b4df9fcd5d","modified":1638607877668},{"_id":"public/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1638607877668},{"_id":"public/lib/fastclick/bower.json","hash":"4dcecf83afddba148464d5339c93f6d0aa9f42e9","modified":1638607877668},{"_id":"public/lib/canvas-ribbon/canvas-ribbon.js","hash":"7fd2f3e2773555392ef40df40cae3bedb884f17a","modified":1638607877669},{"_id":"public/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1638607877669},{"_id":"public/lib/font-awesome/bower.json","hash":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad","modified":1638607877669},{"_id":"public/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1638607877669},{"_id":"public/lib/jquery_lazyload/bower.json","hash":"ae3c3b61e6e7f9e1d7e3585ad854380ecc04cf53","modified":1638607877669},{"_id":"public/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1638607877669},{"_id":"public/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1638607877669},{"_id":"public/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1638607877669},{"_id":"public/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1638607877669},{"_id":"public/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1638607877669},{"_id":"public/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1638607877669},{"_id":"public/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1638607877669},{"_id":"public/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1638607877669},{"_id":"public/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1638607877669},{"_id":"public/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1638607877670},{"_id":"public/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1638607877670},{"_id":"public/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1638607877670},{"_id":"public/lib/pace/pace.min.js","hash":"9944dfb7814b911090e96446cea4d36e2b487234","modified":1638607877670},{"_id":"public/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1638607877670},{"_id":"public/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1638607877670},{"_id":"public/lib/velocity/bower.json","hash":"0ef14e7ccdfba5db6eb3f8fc6aa3b47282c36409","modified":1638607877670},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1638607877670},{"_id":"public/js/src/schemes/pisces.js","hash":"79da92119bc246fe05d1626ac98426a83ec90a94","modified":1638607877670},{"_id":"public/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1638607877670},{"_id":"public/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1638607877670},{"_id":"public/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1638607877670},{"_id":"public/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1638607877671},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1638607877671},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1638607877671},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1638607877671},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1638607877671},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1638607877671},{"_id":"public/lib/fastclick/README.html","hash":"d6e90449a2c09f3033f7e43d68b0cc8208e22e09","modified":1638607877671},{"_id":"public/lib/jquery_lazyload/README.html","hash":"a08fccd381c8fdb70ba8974b208254c5ba23a95f","modified":1638607877672},{"_id":"public/lib/jquery_lazyload/CONTRIBUTING.html","hash":"06811ca2f722dead021493457f27cdc264ef928d","modified":1638607877672},{"_id":"public/css/main.css","hash":"db546f069d3f97dfd2005ee383eedee7a49d7d13","modified":1638607877672},{"_id":"public/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1638607877672},{"_id":"public/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1638607877672},{"_id":"public/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1638607877672},{"_id":"public/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1638607877672},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1638607877672},{"_id":"public/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1638607877672},{"_id":"public/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1638607877672},{"_id":"public/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1638607877672},{"_id":"public/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1638607877672},{"_id":"public/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1638607877673},{"_id":"public/lib/Han/dist/han.min.css","hash":"a0c9e32549a8b8cf327ab9227b037f323cdb60ee","modified":1638607877673},{"_id":"public/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1638607877673},{"_id":"public/lib/Han/dist/han.css","hash":"bd40da3fba8735df5850956814e312bd7b3193d7","modified":1638607877673},{"_id":"public/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1638607877673},{"_id":"public/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1638607877673},{"_id":"public/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1638607877673},{"_id":"public/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1638607877674},{"_id":"public/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1638607877674},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1638607877674},{"_id":"public/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1638607877674},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1638607877779},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1638607878071}],"Category":[{"name":"Life","_id":"ckwrktiyx000oqxt67y30h1ok"},{"name":"ML","_id":"ckwrktiz4000qqxt6lt5mcuhc"},{"name":"Develop","_id":"ckwrktizc001eqxt6kmyi94x1"},{"name":"Mathematics","_id":"ckwrktize001nqxt6j88gtts3"}],"Data":[],"Page":[{"title":"","date":"2017-08-07T14:12:32.000Z","type":"tags","comments":0,"_content":"","source":"tags/index.md","raw":"---\ntitle: \ndate: 2017-08-07 22:12:32\ntype: \"tags\"\ncomments: false\n---\n","updated":"2021-07-19T13:03:31.699Z","path":"tags/index.html","layout":"page","_id":"ckwrktiv20000qxt6ghykemn7","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"","date":"2017-08-07T14:13:45.000Z","type":"categories","comments":0,"_content":"","source":"categories/index.md","raw":"---\ntitle: \ndate: 2017-08-07 22:13:45\ntype: \"categories\"\ncomments: false\n---\n","updated":"2021-07-19T13:03:31.699Z","path":"categories/index.html","layout":"page","_id":"ckwrktiv60002qxt6ki8vqqf3","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":" 5 ","date":"2019-09-10T12:05:04.000Z","description":" 5 ","_content":"\n\n\n 5 \n\n1. ****\n2. ****\n3. ****\n4. ****\n5. ****\n\n\n\n","source":"_posts/5-steps-for-asking-good-questions.md","raw":"---\ntitle:  5 \ndate: 2019-09-10 20:05:04\ncategories: Life\ntags:\n- Tips\ndescription:  5 \n---\n\n\n\n 5 \n\n1. ****\n2. ****\n3. ****\n4. ****\n5. ****\n\n\n\n","slug":"5-steps-for-asking-good-questions","published":1,"updated":"2021-07-19T13:03:31.695Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckwrktiv30001qxt6wl19k31f","content":"<p></p>\n<p> 5 </p>\n<ol>\n<li><strong></strong></li>\n<li><strong></strong></li>\n<li><strong></strong></li>\n<li><strong></strong></li>\n<li><strong></strong></li>\n</ol>\n<p></p>\n","site":{"data":{}},"excerpt":"","more":"<p></p>\n<p> 5 </p>\n<ol>\n<li><strong></strong></li>\n<li><strong></strong></li>\n<li><strong></strong></li>\n<li><strong></strong></li>\n<li><strong></strong></li>\n</ol>\n<p></p>\n"},{"title":" GAN  TensorFlow ","date":"2017-11-01T08:09:01.000Z","description":"GAN TensorFlow ","_content":"\n# GAN\n\n2014 Goodfellow **GAN**\n\n- \n- \n-  D G\n- G  D \n\nG  D G D  D  G D  $1/2$  G \n\n $x$  $p_g(x)$  $p_z(z)$  $G(z;\\theta_g)$  $G$  $D(x;\\theta_d)$  $x$  $D$  $\\log (1-D(G(z)))$ G  $G$ \n$$\n\\min_G \\max_D V(D,G) = \\mathbb{E}_{x \\sim p_{\\text{data}}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_{z}(z)}[\\log (1-D(G(z)))]\n$$\n\n# \n\n\n$$\n\\mathbb{E}_{z \\sim p_{z}(z)}[\\log (1-D(G(z)))] = \\mathbb{E}_{x \\sim p_{g}(x)}[\\log (1-D(x)]\n$$\n $V(D,G)$  $x$ \n$$\nV(G, D) = \\int_{x} p_{\\text{data}}(x) \\log (D(x)) + p_g(x) \\log (1-D(x)) \\, dx\n$$\n $p_{\\text{data}}(x)$  $p_g(x)$ \n$$\nf(y) = a \\log y + b \\log (1-y)\n$$\n\n$$\nf'(y) = 0 \\Rightarrow \\frac{a}{y}-\\frac{b}{1-y}=0 \\Rightarrow y=\\frac{a}{a+b}\n$$\n\n$$\nf''(y) \\Big|_{y=\\frac{a}{a+b}} = -\\frac{a}{\\left( \\frac{a}{a+b} \\right)^{2}}-\\frac{b}{1-\\left( \\frac{a}{a+b} \\right)^{2}}<0\n$$\n $\\frac{a}{a+b}$  $a=p_{\\text{data}}(x), b=p_g(x), y=D(x)$ \n$$\nD(x) = \\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)}\n$$\n\n# \n\nGAN  $p_{\\text{data}}=p_g$ \n$$\nD_G^{\\ast}=\\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)}=\\frac{1}{2}\n$$\nGoodfellow  min max \n\n#  TensorFlow  MNIST \n\n CNN G  D \n\n```python\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.examples.tutorials.mnist import input_data\n# from PIL import Image\n\nmnist = input_data.read_data_sets(\"MNIST_data/\")\nimages = mnist.train.images\n\n\ndef xavier_initializer(shape):\n    return tf.random_normal(shape=shape, stddev=1.0 / shape[0])\n\n\n# Generator\nz_size = 100  # maybe larger\ng_w1_size = 400\ng_out_size = 28 * 28\n\n# Discriminator\nx_size = 28 * 28\nd_w1_size = 400\nd_out_size = 1\n\nz = tf.placeholder('float', shape=(None, z_size))\nX = tf.placeholder('float', shape=(None, x_size))\n\n# use dict to share variables\ng_weights = {\n    'w1': tf.Variable(xavier_initializer(shape=(z_size, g_w1_size))),\n    'b1': tf.Variable(tf.zeros(shape=[g_w1_size])),\n    'out': tf.Variable(xavier_initializer(shape=(g_w1_size, g_out_size))),\n    'b2': tf.Variable(tf.zeros(shape=[g_out_size])),\n}\n\nd_weights = {\n    'w1': tf.Variable(xavier_initializer(shape=(x_size, d_w1_size))),\n    'b1': tf.Variable(tf.zeros(shape=[d_w1_size])),\n    'out': tf.Variable(xavier_initializer(shape=(d_w1_size, d_out_size))),\n    'b2': tf.Variable(tf.zeros(shape=[d_out_size])),\n}\n\n\ndef G(z, w=g_weights):\n    # here tanh is better than relu\n    h1 = tf.tanh(tf.matmul(z, w['w1']) + w['b1'])\n    # pixel output is in range [0, 255]\n    return tf.sigmoid(tf.matmul(h1, w['out']) + w['b2']) * 255\n\n\ndef D(x, w=d_weights):\n    # here tanh is better than relu\n    h1 = tf.tanh(tf.matmul(x, w['w1']) + w['b1'])\n    h2 = tf.matmul(h1, w['out']) + w['b2']\n    return h2  # use h2 to calculate logits loss\n\n\ndef generate_z(n=1):\n    return np.random.normal(size=(n, z_size))\n\n\nsample = G(z)\n\ndout_real = D(X)\ndout_fake = D(G(z))\n\nG_obj = tf.reduce_mean(\n    tf.nn.sigmoid_cross_entropy_with_logits(logits=dout_fake, labels=tf.ones_like(dout_fake)))\nD_obj_real = tf.reduce_mean(  # use single side smoothing\n    tf.nn.sigmoid_cross_entropy_with_logits(logits=dout_real, labels=(tf.ones_like(dout_real) - 0.1)))\nD_obj_fake = tf.reduce_mean(\n    tf.nn.sigmoid_cross_entropy_with_logits(logits=dout_fake, labels=tf.zeros_like(dout_fake)))\nD_obj = D_obj_real + D_obj_fake\n\nG_opt = tf.train.AdamOptimizer().minimize(G_obj, var_list=g_weights.values())\nD_opt = tf.train.AdamOptimizer().minimize(D_obj, var_list=d_weights.values())\n\n# Training\nbatch_size = 128\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n\n    for i in range(200):\n        sess.run(D_opt, feed_dict={\n            X: images[np.random.choice(range(len(images)), batch_size)].reshape(batch_size, x_size),\n            z: generate_z(batch_size),\n        })\n        # run two phases of generator\n        sess.run(G_opt, feed_dict={\n            z: generate_z(batch_size)\n        })\n        sess.run(G_opt, feed_dict={\n            z: generate_z(batch_size)\n        })\n\n        g_cost = sess.run(G_obj, feed_dict={z: generate_z(batch_size)})\n        d_cost = sess.run(D_obj, feed_dict={\n            X: images[np.random.choice(range(len(images)), batch_size)].reshape(batch_size, x_size),\n            z: generate_z(batch_size),\n        })\n        image = sess.run(G(z), feed_dict={z: generate_z()})\n        df = sess.run(tf.sigmoid(dout_fake), feed_dict={z: generate_z()})\n        # print i, G cost, D cost, image max pixel, D output of fake\n        print(i, g_cost, d_cost, image.max(), df[0][0])\n\n    # You may wish to save or plot the image generated\n    # to see how it looks like\n    image = sess.run(G(z), feed_dict={z: generate_z()})\n    image1 = image[0].reshape([28, 28])\n    # print(image1)\n    # im = Image.fromarray(image1)\n    # im.show()\n```\n\n","source":"_posts/GAN-01.md","raw":"---\ntitle:  GAN  TensorFlow \ndate: 2017-11-01 16:09:01\ncategories: ML\ntags:\n     - Deep learning\n     - GAN\ndescription: GAN TensorFlow \n---\n\n# GAN\n\n2014 Goodfellow **GAN**\n\n- \n- \n-  D G\n- G  D \n\nG  D G D  D  G D  $1/2$  G \n\n $x$  $p_g(x)$  $p_z(z)$  $G(z;\\theta_g)$  $G$  $D(x;\\theta_d)$  $x$  $D$  $\\log (1-D(G(z)))$ G  $G$ \n$$\n\\min_G \\max_D V(D,G) = \\mathbb{E}_{x \\sim p_{\\text{data}}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_{z}(z)}[\\log (1-D(G(z)))]\n$$\n\n# \n\n\n$$\n\\mathbb{E}_{z \\sim p_{z}(z)}[\\log (1-D(G(z)))] = \\mathbb{E}_{x \\sim p_{g}(x)}[\\log (1-D(x)]\n$$\n $V(D,G)$  $x$ \n$$\nV(G, D) = \\int_{x} p_{\\text{data}}(x) \\log (D(x)) + p_g(x) \\log (1-D(x)) \\, dx\n$$\n $p_{\\text{data}}(x)$  $p_g(x)$ \n$$\nf(y) = a \\log y + b \\log (1-y)\n$$\n\n$$\nf'(y) = 0 \\Rightarrow \\frac{a}{y}-\\frac{b}{1-y}=0 \\Rightarrow y=\\frac{a}{a+b}\n$$\n\n$$\nf''(y) \\Big|_{y=\\frac{a}{a+b}} = -\\frac{a}{\\left( \\frac{a}{a+b} \\right)^{2}}-\\frac{b}{1-\\left( \\frac{a}{a+b} \\right)^{2}}<0\n$$\n $\\frac{a}{a+b}$  $a=p_{\\text{data}}(x), b=p_g(x), y=D(x)$ \n$$\nD(x) = \\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)}\n$$\n\n# \n\nGAN  $p_{\\text{data}}=p_g$ \n$$\nD_G^{\\ast}=\\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)}=\\frac{1}{2}\n$$\nGoodfellow  min max \n\n#  TensorFlow  MNIST \n\n CNN G  D \n\n```python\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.examples.tutorials.mnist import input_data\n# from PIL import Image\n\nmnist = input_data.read_data_sets(\"MNIST_data/\")\nimages = mnist.train.images\n\n\ndef xavier_initializer(shape):\n    return tf.random_normal(shape=shape, stddev=1.0 / shape[0])\n\n\n# Generator\nz_size = 100  # maybe larger\ng_w1_size = 400\ng_out_size = 28 * 28\n\n# Discriminator\nx_size = 28 * 28\nd_w1_size = 400\nd_out_size = 1\n\nz = tf.placeholder('float', shape=(None, z_size))\nX = tf.placeholder('float', shape=(None, x_size))\n\n# use dict to share variables\ng_weights = {\n    'w1': tf.Variable(xavier_initializer(shape=(z_size, g_w1_size))),\n    'b1': tf.Variable(tf.zeros(shape=[g_w1_size])),\n    'out': tf.Variable(xavier_initializer(shape=(g_w1_size, g_out_size))),\n    'b2': tf.Variable(tf.zeros(shape=[g_out_size])),\n}\n\nd_weights = {\n    'w1': tf.Variable(xavier_initializer(shape=(x_size, d_w1_size))),\n    'b1': tf.Variable(tf.zeros(shape=[d_w1_size])),\n    'out': tf.Variable(xavier_initializer(shape=(d_w1_size, d_out_size))),\n    'b2': tf.Variable(tf.zeros(shape=[d_out_size])),\n}\n\n\ndef G(z, w=g_weights):\n    # here tanh is better than relu\n    h1 = tf.tanh(tf.matmul(z, w['w1']) + w['b1'])\n    # pixel output is in range [0, 255]\n    return tf.sigmoid(tf.matmul(h1, w['out']) + w['b2']) * 255\n\n\ndef D(x, w=d_weights):\n    # here tanh is better than relu\n    h1 = tf.tanh(tf.matmul(x, w['w1']) + w['b1'])\n    h2 = tf.matmul(h1, w['out']) + w['b2']\n    return h2  # use h2 to calculate logits loss\n\n\ndef generate_z(n=1):\n    return np.random.normal(size=(n, z_size))\n\n\nsample = G(z)\n\ndout_real = D(X)\ndout_fake = D(G(z))\n\nG_obj = tf.reduce_mean(\n    tf.nn.sigmoid_cross_entropy_with_logits(logits=dout_fake, labels=tf.ones_like(dout_fake)))\nD_obj_real = tf.reduce_mean(  # use single side smoothing\n    tf.nn.sigmoid_cross_entropy_with_logits(logits=dout_real, labels=(tf.ones_like(dout_real) - 0.1)))\nD_obj_fake = tf.reduce_mean(\n    tf.nn.sigmoid_cross_entropy_with_logits(logits=dout_fake, labels=tf.zeros_like(dout_fake)))\nD_obj = D_obj_real + D_obj_fake\n\nG_opt = tf.train.AdamOptimizer().minimize(G_obj, var_list=g_weights.values())\nD_opt = tf.train.AdamOptimizer().minimize(D_obj, var_list=d_weights.values())\n\n# Training\nbatch_size = 128\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n\n    for i in range(200):\n        sess.run(D_opt, feed_dict={\n            X: images[np.random.choice(range(len(images)), batch_size)].reshape(batch_size, x_size),\n            z: generate_z(batch_size),\n        })\n        # run two phases of generator\n        sess.run(G_opt, feed_dict={\n            z: generate_z(batch_size)\n        })\n        sess.run(G_opt, feed_dict={\n            z: generate_z(batch_size)\n        })\n\n        g_cost = sess.run(G_obj, feed_dict={z: generate_z(batch_size)})\n        d_cost = sess.run(D_obj, feed_dict={\n            X: images[np.random.choice(range(len(images)), batch_size)].reshape(batch_size, x_size),\n            z: generate_z(batch_size),\n        })\n        image = sess.run(G(z), feed_dict={z: generate_z()})\n        df = sess.run(tf.sigmoid(dout_fake), feed_dict={z: generate_z()})\n        # print i, G cost, D cost, image max pixel, D output of fake\n        print(i, g_cost, d_cost, image.max(), df[0][0])\n\n    # You may wish to save or plot the image generated\n    # to see how it looks like\n    image = sess.run(G(z), feed_dict={z: generate_z()})\n    image1 = image[0].reshape([28, 28])\n    # print(image1)\n    # im = Image.fromarray(image1)\n    # im.show()\n```\n\n","slug":"GAN-01","published":1,"updated":"2021-07-19T13:03:31.695Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckwrktiv60003qxt6hkkafdl8","content":"<h1 id=\"GAN\"><a href=\"#GAN\" class=\"headerlink\" title=\"GAN\"></a>GAN</h1><p>2014 Goodfellow <strong>GAN</strong></p>\n<ul>\n<li></li>\n<li></li>\n<li> D G</li>\n<li>G  D </li>\n</ul>\n<p>G  D G D  D  G D  $1/2$  G </p>\n<p> $x$  $p_g(x)$  $p_z(z)$  $G(z;\\theta_g)$  $G$  $D(x;\\theta_d)$  $x$  $D$  $\\log (1-D(G(z)))$ G  $G$ </p>\n<script type=\"math/tex; mode=display\">\n\\min_G \\max_D V(D,G) = \\mathbb{E}_{x \\sim p_{\\text{data}}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_{z}(z)}[\\log (1-D(G(z)))]</script><h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n<script type=\"math/tex; mode=display\">\n\\mathbb{E}_{z \\sim p_{z}(z)}[\\log (1-D(G(z)))] = \\mathbb{E}_{x \\sim p_{g}(x)}[\\log (1-D(x)]</script><p> $V(D,G)$  $x$ </p>\n<script type=\"math/tex; mode=display\">\nV(G, D) = \\int_{x} p_{\\text{data}}(x) \\log (D(x)) + p_g(x) \\log (1-D(x)) \\, dx</script><p> $p_{\\text{data}}(x)$  $p_g(x)$ </p>\n<script type=\"math/tex; mode=display\">\nf(y) = a \\log y + b \\log (1-y)</script><p></p>\n<script type=\"math/tex; mode=display\">\nf'(y) = 0 \\Rightarrow \\frac{a}{y}-\\frac{b}{1-y}=0 \\Rightarrow y=\\frac{a}{a+b}</script><p></p>\n<script type=\"math/tex; mode=display\">\nf''(y) \\Big|_{y=\\frac{a}{a+b}} = -\\frac{a}{\\left( \\frac{a}{a+b} \\right)^{2}}-\\frac{b}{1-\\left( \\frac{a}{a+b} \\right)^{2}}<0</script><p> $\\frac{a}{a+b}$  $a=p_{\\text{data}}(x), b=p_g(x), y=D(x)$ </p>\n<script type=\"math/tex; mode=display\">\nD(x) = \\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)}</script><h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>GAN  $p_{\\text{data}}=p_g$ </p>\n<script type=\"math/tex; mode=display\">\nD_G^{\\ast}=\\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)}=\\frac{1}{2}</script><p>Goodfellow  min max </p>\n<h1 id=\"-TensorFlow--MNIST-\"><a href=\"#-TensorFlow--MNIST-\" class=\"headerlink\" title=\" TensorFlow  MNIST \"></a> TensorFlow  MNIST </h1><p> CNN G  D </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">from</span> tensorflow.examples.tutorials.mnist <span class=\"keyword\">import</span> input_data</span><br><span class=\"line\"><span class=\"comment\"># from PIL import Image</span></span><br><span class=\"line\"></span><br><span class=\"line\">mnist = input_data.read_data_sets(<span class=\"string\">\"MNIST_data/\"</span>)</span><br><span class=\"line\">images = mnist.train.images</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">xavier_initializer</span><span class=\"params\">(shape)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> tf.random_normal(shape=shape, stddev=<span class=\"number\">1.0</span> / shape[<span class=\"number\">0</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Generator</span></span><br><span class=\"line\">z_size = <span class=\"number\">100</span>  <span class=\"comment\"># maybe larger</span></span><br><span class=\"line\">g_w1_size = <span class=\"number\">400</span></span><br><span class=\"line\">g_out_size = <span class=\"number\">28</span> * <span class=\"number\">28</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Discriminator</span></span><br><span class=\"line\">x_size = <span class=\"number\">28</span> * <span class=\"number\">28</span></span><br><span class=\"line\">d_w1_size = <span class=\"number\">400</span></span><br><span class=\"line\">d_out_size = <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">z = tf.placeholder(<span class=\"string\">'float'</span>, shape=(<span class=\"keyword\">None</span>, z_size))</span><br><span class=\"line\">X = tf.placeholder(<span class=\"string\">'float'</span>, shape=(<span class=\"keyword\">None</span>, x_size))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># use dict to share variables</span></span><br><span class=\"line\">g_weights = &#123;</span><br><span class=\"line\">    <span class=\"string\">'w1'</span>: tf.Variable(xavier_initializer(shape=(z_size, g_w1_size))),</span><br><span class=\"line\">    <span class=\"string\">'b1'</span>: tf.Variable(tf.zeros(shape=[g_w1_size])),</span><br><span class=\"line\">    <span class=\"string\">'out'</span>: tf.Variable(xavier_initializer(shape=(g_w1_size, g_out_size))),</span><br><span class=\"line\">    <span class=\"string\">'b2'</span>: tf.Variable(tf.zeros(shape=[g_out_size])),</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">d_weights = &#123;</span><br><span class=\"line\">    <span class=\"string\">'w1'</span>: tf.Variable(xavier_initializer(shape=(x_size, d_w1_size))),</span><br><span class=\"line\">    <span class=\"string\">'b1'</span>: tf.Variable(tf.zeros(shape=[d_w1_size])),</span><br><span class=\"line\">    <span class=\"string\">'out'</span>: tf.Variable(xavier_initializer(shape=(d_w1_size, d_out_size))),</span><br><span class=\"line\">    <span class=\"string\">'b2'</span>: tf.Variable(tf.zeros(shape=[d_out_size])),</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">G</span><span class=\"params\">(z, w=g_weights)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># here tanh is better than relu</span></span><br><span class=\"line\">    h1 = tf.tanh(tf.matmul(z, w[<span class=\"string\">'w1'</span>]) + w[<span class=\"string\">'b1'</span>])</span><br><span class=\"line\">    <span class=\"comment\"># pixel output is in range [0, 255]</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> tf.sigmoid(tf.matmul(h1, w[<span class=\"string\">'out'</span>]) + w[<span class=\"string\">'b2'</span>]) * <span class=\"number\">255</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">D</span><span class=\"params\">(x, w=d_weights)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># here tanh is better than relu</span></span><br><span class=\"line\">    h1 = tf.tanh(tf.matmul(x, w[<span class=\"string\">'w1'</span>]) + w[<span class=\"string\">'b1'</span>])</span><br><span class=\"line\">    h2 = tf.matmul(h1, w[<span class=\"string\">'out'</span>]) + w[<span class=\"string\">'b2'</span>]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> h2  <span class=\"comment\"># use h2 to calculate logits loss</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generate_z</span><span class=\"params\">(n=<span class=\"number\">1</span>)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> np.random.normal(size=(n, z_size))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">sample = G(z)</span><br><span class=\"line\"></span><br><span class=\"line\">dout_real = D(X)</span><br><span class=\"line\">dout_fake = D(G(z))</span><br><span class=\"line\"></span><br><span class=\"line\">G_obj = tf.reduce_mean(</span><br><span class=\"line\">    tf.nn.sigmoid_cross_entropy_with_logits(logits=dout_fake, labels=tf.ones_like(dout_fake)))</span><br><span class=\"line\">D_obj_real = tf.reduce_mean(  <span class=\"comment\"># use single side smoothing</span></span><br><span class=\"line\">    tf.nn.sigmoid_cross_entropy_with_logits(logits=dout_real, labels=(tf.ones_like(dout_real) - <span class=\"number\">0.1</span>)))</span><br><span class=\"line\">D_obj_fake = tf.reduce_mean(</span><br><span class=\"line\">    tf.nn.sigmoid_cross_entropy_with_logits(logits=dout_fake, labels=tf.zeros_like(dout_fake)))</span><br><span class=\"line\">D_obj = D_obj_real + D_obj_fake</span><br><span class=\"line\"></span><br><span class=\"line\">G_opt = tf.train.AdamOptimizer().minimize(G_obj, var_list=g_weights.values())</span><br><span class=\"line\">D_opt = tf.train.AdamOptimizer().minimize(D_obj, var_list=d_weights.values())</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Training</span></span><br><span class=\"line\">batch_size = <span class=\"number\">128</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">with</span> tf.Session() <span class=\"keyword\">as</span> sess:</span><br><span class=\"line\">    sess.run(tf.global_variables_initializer())</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">200</span>):</span><br><span class=\"line\">        sess.run(D_opt, feed_dict=&#123;</span><br><span class=\"line\">            X: images[np.random.choice(range(len(images)), batch_size)].reshape(batch_size, x_size),</span><br><span class=\"line\">            z: generate_z(batch_size),</span><br><span class=\"line\">        &#125;)</span><br><span class=\"line\">        <span class=\"comment\"># run two phases of generator</span></span><br><span class=\"line\">        sess.run(G_opt, feed_dict=&#123;</span><br><span class=\"line\">            z: generate_z(batch_size)</span><br><span class=\"line\">        &#125;)</span><br><span class=\"line\">        sess.run(G_opt, feed_dict=&#123;</span><br><span class=\"line\">            z: generate_z(batch_size)</span><br><span class=\"line\">        &#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">        g_cost = sess.run(G_obj, feed_dict=&#123;z: generate_z(batch_size)&#125;)</span><br><span class=\"line\">        d_cost = sess.run(D_obj, feed_dict=&#123;</span><br><span class=\"line\">            X: images[np.random.choice(range(len(images)), batch_size)].reshape(batch_size, x_size),</span><br><span class=\"line\">            z: generate_z(batch_size),</span><br><span class=\"line\">        &#125;)</span><br><span class=\"line\">        image = sess.run(G(z), feed_dict=&#123;z: generate_z()&#125;)</span><br><span class=\"line\">        df = sess.run(tf.sigmoid(dout_fake), feed_dict=&#123;z: generate_z()&#125;)</span><br><span class=\"line\">        <span class=\"comment\"># print i, G cost, D cost, image max pixel, D output of fake</span></span><br><span class=\"line\">        print(i, g_cost, d_cost, image.max(), df[<span class=\"number\">0</span>][<span class=\"number\">0</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># You may wish to save or plot the image generated</span></span><br><span class=\"line\">    <span class=\"comment\"># to see how it looks like</span></span><br><span class=\"line\">    image = sess.run(G(z), feed_dict=&#123;z: generate_z()&#125;)</span><br><span class=\"line\">    image1 = image[<span class=\"number\">0</span>].reshape([<span class=\"number\">28</span>, <span class=\"number\">28</span>])</span><br><span class=\"line\">    <span class=\"comment\"># print(image1)</span></span><br><span class=\"line\">    <span class=\"comment\"># im = Image.fromarray(image1)</span></span><br><span class=\"line\">    <span class=\"comment\"># im.show()</span></span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"GAN\"><a href=\"#GAN\" class=\"headerlink\" title=\"GAN\"></a>GAN</h1><p>2014 Goodfellow <strong>GAN</strong></p>\n<ul>\n<li></li>\n<li></li>\n<li> D G</li>\n<li>G  D </li>\n</ul>\n<p>G  D G D  D  G D  $1/2$  G </p>\n<p> $x$  $p_g(x)$  $p_z(z)$  $G(z;\\theta_g)$  $G$  $D(x;\\theta_d)$  $x$  $D$  $\\log (1-D(G(z)))$ G  $G$ </p>\n<script type=\"math/tex; mode=display\">\n\\min_G \\max_D V(D,G) = \\mathbb{E}_{x \\sim p_{\\text{data}}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_{z}(z)}[\\log (1-D(G(z)))]</script><h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n<script type=\"math/tex; mode=display\">\n\\mathbb{E}_{z \\sim p_{z}(z)}[\\log (1-D(G(z)))] = \\mathbb{E}_{x \\sim p_{g}(x)}[\\log (1-D(x)]</script><p> $V(D,G)$  $x$ </p>\n<script type=\"math/tex; mode=display\">\nV(G, D) = \\int_{x} p_{\\text{data}}(x) \\log (D(x)) + p_g(x) \\log (1-D(x)) \\, dx</script><p> $p_{\\text{data}}(x)$  $p_g(x)$ </p>\n<script type=\"math/tex; mode=display\">\nf(y) = a \\log y + b \\log (1-y)</script><p></p>\n<script type=\"math/tex; mode=display\">\nf'(y) = 0 \\Rightarrow \\frac{a}{y}-\\frac{b}{1-y}=0 \\Rightarrow y=\\frac{a}{a+b}</script><p></p>\n<script type=\"math/tex; mode=display\">\nf''(y) \\Big|_{y=\\frac{a}{a+b}} = -\\frac{a}{\\left( \\frac{a}{a+b} \\right)^{2}}-\\frac{b}{1-\\left( \\frac{a}{a+b} \\right)^{2}}<0</script><p> $\\frac{a}{a+b}$  $a=p_{\\text{data}}(x), b=p_g(x), y=D(x)$ </p>\n<script type=\"math/tex; mode=display\">\nD(x) = \\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)}</script><h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>GAN  $p_{\\text{data}}=p_g$ </p>\n<script type=\"math/tex; mode=display\">\nD_G^{\\ast}=\\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)}=\\frac{1}{2}</script><p>Goodfellow  min max </p>\n<h1 id=\"-TensorFlow--MNIST-\"><a href=\"#-TensorFlow--MNIST-\" class=\"headerlink\" title=\" TensorFlow  MNIST \"></a> TensorFlow  MNIST </h1><p> CNN G  D </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">from</span> tensorflow.examples.tutorials.mnist <span class=\"keyword\">import</span> input_data</span><br><span class=\"line\"><span class=\"comment\"># from PIL import Image</span></span><br><span class=\"line\"></span><br><span class=\"line\">mnist = input_data.read_data_sets(<span class=\"string\">\"MNIST_data/\"</span>)</span><br><span class=\"line\">images = mnist.train.images</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">xavier_initializer</span><span class=\"params\">(shape)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> tf.random_normal(shape=shape, stddev=<span class=\"number\">1.0</span> / shape[<span class=\"number\">0</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Generator</span></span><br><span class=\"line\">z_size = <span class=\"number\">100</span>  <span class=\"comment\"># maybe larger</span></span><br><span class=\"line\">g_w1_size = <span class=\"number\">400</span></span><br><span class=\"line\">g_out_size = <span class=\"number\">28</span> * <span class=\"number\">28</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Discriminator</span></span><br><span class=\"line\">x_size = <span class=\"number\">28</span> * <span class=\"number\">28</span></span><br><span class=\"line\">d_w1_size = <span class=\"number\">400</span></span><br><span class=\"line\">d_out_size = <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">z = tf.placeholder(<span class=\"string\">'float'</span>, shape=(<span class=\"keyword\">None</span>, z_size))</span><br><span class=\"line\">X = tf.placeholder(<span class=\"string\">'float'</span>, shape=(<span class=\"keyword\">None</span>, x_size))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># use dict to share variables</span></span><br><span class=\"line\">g_weights = &#123;</span><br><span class=\"line\">    <span class=\"string\">'w1'</span>: tf.Variable(xavier_initializer(shape=(z_size, g_w1_size))),</span><br><span class=\"line\">    <span class=\"string\">'b1'</span>: tf.Variable(tf.zeros(shape=[g_w1_size])),</span><br><span class=\"line\">    <span class=\"string\">'out'</span>: tf.Variable(xavier_initializer(shape=(g_w1_size, g_out_size))),</span><br><span class=\"line\">    <span class=\"string\">'b2'</span>: tf.Variable(tf.zeros(shape=[g_out_size])),</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">d_weights = &#123;</span><br><span class=\"line\">    <span class=\"string\">'w1'</span>: tf.Variable(xavier_initializer(shape=(x_size, d_w1_size))),</span><br><span class=\"line\">    <span class=\"string\">'b1'</span>: tf.Variable(tf.zeros(shape=[d_w1_size])),</span><br><span class=\"line\">    <span class=\"string\">'out'</span>: tf.Variable(xavier_initializer(shape=(d_w1_size, d_out_size))),</span><br><span class=\"line\">    <span class=\"string\">'b2'</span>: tf.Variable(tf.zeros(shape=[d_out_size])),</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">G</span><span class=\"params\">(z, w=g_weights)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># here tanh is better than relu</span></span><br><span class=\"line\">    h1 = tf.tanh(tf.matmul(z, w[<span class=\"string\">'w1'</span>]) + w[<span class=\"string\">'b1'</span>])</span><br><span class=\"line\">    <span class=\"comment\"># pixel output is in range [0, 255]</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> tf.sigmoid(tf.matmul(h1, w[<span class=\"string\">'out'</span>]) + w[<span class=\"string\">'b2'</span>]) * <span class=\"number\">255</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">D</span><span class=\"params\">(x, w=d_weights)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># here tanh is better than relu</span></span><br><span class=\"line\">    h1 = tf.tanh(tf.matmul(x, w[<span class=\"string\">'w1'</span>]) + w[<span class=\"string\">'b1'</span>])</span><br><span class=\"line\">    h2 = tf.matmul(h1, w[<span class=\"string\">'out'</span>]) + w[<span class=\"string\">'b2'</span>]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> h2  <span class=\"comment\"># use h2 to calculate logits loss</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generate_z</span><span class=\"params\">(n=<span class=\"number\">1</span>)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> np.random.normal(size=(n, z_size))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">sample = G(z)</span><br><span class=\"line\"></span><br><span class=\"line\">dout_real = D(X)</span><br><span class=\"line\">dout_fake = D(G(z))</span><br><span class=\"line\"></span><br><span class=\"line\">G_obj = tf.reduce_mean(</span><br><span class=\"line\">    tf.nn.sigmoid_cross_entropy_with_logits(logits=dout_fake, labels=tf.ones_like(dout_fake)))</span><br><span class=\"line\">D_obj_real = tf.reduce_mean(  <span class=\"comment\"># use single side smoothing</span></span><br><span class=\"line\">    tf.nn.sigmoid_cross_entropy_with_logits(logits=dout_real, labels=(tf.ones_like(dout_real) - <span class=\"number\">0.1</span>)))</span><br><span class=\"line\">D_obj_fake = tf.reduce_mean(</span><br><span class=\"line\">    tf.nn.sigmoid_cross_entropy_with_logits(logits=dout_fake, labels=tf.zeros_like(dout_fake)))</span><br><span class=\"line\">D_obj = D_obj_real + D_obj_fake</span><br><span class=\"line\"></span><br><span class=\"line\">G_opt = tf.train.AdamOptimizer().minimize(G_obj, var_list=g_weights.values())</span><br><span class=\"line\">D_opt = tf.train.AdamOptimizer().minimize(D_obj, var_list=d_weights.values())</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Training</span></span><br><span class=\"line\">batch_size = <span class=\"number\">128</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">with</span> tf.Session() <span class=\"keyword\">as</span> sess:</span><br><span class=\"line\">    sess.run(tf.global_variables_initializer())</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">200</span>):</span><br><span class=\"line\">        sess.run(D_opt, feed_dict=&#123;</span><br><span class=\"line\">            X: images[np.random.choice(range(len(images)), batch_size)].reshape(batch_size, x_size),</span><br><span class=\"line\">            z: generate_z(batch_size),</span><br><span class=\"line\">        &#125;)</span><br><span class=\"line\">        <span class=\"comment\"># run two phases of generator</span></span><br><span class=\"line\">        sess.run(G_opt, feed_dict=&#123;</span><br><span class=\"line\">            z: generate_z(batch_size)</span><br><span class=\"line\">        &#125;)</span><br><span class=\"line\">        sess.run(G_opt, feed_dict=&#123;</span><br><span class=\"line\">            z: generate_z(batch_size)</span><br><span class=\"line\">        &#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">        g_cost = sess.run(G_obj, feed_dict=&#123;z: generate_z(batch_size)&#125;)</span><br><span class=\"line\">        d_cost = sess.run(D_obj, feed_dict=&#123;</span><br><span class=\"line\">            X: images[np.random.choice(range(len(images)), batch_size)].reshape(batch_size, x_size),</span><br><span class=\"line\">            z: generate_z(batch_size),</span><br><span class=\"line\">        &#125;)</span><br><span class=\"line\">        image = sess.run(G(z), feed_dict=&#123;z: generate_z()&#125;)</span><br><span class=\"line\">        df = sess.run(tf.sigmoid(dout_fake), feed_dict=&#123;z: generate_z()&#125;)</span><br><span class=\"line\">        <span class=\"comment\"># print i, G cost, D cost, image max pixel, D output of fake</span></span><br><span class=\"line\">        print(i, g_cost, d_cost, image.max(), df[<span class=\"number\">0</span>][<span class=\"number\">0</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># You may wish to save or plot the image generated</span></span><br><span class=\"line\">    <span class=\"comment\"># to see how it looks like</span></span><br><span class=\"line\">    image = sess.run(G(z), feed_dict=&#123;z: generate_z()&#125;)</span><br><span class=\"line\">    image1 = image[<span class=\"number\">0</span>].reshape([<span class=\"number\">28</span>, <span class=\"number\">28</span>])</span><br><span class=\"line\">    <span class=\"comment\"># print(image1)</span></span><br><span class=\"line\">    <span class=\"comment\"># im = Image.fromarray(image1)</span></span><br><span class=\"line\">    <span class=\"comment\"># im.show()</span></span><br></pre></td></tr></table></figure>\n"},{"title":" Wasserstein GAN  TensorFlow ","date":"2017-11-04T03:21:51.000Z","description":" Wasserstein GAN  TensorFlow ","_content":"\n#  GAN \n\n loss  $P_{\\text{data}}$  $P_g$  JS  $P_{\\text{data}}$  $P_g$ JS  $\\log 2$  $0$ \n\n\n$$\nKL(P_{g} \\Vert P_{\\text{data}}) - 2 JS(P_{\\text{data}} \\Vert P_{g})\n$$\n KL  JS  collapse mode\n\n# Wasserstein GAN \n\nWasserstein GAN  GAN \n\n-  sigmoid\n-  loss  log\n-  c\n-  momentum  Adam RMSPropSGD \n\nPS Adam loss  Adam  cos  loss  RMSProp \n\nPPS learning rate $\\alpha=0.00005$ \n\n#  TensorFlow  MNIST \n\n```python\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport os\n\n\nmb_size = 32\nX_dim = 784\nz_dim = 10\nh_dim = 128\n\nmnist = input_data.read_data_sets('MNIST_data/', one_hot=True)\n\n\ndef plot(samples):\n    fig = plt.figure(figsize=(4, 4))\n    gs = gridspec.GridSpec(4, 4)\n    gs.update(wspace=0.05, hspace=0.05)\n\n    for i, sample in enumerate(samples):\n        ax = plt.subplot(gs[i])\n        plt.axis('off')\n        ax.set_xticklabels([])\n        ax.set_yticklabels([])\n        ax.set_aspect('equal')\n        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n\n    return fig\n\n\ndef xavier_init(size):\n    in_dim = size[0]\n    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n    return tf.random_normal(shape=size, stddev=xavier_stddev)\n\n\nX = tf.placeholder(tf.float32, shape=[None, X_dim])\n\nD_W1 = tf.Variable(xavier_init([X_dim, h_dim]))\nD_b1 = tf.Variable(tf.zeros(shape=[h_dim]))\n\nD_W2 = tf.Variable(xavier_init([h_dim, 1]))\nD_b2 = tf.Variable(tf.zeros(shape=[1]))\n\ntheta_D = [D_W1, D_W2, D_b1, D_b2]\n\n\nz = tf.placeholder(tf.float32, shape=[None, z_dim])\n\nG_W1 = tf.Variable(xavier_init([z_dim, h_dim]))\nG_b1 = tf.Variable(tf.zeros(shape=[h_dim]))\n\nG_W2 = tf.Variable(xavier_init([h_dim, X_dim]))\nG_b2 = tf.Variable(tf.zeros(shape=[X_dim]))\n\ntheta_G = [G_W1, G_W2, G_b1, G_b2]\n\n\ndef sample_z(m, n):\n    return np.random.uniform(-1., 1., size=[m, n])\n\n\ndef generator(z):\n    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)\n    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2\n    G_prob = tf.nn.sigmoid(G_log_prob)\n    return G_prob\n\n\ndef discriminator(x):\n    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)\n    out = tf.matmul(D_h1, D_W2) + D_b2\n    return out\n\n\nG_sample = generator(z)\nD_real = discriminator(X)\nD_fake = discriminator(G_sample)\n\nD_loss = tf.reduce_mean(D_real) - tf.reduce_mean(D_fake)\nG_loss = -tf.reduce_mean(D_fake)\n\nD_solver = (tf.train.RMSPropOptimizer(learning_rate=1e-4)\n            .minimize(-D_loss, var_list=theta_D))\nG_solver = (tf.train.RMSPropOptimizer(learning_rate=1e-4)\n            .minimize(G_loss, var_list=theta_G))\n\nclip_D = [p.assign(tf.clip_by_value(p, -0.01, 0.01)) for p in theta_D]\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\nif not os.path.exists('out/'):\n    os.makedirs('out/')\n\ni = 0\n\nfor it in range(1000000):\n    for _ in range(5):\n        X_mb, _ = mnist.train.next_batch(mb_size)\n\n        _, D_loss_curr, _ = sess.run(\n            [D_solver, D_loss, clip_D],\n            feed_dict={X: X_mb, z: sample_z(mb_size, z_dim)}\n        )\n\n    _, G_loss_curr = sess.run(\n        [G_solver, G_loss],\n        feed_dict={z: sample_z(mb_size, z_dim)}\n    )\n\n    if it % 100 == 0:\n        print('Iter: {}; D loss: {:.4}; G_loss: {:.4}'.format(it, D_loss_curr, G_loss_curr))\n\n        if it % 1000 == 0:\n            samples = sess.run(G_sample, feed_dict={z: sample_z(16, z_dim)})\n\n            fig = plot(samples)\n            plt.savefig('out/{}.png'.format(str(i).zfill(3)), bbox_inches='tight')\n            i += 1\n            plt.close(fig)\n```\n\n","source":"_posts/GAN-02.md","raw":"---\ntitle:  Wasserstein GAN  TensorFlow \ndate: 2017-11-04 11:21:51\ncategories: ML\ntags:\n     - Deep learning\n     - GAN\ndescription:  Wasserstein GAN  TensorFlow \n---\n\n#  GAN \n\n loss  $P_{\\text{data}}$  $P_g$  JS  $P_{\\text{data}}$  $P_g$ JS  $\\log 2$  $0$ \n\n\n$$\nKL(P_{g} \\Vert P_{\\text{data}}) - 2 JS(P_{\\text{data}} \\Vert P_{g})\n$$\n KL  JS  collapse mode\n\n# Wasserstein GAN \n\nWasserstein GAN  GAN \n\n-  sigmoid\n-  loss  log\n-  c\n-  momentum  Adam RMSPropSGD \n\nPS Adam loss  Adam  cos  loss  RMSProp \n\nPPS learning rate $\\alpha=0.00005$ \n\n#  TensorFlow  MNIST \n\n```python\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport os\n\n\nmb_size = 32\nX_dim = 784\nz_dim = 10\nh_dim = 128\n\nmnist = input_data.read_data_sets('MNIST_data/', one_hot=True)\n\n\ndef plot(samples):\n    fig = plt.figure(figsize=(4, 4))\n    gs = gridspec.GridSpec(4, 4)\n    gs.update(wspace=0.05, hspace=0.05)\n\n    for i, sample in enumerate(samples):\n        ax = plt.subplot(gs[i])\n        plt.axis('off')\n        ax.set_xticklabels([])\n        ax.set_yticklabels([])\n        ax.set_aspect('equal')\n        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n\n    return fig\n\n\ndef xavier_init(size):\n    in_dim = size[0]\n    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n    return tf.random_normal(shape=size, stddev=xavier_stddev)\n\n\nX = tf.placeholder(tf.float32, shape=[None, X_dim])\n\nD_W1 = tf.Variable(xavier_init([X_dim, h_dim]))\nD_b1 = tf.Variable(tf.zeros(shape=[h_dim]))\n\nD_W2 = tf.Variable(xavier_init([h_dim, 1]))\nD_b2 = tf.Variable(tf.zeros(shape=[1]))\n\ntheta_D = [D_W1, D_W2, D_b1, D_b2]\n\n\nz = tf.placeholder(tf.float32, shape=[None, z_dim])\n\nG_W1 = tf.Variable(xavier_init([z_dim, h_dim]))\nG_b1 = tf.Variable(tf.zeros(shape=[h_dim]))\n\nG_W2 = tf.Variable(xavier_init([h_dim, X_dim]))\nG_b2 = tf.Variable(tf.zeros(shape=[X_dim]))\n\ntheta_G = [G_W1, G_W2, G_b1, G_b2]\n\n\ndef sample_z(m, n):\n    return np.random.uniform(-1., 1., size=[m, n])\n\n\ndef generator(z):\n    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)\n    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2\n    G_prob = tf.nn.sigmoid(G_log_prob)\n    return G_prob\n\n\ndef discriminator(x):\n    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)\n    out = tf.matmul(D_h1, D_W2) + D_b2\n    return out\n\n\nG_sample = generator(z)\nD_real = discriminator(X)\nD_fake = discriminator(G_sample)\n\nD_loss = tf.reduce_mean(D_real) - tf.reduce_mean(D_fake)\nG_loss = -tf.reduce_mean(D_fake)\n\nD_solver = (tf.train.RMSPropOptimizer(learning_rate=1e-4)\n            .minimize(-D_loss, var_list=theta_D))\nG_solver = (tf.train.RMSPropOptimizer(learning_rate=1e-4)\n            .minimize(G_loss, var_list=theta_G))\n\nclip_D = [p.assign(tf.clip_by_value(p, -0.01, 0.01)) for p in theta_D]\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\nif not os.path.exists('out/'):\n    os.makedirs('out/')\n\ni = 0\n\nfor it in range(1000000):\n    for _ in range(5):\n        X_mb, _ = mnist.train.next_batch(mb_size)\n\n        _, D_loss_curr, _ = sess.run(\n            [D_solver, D_loss, clip_D],\n            feed_dict={X: X_mb, z: sample_z(mb_size, z_dim)}\n        )\n\n    _, G_loss_curr = sess.run(\n        [G_solver, G_loss],\n        feed_dict={z: sample_z(mb_size, z_dim)}\n    )\n\n    if it % 100 == 0:\n        print('Iter: {}; D loss: {:.4}; G_loss: {:.4}'.format(it, D_loss_curr, G_loss_curr))\n\n        if it % 1000 == 0:\n            samples = sess.run(G_sample, feed_dict={z: sample_z(16, z_dim)})\n\n            fig = plot(samples)\n            plt.savefig('out/{}.png'.format(str(i).zfill(3)), bbox_inches='tight')\n            i += 1\n            plt.close(fig)\n```\n\n","slug":"GAN-02","published":1,"updated":"2021-07-19T13:03:31.695Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckwrktiv70004qxt6ceoapanx","content":"<h1 id=\"-GAN-\"><a href=\"#-GAN-\" class=\"headerlink\" title=\" GAN \"></a> GAN </h1><p> loss  $P_{\\text{data}}$  $P_g$  JS  $P_{\\text{data}}$  $P_g$ JS  $\\log 2$  $0$ </p>\n<p></p>\n<script type=\"math/tex; mode=display\">\nKL(P_{g} \\Vert P_{\\text{data}}) - 2 JS(P_{\\text{data}} \\Vert P_{g})</script><p> KL  JS  collapse mode</p>\n<h1 id=\"Wasserstein-GAN-\"><a href=\"#Wasserstein-GAN-\" class=\"headerlink\" title=\"Wasserstein GAN \"></a>Wasserstein GAN </h1><p>Wasserstein GAN  GAN </p>\n<ul>\n<li> sigmoid</li>\n<li> loss  log</li>\n<li> c</li>\n<li> momentum  Adam RMSPropSGD </li>\n</ul>\n<p>PS Adam loss  Adam  cos  loss  RMSProp </p>\n<p>PPS learning rate $\\alpha=0.00005$ </p>\n<h1 id=\"-TensorFlow--MNIST-\"><a href=\"#-TensorFlow--MNIST-\" class=\"headerlink\" title=\" TensorFlow  MNIST \"></a> TensorFlow  MNIST </h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf</span><br><span class=\"line\"><span class=\"keyword\">from</span> tensorflow.examples.tutorials.mnist <span class=\"keyword\">import</span> input_data</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.gridspec <span class=\"keyword\">as</span> gridspec</span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">mb_size = <span class=\"number\">32</span></span><br><span class=\"line\">X_dim = <span class=\"number\">784</span></span><br><span class=\"line\">z_dim = <span class=\"number\">10</span></span><br><span class=\"line\">h_dim = <span class=\"number\">128</span></span><br><span class=\"line\"></span><br><span class=\"line\">mnist = input_data.read_data_sets(<span class=\"string\">'MNIST_data/'</span>, one_hot=<span class=\"keyword\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">plot</span><span class=\"params\">(samples)</span>:</span></span><br><span class=\"line\">    fig = plt.figure(figsize=(<span class=\"number\">4</span>, <span class=\"number\">4</span>))</span><br><span class=\"line\">    gs = gridspec.GridSpec(<span class=\"number\">4</span>, <span class=\"number\">4</span>)</span><br><span class=\"line\">    gs.update(wspace=<span class=\"number\">0.05</span>, hspace=<span class=\"number\">0.05</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i, sample <span class=\"keyword\">in</span> enumerate(samples):</span><br><span class=\"line\">        ax = plt.subplot(gs[i])</span><br><span class=\"line\">        plt.axis(<span class=\"string\">'off'</span>)</span><br><span class=\"line\">        ax.set_xticklabels([])</span><br><span class=\"line\">        ax.set_yticklabels([])</span><br><span class=\"line\">        ax.set_aspect(<span class=\"string\">'equal'</span>)</span><br><span class=\"line\">        plt.imshow(sample.reshape(<span class=\"number\">28</span>, <span class=\"number\">28</span>), cmap=<span class=\"string\">'Greys_r'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> fig</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">xavier_init</span><span class=\"params\">(size)</span>:</span></span><br><span class=\"line\">    in_dim = size[<span class=\"number\">0</span>]</span><br><span class=\"line\">    xavier_stddev = <span class=\"number\">1.</span> / tf.sqrt(in_dim / <span class=\"number\">2.</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> tf.random_normal(shape=size, stddev=xavier_stddev)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">X = tf.placeholder(tf.float32, shape=[<span class=\"keyword\">None</span>, X_dim])</span><br><span class=\"line\"></span><br><span class=\"line\">D_W1 = tf.Variable(xavier_init([X_dim, h_dim]))</span><br><span class=\"line\">D_b1 = tf.Variable(tf.zeros(shape=[h_dim]))</span><br><span class=\"line\"></span><br><span class=\"line\">D_W2 = tf.Variable(xavier_init([h_dim, <span class=\"number\">1</span>]))</span><br><span class=\"line\">D_b2 = tf.Variable(tf.zeros(shape=[<span class=\"number\">1</span>]))</span><br><span class=\"line\"></span><br><span class=\"line\">theta_D = [D_W1, D_W2, D_b1, D_b2]</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">z = tf.placeholder(tf.float32, shape=[<span class=\"keyword\">None</span>, z_dim])</span><br><span class=\"line\"></span><br><span class=\"line\">G_W1 = tf.Variable(xavier_init([z_dim, h_dim]))</span><br><span class=\"line\">G_b1 = tf.Variable(tf.zeros(shape=[h_dim]))</span><br><span class=\"line\"></span><br><span class=\"line\">G_W2 = tf.Variable(xavier_init([h_dim, X_dim]))</span><br><span class=\"line\">G_b2 = tf.Variable(tf.zeros(shape=[X_dim]))</span><br><span class=\"line\"></span><br><span class=\"line\">theta_G = [G_W1, G_W2, G_b1, G_b2]</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sample_z</span><span class=\"params\">(m, n)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> np.random.uniform(<span class=\"number\">-1.</span>, <span class=\"number\">1.</span>, size=[m, n])</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generator</span><span class=\"params\">(z)</span>:</span></span><br><span class=\"line\">    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)</span><br><span class=\"line\">    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2</span><br><span class=\"line\">    G_prob = tf.nn.sigmoid(G_log_prob)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> G_prob</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">discriminator</span><span class=\"params\">(x)</span>:</span></span><br><span class=\"line\">    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)</span><br><span class=\"line\">    out = tf.matmul(D_h1, D_W2) + D_b2</span><br><span class=\"line\">    <span class=\"keyword\">return</span> out</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">G_sample = generator(z)</span><br><span class=\"line\">D_real = discriminator(X)</span><br><span class=\"line\">D_fake = discriminator(G_sample)</span><br><span class=\"line\"></span><br><span class=\"line\">D_loss = tf.reduce_mean(D_real) - tf.reduce_mean(D_fake)</span><br><span class=\"line\">G_loss = -tf.reduce_mean(D_fake)</span><br><span class=\"line\"></span><br><span class=\"line\">D_solver = (tf.train.RMSPropOptimizer(learning_rate=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\">            .minimize(-D_loss, var_list=theta_D))</span><br><span class=\"line\">G_solver = (tf.train.RMSPropOptimizer(learning_rate=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\">            .minimize(G_loss, var_list=theta_G))</span><br><span class=\"line\"></span><br><span class=\"line\">clip_D = [p.assign(tf.clip_by_value(p, <span class=\"number\">-0.01</span>, <span class=\"number\">0.01</span>)) <span class=\"keyword\">for</span> p <span class=\"keyword\">in</span> theta_D]</span><br><span class=\"line\"></span><br><span class=\"line\">sess = tf.Session()</span><br><span class=\"line\">sess.run(tf.global_variables_initializer())</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> <span class=\"keyword\">not</span> os.path.exists(<span class=\"string\">'out/'</span>):</span><br><span class=\"line\">    os.makedirs(<span class=\"string\">'out/'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">i = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> it <span class=\"keyword\">in</span> range(<span class=\"number\">1000000</span>):</span><br><span class=\"line\">    <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> range(<span class=\"number\">5</span>):</span><br><span class=\"line\">        X_mb, _ = mnist.train.next_batch(mb_size)</span><br><span class=\"line\"></span><br><span class=\"line\">        _, D_loss_curr, _ = sess.run(</span><br><span class=\"line\">            [D_solver, D_loss, clip_D],</span><br><span class=\"line\">            feed_dict=&#123;X: X_mb, z: sample_z(mb_size, z_dim)&#125;</span><br><span class=\"line\">        )</span><br><span class=\"line\"></span><br><span class=\"line\">    _, G_loss_curr = sess.run(</span><br><span class=\"line\">        [G_solver, G_loss],</span><br><span class=\"line\">        feed_dict=&#123;z: sample_z(mb_size, z_dim)&#125;</span><br><span class=\"line\">    )</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> it % <span class=\"number\">100</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">        print(<span class=\"string\">'Iter: &#123;&#125;; D loss: &#123;:.4&#125;; G_loss: &#123;:.4&#125;'</span>.format(it, D_loss_curr, G_loss_curr))</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> it % <span class=\"number\">1000</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            samples = sess.run(G_sample, feed_dict=&#123;z: sample_z(<span class=\"number\">16</span>, z_dim)&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">            fig = plot(samples)</span><br><span class=\"line\">            plt.savefig(<span class=\"string\">'out/&#123;&#125;.png'</span>.format(str(i).zfill(<span class=\"number\">3</span>)), bbox_inches=<span class=\"string\">'tight'</span>)</span><br><span class=\"line\">            i += <span class=\"number\">1</span></span><br><span class=\"line\">            plt.close(fig)</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"-GAN-\"><a href=\"#-GAN-\" class=\"headerlink\" title=\" GAN \"></a> GAN </h1><p> loss  $P_{\\text{data}}$  $P_g$  JS  $P_{\\text{data}}$  $P_g$ JS  $\\log 2$  $0$ </p>\n<p></p>\n<script type=\"math/tex; mode=display\">\nKL(P_{g} \\Vert P_{\\text{data}}) - 2 JS(P_{\\text{data}} \\Vert P_{g})</script><p> KL  JS  collapse mode</p>\n<h1 id=\"Wasserstein-GAN-\"><a href=\"#Wasserstein-GAN-\" class=\"headerlink\" title=\"Wasserstein GAN \"></a>Wasserstein GAN </h1><p>Wasserstein GAN  GAN </p>\n<ul>\n<li> sigmoid</li>\n<li> loss  log</li>\n<li> c</li>\n<li> momentum  Adam RMSPropSGD </li>\n</ul>\n<p>PS Adam loss  Adam  cos  loss  RMSProp </p>\n<p>PPS learning rate $\\alpha=0.00005$ </p>\n<h1 id=\"-TensorFlow--MNIST-\"><a href=\"#-TensorFlow--MNIST-\" class=\"headerlink\" title=\" TensorFlow  MNIST \"></a> TensorFlow  MNIST </h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf</span><br><span class=\"line\"><span class=\"keyword\">from</span> tensorflow.examples.tutorials.mnist <span class=\"keyword\">import</span> input_data</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.gridspec <span class=\"keyword\">as</span> gridspec</span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">mb_size = <span class=\"number\">32</span></span><br><span class=\"line\">X_dim = <span class=\"number\">784</span></span><br><span class=\"line\">z_dim = <span class=\"number\">10</span></span><br><span class=\"line\">h_dim = <span class=\"number\">128</span></span><br><span class=\"line\"></span><br><span class=\"line\">mnist = input_data.read_data_sets(<span class=\"string\">'MNIST_data/'</span>, one_hot=<span class=\"keyword\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">plot</span><span class=\"params\">(samples)</span>:</span></span><br><span class=\"line\">    fig = plt.figure(figsize=(<span class=\"number\">4</span>, <span class=\"number\">4</span>))</span><br><span class=\"line\">    gs = gridspec.GridSpec(<span class=\"number\">4</span>, <span class=\"number\">4</span>)</span><br><span class=\"line\">    gs.update(wspace=<span class=\"number\">0.05</span>, hspace=<span class=\"number\">0.05</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i, sample <span class=\"keyword\">in</span> enumerate(samples):</span><br><span class=\"line\">        ax = plt.subplot(gs[i])</span><br><span class=\"line\">        plt.axis(<span class=\"string\">'off'</span>)</span><br><span class=\"line\">        ax.set_xticklabels([])</span><br><span class=\"line\">        ax.set_yticklabels([])</span><br><span class=\"line\">        ax.set_aspect(<span class=\"string\">'equal'</span>)</span><br><span class=\"line\">        plt.imshow(sample.reshape(<span class=\"number\">28</span>, <span class=\"number\">28</span>), cmap=<span class=\"string\">'Greys_r'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> fig</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">xavier_init</span><span class=\"params\">(size)</span>:</span></span><br><span class=\"line\">    in_dim = size[<span class=\"number\">0</span>]</span><br><span class=\"line\">    xavier_stddev = <span class=\"number\">1.</span> / tf.sqrt(in_dim / <span class=\"number\">2.</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> tf.random_normal(shape=size, stddev=xavier_stddev)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">X = tf.placeholder(tf.float32, shape=[<span class=\"keyword\">None</span>, X_dim])</span><br><span class=\"line\"></span><br><span class=\"line\">D_W1 = tf.Variable(xavier_init([X_dim, h_dim]))</span><br><span class=\"line\">D_b1 = tf.Variable(tf.zeros(shape=[h_dim]))</span><br><span class=\"line\"></span><br><span class=\"line\">D_W2 = tf.Variable(xavier_init([h_dim, <span class=\"number\">1</span>]))</span><br><span class=\"line\">D_b2 = tf.Variable(tf.zeros(shape=[<span class=\"number\">1</span>]))</span><br><span class=\"line\"></span><br><span class=\"line\">theta_D = [D_W1, D_W2, D_b1, D_b2]</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">z = tf.placeholder(tf.float32, shape=[<span class=\"keyword\">None</span>, z_dim])</span><br><span class=\"line\"></span><br><span class=\"line\">G_W1 = tf.Variable(xavier_init([z_dim, h_dim]))</span><br><span class=\"line\">G_b1 = tf.Variable(tf.zeros(shape=[h_dim]))</span><br><span class=\"line\"></span><br><span class=\"line\">G_W2 = tf.Variable(xavier_init([h_dim, X_dim]))</span><br><span class=\"line\">G_b2 = tf.Variable(tf.zeros(shape=[X_dim]))</span><br><span class=\"line\"></span><br><span class=\"line\">theta_G = [G_W1, G_W2, G_b1, G_b2]</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sample_z</span><span class=\"params\">(m, n)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> np.random.uniform(<span class=\"number\">-1.</span>, <span class=\"number\">1.</span>, size=[m, n])</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generator</span><span class=\"params\">(z)</span>:</span></span><br><span class=\"line\">    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)</span><br><span class=\"line\">    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2</span><br><span class=\"line\">    G_prob = tf.nn.sigmoid(G_log_prob)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> G_prob</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">discriminator</span><span class=\"params\">(x)</span>:</span></span><br><span class=\"line\">    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)</span><br><span class=\"line\">    out = tf.matmul(D_h1, D_W2) + D_b2</span><br><span class=\"line\">    <span class=\"keyword\">return</span> out</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">G_sample = generator(z)</span><br><span class=\"line\">D_real = discriminator(X)</span><br><span class=\"line\">D_fake = discriminator(G_sample)</span><br><span class=\"line\"></span><br><span class=\"line\">D_loss = tf.reduce_mean(D_real) - tf.reduce_mean(D_fake)</span><br><span class=\"line\">G_loss = -tf.reduce_mean(D_fake)</span><br><span class=\"line\"></span><br><span class=\"line\">D_solver = (tf.train.RMSPropOptimizer(learning_rate=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\">            .minimize(-D_loss, var_list=theta_D))</span><br><span class=\"line\">G_solver = (tf.train.RMSPropOptimizer(learning_rate=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\">            .minimize(G_loss, var_list=theta_G))</span><br><span class=\"line\"></span><br><span class=\"line\">clip_D = [p.assign(tf.clip_by_value(p, <span class=\"number\">-0.01</span>, <span class=\"number\">0.01</span>)) <span class=\"keyword\">for</span> p <span class=\"keyword\">in</span> theta_D]</span><br><span class=\"line\"></span><br><span class=\"line\">sess = tf.Session()</span><br><span class=\"line\">sess.run(tf.global_variables_initializer())</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> <span class=\"keyword\">not</span> os.path.exists(<span class=\"string\">'out/'</span>):</span><br><span class=\"line\">    os.makedirs(<span class=\"string\">'out/'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">i = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> it <span class=\"keyword\">in</span> range(<span class=\"number\">1000000</span>):</span><br><span class=\"line\">    <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> range(<span class=\"number\">5</span>):</span><br><span class=\"line\">        X_mb, _ = mnist.train.next_batch(mb_size)</span><br><span class=\"line\"></span><br><span class=\"line\">        _, D_loss_curr, _ = sess.run(</span><br><span class=\"line\">            [D_solver, D_loss, clip_D],</span><br><span class=\"line\">            feed_dict=&#123;X: X_mb, z: sample_z(mb_size, z_dim)&#125;</span><br><span class=\"line\">        )</span><br><span class=\"line\"></span><br><span class=\"line\">    _, G_loss_curr = sess.run(</span><br><span class=\"line\">        [G_solver, G_loss],</span><br><span class=\"line\">        feed_dict=&#123;z: sample_z(mb_size, z_dim)&#125;</span><br><span class=\"line\">    )</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> it % <span class=\"number\">100</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">        print(<span class=\"string\">'Iter: &#123;&#125;; D loss: &#123;:.4&#125;; G_loss: &#123;:.4&#125;'</span>.format(it, D_loss_curr, G_loss_curr))</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> it % <span class=\"number\">1000</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            samples = sess.run(G_sample, feed_dict=&#123;z: sample_z(<span class=\"number\">16</span>, z_dim)&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">            fig = plot(samples)</span><br><span class=\"line\">            plt.savefig(<span class=\"string\">'out/&#123;&#125;.png'</span>.format(str(i).zfill(<span class=\"number\">3</span>)), bbox_inches=<span class=\"string\">'tight'</span>)</span><br><span class=\"line\">            i += <span class=\"number\">1</span></span><br><span class=\"line\">            plt.close(fig)</span><br></pre></td></tr></table></figure>\n"},{"title":"","date":"2017-12-23T11:51:16.000Z","description":"","_content":"\n# \n\n SGDSGDMNAGAdaGradAdaDeltaAdamNadam Momentum\n\n\n\n\n\n- $w$\n- $f(w)$\n- $\\alpha$\n\n epoch $t$ \n\n> 1. $g_{t} = \\nabla f(w_{t})$\n> 2. $m_{t} = \\phi(g_{1}, g_{2}, \\cdots , g_{t}); \\, V_{t} = \\psi(g_{1}, g_{2}, \\cdots , g_{t})$\n> 3. $\\eta_{t} = \\alpha \\cdot m_{t} \\Big/ \\sqrt{V_{t}}$\n> 4. $w_{t+1} = w_{t} - \\eta_{t}$\n\n## SGD\n\nSGD $m_{t} = g_{t}; \\, V_{t} = I^{2}$\n\n$\\eta_{t} = \\alpha \\cdot g_{t}$\n\nSGD \n\n## SGD with Momentum\n\n SGD SGDM $m_{t} = \\beta_{1} \\cdot m_{t-1} + (1 - \\beta_{1}) \\cdot g_{t}$\n\n $1 / (1 - \\beta_{1})$ \n\n$\\beta_{1}$  0.9 \n\n## SGD with Nesterov Acceleration\n\nSGD \n\nNesterov  1 $g_{t} = \\nabla f(w_{t} - \\alpha \\cdot m_{t-1})$ \n\n SGDM  $m_{t}$ \n\n## AdaGrad\n\n\n\n\n\n$V_{t} = \\sum_{\\tau = 1}^{t} g_{\\tau}^{2}$ \n\n 3 $\\eta_{t} = \\alpha \\cdot m_{t} \\Big/ \\sqrt{V_{t}}$ Ada = Adaptive 0 \n\n 0 \n\n## AdaDelta / RMSProp\n\n AdaGrad  Delta \n\n$V_{t} = \\beta_{2} \\cdot V_{t-1} + (1 - \\beta_{2}) \\cdot g_{t}^{2}$ \n\n## Adam / Nadam\n\n Adam  Nadam \n\nAdam = Adaptive + Momentum\n\nNadam = Nesterov + Adam\n\n $\\beta_{1}$  $\\beta_{2}$ \n\n#  SGD\n\n Adam  Nadam  SGD \n\n\n\nSGD \n\n\n\n# \n\nNo free lunch theorem","source":"_posts/GD-Series.md","raw":"---\ntitle: \ndate: 2017-12-23 19:51:16\ncategories: ML\ntags:\n     - GD\n     - Momentum\ndescription: \n---\n\n# \n\n SGDSGDMNAGAdaGradAdaDeltaAdamNadam Momentum\n\n\n\n\n\n- $w$\n- $f(w)$\n- $\\alpha$\n\n epoch $t$ \n\n> 1. $g_{t} = \\nabla f(w_{t})$\n> 2. $m_{t} = \\phi(g_{1}, g_{2}, \\cdots , g_{t}); \\, V_{t} = \\psi(g_{1}, g_{2}, \\cdots , g_{t})$\n> 3. $\\eta_{t} = \\alpha \\cdot m_{t} \\Big/ \\sqrt{V_{t}}$\n> 4. $w_{t+1} = w_{t} - \\eta_{t}$\n\n## SGD\n\nSGD $m_{t} = g_{t}; \\, V_{t} = I^{2}$\n\n$\\eta_{t} = \\alpha \\cdot g_{t}$\n\nSGD \n\n## SGD with Momentum\n\n SGD SGDM $m_{t} = \\beta_{1} \\cdot m_{t-1} + (1 - \\beta_{1}) \\cdot g_{t}$\n\n $1 / (1 - \\beta_{1})$ \n\n$\\beta_{1}$  0.9 \n\n## SGD with Nesterov Acceleration\n\nSGD \n\nNesterov  1 $g_{t} = \\nabla f(w_{t} - \\alpha \\cdot m_{t-1})$ \n\n SGDM  $m_{t}$ \n\n## AdaGrad\n\n\n\n\n\n$V_{t} = \\sum_{\\tau = 1}^{t} g_{\\tau}^{2}$ \n\n 3 $\\eta_{t} = \\alpha \\cdot m_{t} \\Big/ \\sqrt{V_{t}}$ Ada = Adaptive 0 \n\n 0 \n\n## AdaDelta / RMSProp\n\n AdaGrad  Delta \n\n$V_{t} = \\beta_{2} \\cdot V_{t-1} + (1 - \\beta_{2}) \\cdot g_{t}^{2}$ \n\n## Adam / Nadam\n\n Adam  Nadam \n\nAdam = Adaptive + Momentum\n\nNadam = Nesterov + Adam\n\n $\\beta_{1}$  $\\beta_{2}$ \n\n#  SGD\n\n Adam  Nadam  SGD \n\n\n\nSGD \n\n\n\n# \n\nNo free lunch theorem","slug":"GD-Series","published":1,"updated":"2021-07-19T13:03:31.695Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckwrktiv80005qxt6q11ojijw","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> SGDSGDMNAGAdaGradAdaDeltaAdamNadam Momentum</p>\n<p></p>\n<p></p>\n<ul>\n<li>$w$</li>\n<li>$f(w)$</li>\n<li>$\\alpha$</li>\n</ul>\n<p> epoch $t$ </p>\n<blockquote>\n<ol>\n<li>$g_{t} = \\nabla f(w_{t})$</li>\n<li>$m_{t} = \\phi(g_{1}, g_{2}, \\cdots , g_{t}); \\, V_{t} = \\psi(g_{1}, g_{2}, \\cdots , g_{t})$</li>\n<li>$\\eta_{t} = \\alpha \\cdot m_{t} \\Big/ \\sqrt{V_{t}}$</li>\n<li>$w_{t+1} = w_{t} - \\eta_{t}$</li>\n</ol>\n</blockquote>\n<h2 id=\"SGD\"><a href=\"#SGD\" class=\"headerlink\" title=\"SGD\"></a>SGD</h2><p>SGD $m_{t} = g_{t}; \\, V_{t} = I^{2}$</p>\n<p>$\\eta_{t} = \\alpha \\cdot g_{t}$</p>\n<p>SGD </p>\n<h2 id=\"SGD-with-Momentum\"><a href=\"#SGD-with-Momentum\" class=\"headerlink\" title=\"SGD with Momentum\"></a>SGD with Momentum</h2><p> SGD SGDM $m_{t} = \\beta_{1} \\cdot m_{t-1} + (1 - \\beta_{1}) \\cdot g_{t}$</p>\n<p> $1 / (1 - \\beta_{1})$ </p>\n<p>$\\beta_{1}$  0.9 </p>\n<h2 id=\"SGD-with-Nesterov-Acceleration\"><a href=\"#SGD-with-Nesterov-Acceleration\" class=\"headerlink\" title=\"SGD with Nesterov Acceleration\"></a>SGD with Nesterov Acceleration</h2><p>SGD </p>\n<p>Nesterov  1 $g_{t} = \\nabla f(w_{t} - \\alpha \\cdot m_{t-1})$ </p>\n<p> SGDM  $m_{t}$ </p>\n<h2 id=\"AdaGrad\"><a href=\"#AdaGrad\" class=\"headerlink\" title=\"AdaGrad\"></a>AdaGrad</h2><p></p>\n<p></p>\n<p>$V_{t} = \\sum_{\\tau = 1}^{t} g_{\\tau}^{2}$ </p>\n<p> 3 $\\eta_{t} = \\alpha \\cdot m_{t} \\Big/ \\sqrt{V_{t}}$ Ada = Adaptive 0 </p>\n<p> 0 </p>\n<h2 id=\"AdaDelta-RMSProp\"><a href=\"#AdaDelta-RMSProp\" class=\"headerlink\" title=\"AdaDelta / RMSProp\"></a>AdaDelta / RMSProp</h2><p> AdaGrad  Delta </p>\n<p>$V_{t} = \\beta_{2} \\cdot V_{t-1} + (1 - \\beta_{2}) \\cdot g_{t}^{2}$ </p>\n<h2 id=\"Adam-Nadam\"><a href=\"#Adam-Nadam\" class=\"headerlink\" title=\"Adam / Nadam\"></a>Adam / Nadam</h2><p> Adam  Nadam </p>\n<p>Adam = Adaptive + Momentum</p>\n<p>Nadam = Nesterov + Adam</p>\n<p> $\\beta_{1}$  $\\beta_{2}$ </p>\n<h1 id=\"-SGD\"><a href=\"#-SGD\" class=\"headerlink\" title=\" SGD\"></a> SGD</h1><p> Adam  Nadam  SGD </p>\n<p></p>\n<p>SGD </p>\n<p></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>No free lunch theorem</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> SGDSGDMNAGAdaGradAdaDeltaAdamNadam Momentum</p>\n<p></p>\n<p></p>\n<ul>\n<li>$w$</li>\n<li>$f(w)$</li>\n<li>$\\alpha$</li>\n</ul>\n<p> epoch $t$ </p>\n<blockquote>\n<ol>\n<li>$g_{t} = \\nabla f(w_{t})$</li>\n<li>$m_{t} = \\phi(g_{1}, g_{2}, \\cdots , g_{t}); \\, V_{t} = \\psi(g_{1}, g_{2}, \\cdots , g_{t})$</li>\n<li>$\\eta_{t} = \\alpha \\cdot m_{t} \\Big/ \\sqrt{V_{t}}$</li>\n<li>$w_{t+1} = w_{t} - \\eta_{t}$</li>\n</ol>\n</blockquote>\n<h2 id=\"SGD\"><a href=\"#SGD\" class=\"headerlink\" title=\"SGD\"></a>SGD</h2><p>SGD $m_{t} = g_{t}; \\, V_{t} = I^{2}$</p>\n<p>$\\eta_{t} = \\alpha \\cdot g_{t}$</p>\n<p>SGD </p>\n<h2 id=\"SGD-with-Momentum\"><a href=\"#SGD-with-Momentum\" class=\"headerlink\" title=\"SGD with Momentum\"></a>SGD with Momentum</h2><p> SGD SGDM $m_{t} = \\beta_{1} \\cdot m_{t-1} + (1 - \\beta_{1}) \\cdot g_{t}$</p>\n<p> $1 / (1 - \\beta_{1})$ </p>\n<p>$\\beta_{1}$  0.9 </p>\n<h2 id=\"SGD-with-Nesterov-Acceleration\"><a href=\"#SGD-with-Nesterov-Acceleration\" class=\"headerlink\" title=\"SGD with Nesterov Acceleration\"></a>SGD with Nesterov Acceleration</h2><p>SGD </p>\n<p>Nesterov  1 $g_{t} = \\nabla f(w_{t} - \\alpha \\cdot m_{t-1})$ </p>\n<p> SGDM  $m_{t}$ </p>\n<h2 id=\"AdaGrad\"><a href=\"#AdaGrad\" class=\"headerlink\" title=\"AdaGrad\"></a>AdaGrad</h2><p></p>\n<p></p>\n<p>$V_{t} = \\sum_{\\tau = 1}^{t} g_{\\tau}^{2}$ </p>\n<p> 3 $\\eta_{t} = \\alpha \\cdot m_{t} \\Big/ \\sqrt{V_{t}}$ Ada = Adaptive 0 </p>\n<p> 0 </p>\n<h2 id=\"AdaDelta-RMSProp\"><a href=\"#AdaDelta-RMSProp\" class=\"headerlink\" title=\"AdaDelta / RMSProp\"></a>AdaDelta / RMSProp</h2><p> AdaGrad  Delta </p>\n<p>$V_{t} = \\beta_{2} \\cdot V_{t-1} + (1 - \\beta_{2}) \\cdot g_{t}^{2}$ </p>\n<h2 id=\"Adam-Nadam\"><a href=\"#Adam-Nadam\" class=\"headerlink\" title=\"Adam / Nadam\"></a>Adam / Nadam</h2><p> Adam  Nadam </p>\n<p>Adam = Adaptive + Momentum</p>\n<p>Nadam = Nesterov + Adam</p>\n<p> $\\beta_{1}$  $\\beta_{2}$ </p>\n<h1 id=\"-SGD\"><a href=\"#-SGD\" class=\"headerlink\" title=\" SGD\"></a> SGD</h1><p> Adam  Nadam  SGD </p>\n<p></p>\n<p>SGD </p>\n<p></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>No free lunch theorem</p>\n"},{"title":"","date":"2017-08-13T08:38:35.000Z","description":"","_content":"\n~~~~\n\n\nkernel function\n\nkernel\n****\n\n\n\n\n1. kernel  SVM  kernel  SVM  SVM  kernel \n\n2. **kernel  SVM ** SVM reproducing kernel Hilbert spaceRKHSsignal detectiontime seriesrandom walkpatternRKHS likelihood ratio kernel correlation\n\n    logistic regressionleast squaredimension reduction SVM \n\n3. ** kernel **Gaussian kernelradial basis functionRBF  RKHS \n\n4.  $\\Phi : X \\to \\mathcal{H}, \\Phi(x) = K(x, \\cdot)$  $\\mathcal{H}$  RKHS  $x$  RKHS  $<\\Phi(x), \\Phi(y)>_{\\mathcal{H}} = K(x, y)$ ~~~~\n\n5. \n   \n\n   ","source":"_posts/about-kernel-01.md","raw":"---\ntitle: \ndate: 2017-08-13 16:38:35\ncategories: ML\ntags:\n     - Kernel\n     - SVM\n     - RKHS\ndescription: \n---\n\n~~~~\n\n\nkernel function\n\nkernel\n****\n\n\n\n\n1. kernel  SVM  kernel  SVM  SVM  kernel \n\n2. **kernel  SVM ** SVM reproducing kernel Hilbert spaceRKHSsignal detectiontime seriesrandom walkpatternRKHS likelihood ratio kernel correlation\n\n    logistic regressionleast squaredimension reduction SVM \n\n3. ** kernel **Gaussian kernelradial basis functionRBF  RKHS \n\n4.  $\\Phi : X \\to \\mathcal{H}, \\Phi(x) = K(x, \\cdot)$  $\\mathcal{H}$  RKHS  $x$  RKHS  $<\\Phi(x), \\Phi(y)>_{\\mathcal{H}} = K(x, y)$ ~~~~\n\n5. \n   \n\n   ","slug":"about-kernel-01","published":1,"updated":"2021-07-19T13:03:31.696Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckwrktiv80006qxt6bf7p7wb0","content":"<p><del></del></p>\n<p><br>kernel function</p>\n<p>kernel<br><strong></strong><br></p>\n<p></p>\n<ol>\n<li><p>kernel  SVM  kernel  SVM  SVM  kernel </p>\n</li>\n<li><p><strong>kernel  SVM </strong> SVM reproducing kernel Hilbert spaceRKHSsignal detectiontime seriesrandom walkpatternRKHS likelihood ratio kernel correlation</p>\n<p> logistic regressionleast squaredimension reduction SVM </p>\n</li>\n<li><p><strong> kernel </strong>Gaussian kernelradial basis functionRBF  RKHS </p>\n</li>\n<li><p> $\\Phi : X \\to \\mathcal{H}, \\Phi(x) = K(x, \\cdot)$  $\\mathcal{H}$  RKHS  $x$  RKHS  $&lt;\\Phi(x), \\Phi(y)&gt;_{\\mathcal{H}} = K(x, y)$ <del></del></p>\n</li>\n<li><p><br></p>\n<p></p>\n</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<p><del></del></p>\n<p><br>kernel function</p>\n<p>kernel<br><strong></strong><br></p>\n<p></p>\n<ol>\n<li><p>kernel  SVM  kernel  SVM  SVM  kernel </p>\n</li>\n<li><p><strong>kernel  SVM </strong> SVM reproducing kernel Hilbert spaceRKHSsignal detectiontime seriesrandom walkpatternRKHS likelihood ratio kernel correlation</p>\n<p> logistic regressionleast squaredimension reduction SVM </p>\n</li>\n<li><p><strong> kernel </strong>Gaussian kernelradial basis functionRBF  RKHS </p>\n</li>\n<li><p> $\\Phi : X \\to \\mathcal{H}, \\Phi(x) = K(x, \\cdot)$  $\\mathcal{H}$  RKHS  $x$  RKHS  $&lt;\\Phi(x), \\Phi(y)&gt;_{\\mathcal{H}} = K(x, y)$ <del></del></p>\n</li>\n<li><p><br></p>\n<p></p>\n</li>\n</ol>\n"},{"title":"","date":"2020-01-12T02:35:19.000Z","description":"32","_content":"\n# \n\n<br /><br />\n\n3210\n\n#  \n\n****\n\n<br /><br />\n\n<br />\n\n<br /><br />\n\n\n\n- \n- \n\n\n\n****\n\n<br />\n\n****\n\n\n\n#  \n\n****\n\n<br /><br />\n\n****\n\n\n\n- \n- \n- \n\n<br />\n\n<br />\n\n#  \n\n****\n\n<br />\n\n<br />\n\n<br /><br />\n\n<br /><br /><br />\n\n<br /><br />\n\n# \n\n<br /><br />\n\n<br /><br />\n\n****\n\n","source":"_posts/alterable-vs-permanent.md","raw":"---\ntitle: \ndate: 2020-01-12 10:35:19\ncategories: Life\ntags: Tips\ndescription: 32\n---\n\n# \n\n<br /><br />\n\n3210\n\n#  \n\n****\n\n<br /><br />\n\n<br />\n\n<br /><br />\n\n\n\n- \n- \n\n\n\n****\n\n<br />\n\n****\n\n\n\n#  \n\n****\n\n<br /><br />\n\n****\n\n\n\n- \n- \n- \n\n<br />\n\n<br />\n\n#  \n\n****\n\n<br />\n\n<br />\n\n<br /><br />\n\n<br /><br /><br />\n\n<br /><br />\n\n# \n\n<br /><br />\n\n<br /><br />\n\n****\n\n","slug":"alterable-vs-permanent","published":1,"updated":"2021-07-19T13:03:31.696Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckwrktiv90007qxt6yf4w573m","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p><br><br></p>\n<p>3210</p>\n<h1 id=\"-\"><a href=\"#-\" class=\"headerlink\" title=\" \"></a> </h1><p><strong></strong></p>\n<p><br><br></p>\n<p><br></p>\n<p><br><br></p>\n<p></p>\n<ul>\n<li></li>\n<li></li>\n</ul>\n<p></p>\n<p><strong></strong></p>\n<p><br></p>\n<p><strong></strong></p>\n<p></p>\n<h1 id=\"-\"><a href=\"#-\" class=\"headerlink\" title=\" \"></a> </h1><p><strong></strong></p>\n<p><br><br></p>\n<p><strong></strong></p>\n<p></p>\n<ul>\n<li></li>\n<li></li>\n<li></li>\n</ul>\n<p><br></p>\n<p><br></p>\n<h1 id=\"-\"><a href=\"#-\" class=\"headerlink\" title=\" \"></a> </h1><p><strong></strong></p>\n<p><br></p>\n<p><br></p>\n<p><br><br></p>\n<p><br><br><br></p>\n<p><br><br></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p><br><br></p>\n<p><br><br></p>\n<p><strong></strong></p>\n<p></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p><br><br></p>\n<p>3210</p>\n<h1 id=\"-\"><a href=\"#-\" class=\"headerlink\" title=\" \"></a> </h1><p><strong></strong></p>\n<p><br><br></p>\n<p><br></p>\n<p><br><br></p>\n<p></p>\n<ul>\n<li></li>\n<li></li>\n</ul>\n<p></p>\n<p><strong></strong></p>\n<p><br></p>\n<p><strong></strong></p>\n<p></p>\n<h1 id=\"-\"><a href=\"#-\" class=\"headerlink\" title=\" \"></a> </h1><p><strong></strong></p>\n<p><br><br></p>\n<p><strong></strong></p>\n<p></p>\n<ul>\n<li></li>\n<li></li>\n<li></li>\n</ul>\n<p><br></p>\n<p><br></p>\n<h1 id=\"-\"><a href=\"#-\" class=\"headerlink\" title=\" \"></a> </h1><p><strong></strong></p>\n<p><br></p>\n<p><br></p>\n<p><br><br></p>\n<p><br><br><br></p>\n<p><br><br></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p><br><br></p>\n<p><br><br></p>\n<p><strong></strong></p>\n<p></p>\n"},{"title":"","date":"2017-12-03T04:39:14.000Z","description":"","_content":"\n# \n\n\n\n BP  $\\delta$  BP \n\n\n\n- $net$  $net_{i}$  $net = w^{\\mathsf{T}}x$ \n- $a$  $a_{i}$  $a=f(net)$ \n\n# \n\n identical $a=net=w^{\\mathsf{T}}x$ \n\n$$\nJ(w) = \\frac{1}{2} (a-y)^{2}=\\frac{1}{2} (net-y)^{2}\n$$\n\n$$\n\\delta = \\frac{\\partial J}{\\partial net} = net-y = a-y\n$$\n\n# \n\n\n\n## \n\n sigmoid  $a=\\sigma(net)=\\sigma(w^{\\mathsf{T}}x)=\\frac{1}{1+\\exp(-w^{\\mathsf{T}}x)}$ \n\n$$\nJ(w) = -L(w) = -y \\ln a - (1-y) \\ln (1-a)\n$$\n\n$$\nL(w) = y \\ln a + (1-y) \\ln (1-a)\n$$\n log-likelihood\n\n\n$$\n\\delta = \\frac{\\partial J}{\\partial net} = \\frac{\\partial J}{\\partial a} \\frac{\\partial a}{\\partial net} = \\frac{a-y}{(1-a)a}(1-a)a = a-y\n$$\n\n## \n\n softmax $a=\\mathrm{softmax}(net)=\\frac{\\exp(net_{i})}{\\sum_{j=1}^{N}\\exp(net_{j})}$ \n\n\n$$\nJ(w) = -\\sum_{j=1}^{N} y_j \\ln a_j = \\sum_{j=1}^{N} y_j (\\ln \\sum_{k=1}^{N} e^{net_{k}}-net_{j})\n$$\n $i$ \n$$\n\\delta_{i} = \\frac{\\partial J}{\\partial net_{i}} = \\sum_{j=1}^{N} y_j (\\frac{\\sum_{k=1}^{N} e^{net_{k}}\\frac{\\partial net_{k}}{\\partial net_{i}}}{\\sum_{k=1}^{N} e^{net_{k}}} - \\frac{\\partial net_{j}}{\\partial net_{i}}) = \\sum_{j=1}^{N} y_j  \\frac{e^{net_{i}}}{\\sum_{k=1}^{N} e^{net_{k}}} - \\sum_{j=1}^{N} y_j  \\frac{\\partial net_{j}}{\\partial net_{i}} = a_{i}-y_{i}\n$$\n","source":"_posts/common-ground-of-mse-and-cee-in-nn.md","raw":"---\ntitle: \ndate: 2017-12-03 12:39:14\ncategories: ML\ntags: \n     - Deep learning\n     - Loss function\ndescription: \n---\n\n# \n\n\n\n BP  $\\delta$  BP \n\n\n\n- $net$  $net_{i}$  $net = w^{\\mathsf{T}}x$ \n- $a$  $a_{i}$  $a=f(net)$ \n\n# \n\n identical $a=net=w^{\\mathsf{T}}x$ \n\n$$\nJ(w) = \\frac{1}{2} (a-y)^{2}=\\frac{1}{2} (net-y)^{2}\n$$\n\n$$\n\\delta = \\frac{\\partial J}{\\partial net} = net-y = a-y\n$$\n\n# \n\n\n\n## \n\n sigmoid  $a=\\sigma(net)=\\sigma(w^{\\mathsf{T}}x)=\\frac{1}{1+\\exp(-w^{\\mathsf{T}}x)}$ \n\n$$\nJ(w) = -L(w) = -y \\ln a - (1-y) \\ln (1-a)\n$$\n\n$$\nL(w) = y \\ln a + (1-y) \\ln (1-a)\n$$\n log-likelihood\n\n\n$$\n\\delta = \\frac{\\partial J}{\\partial net} = \\frac{\\partial J}{\\partial a} \\frac{\\partial a}{\\partial net} = \\frac{a-y}{(1-a)a}(1-a)a = a-y\n$$\n\n## \n\n softmax $a=\\mathrm{softmax}(net)=\\frac{\\exp(net_{i})}{\\sum_{j=1}^{N}\\exp(net_{j})}$ \n\n\n$$\nJ(w) = -\\sum_{j=1}^{N} y_j \\ln a_j = \\sum_{j=1}^{N} y_j (\\ln \\sum_{k=1}^{N} e^{net_{k}}-net_{j})\n$$\n $i$ \n$$\n\\delta_{i} = \\frac{\\partial J}{\\partial net_{i}} = \\sum_{j=1}^{N} y_j (\\frac{\\sum_{k=1}^{N} e^{net_{k}}\\frac{\\partial net_{k}}{\\partial net_{i}}}{\\sum_{k=1}^{N} e^{net_{k}}} - \\frac{\\partial net_{j}}{\\partial net_{i}}) = \\sum_{j=1}^{N} y_j  \\frac{e^{net_{i}}}{\\sum_{k=1}^{N} e^{net_{k}}} - \\sum_{j=1}^{N} y_j  \\frac{\\partial net_{j}}{\\partial net_{i}} = a_{i}-y_{i}\n$$\n","slug":"common-ground-of-mse-and-cee-in-nn","published":1,"updated":"2021-07-19T13:03:31.696Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckwrktiva0008qxt6d5wsigmj","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n<p> BP  $\\delta$  BP </p>\n<p></p>\n<ul>\n<li>$net$  $net_{i}$  $net = w^{\\mathsf{T}}x$ </li>\n<li>$a$  $a_{i}$  $a=f(net)$ </li>\n</ul>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> identical $a=net=w^{\\mathsf{T}}x$ <br></p>\n<script type=\"math/tex; mode=display\">\nJ(w) = \\frac{1}{2} (a-y)^{2}=\\frac{1}{2} (net-y)^{2}</script><p></p>\n<script type=\"math/tex; mode=display\">\n\\delta = \\frac{\\partial J}{\\partial net} = net-y = a-y</script><h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> sigmoid  $a=\\sigma(net)=\\sigma(w^{\\mathsf{T}}x)=\\frac{1}{1+\\exp(-w^{\\mathsf{T}}x)}$ <br></p>\n<script type=\"math/tex; mode=display\">\nJ(w) = -L(w) = -y \\ln a - (1-y) \\ln (1-a)</script><p></p>\n<script type=\"math/tex; mode=display\">\nL(w) = y \\ln a + (1-y) \\ln (1-a)</script><p> log-likelihood</p>\n<p></p>\n<script type=\"math/tex; mode=display\">\n\\delta = \\frac{\\partial J}{\\partial net} = \\frac{\\partial J}{\\partial a} \\frac{\\partial a}{\\partial net} = \\frac{a-y}{(1-a)a}(1-a)a = a-y</script><h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> softmax $a=\\mathrm{softmax}(net)=\\frac{\\exp(net_{i})}{\\sum_{j=1}^{N}\\exp(net_{j})}$ </p>\n<p></p>\n<script type=\"math/tex; mode=display\">\nJ(w) = -\\sum_{j=1}^{N} y_j \\ln a_j = \\sum_{j=1}^{N} y_j (\\ln \\sum_{k=1}^{N} e^{net_{k}}-net_{j})</script><p> $i$ </p>\n<script type=\"math/tex; mode=display\">\n\\delta_{i} = \\frac{\\partial J}{\\partial net_{i}} = \\sum_{j=1}^{N} y_j (\\frac{\\sum_{k=1}^{N} e^{net_{k}}\\frac{\\partial net_{k}}{\\partial net_{i}}}{\\sum_{k=1}^{N} e^{net_{k}}} - \\frac{\\partial net_{j}}{\\partial net_{i}}) = \\sum_{j=1}^{N} y_j  \\frac{e^{net_{i}}}{\\sum_{k=1}^{N} e^{net_{k}}} - \\sum_{j=1}^{N} y_j  \\frac{\\partial net_{j}}{\\partial net_{i}} = a_{i}-y_{i}</script>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n<p> BP  $\\delta$  BP </p>\n<p></p>\n<ul>\n<li>$net$  $net_{i}$  $net = w^{\\mathsf{T}}x$ </li>\n<li>$a$  $a_{i}$  $a=f(net)$ </li>\n</ul>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> identical $a=net=w^{\\mathsf{T}}x$ <br></p>\n<script type=\"math/tex; mode=display\">\nJ(w) = \\frac{1}{2} (a-y)^{2}=\\frac{1}{2} (net-y)^{2}</script><p></p>\n<script type=\"math/tex; mode=display\">\n\\delta = \\frac{\\partial J}{\\partial net} = net-y = a-y</script><h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> sigmoid  $a=\\sigma(net)=\\sigma(w^{\\mathsf{T}}x)=\\frac{1}{1+\\exp(-w^{\\mathsf{T}}x)}$ <br></p>\n<script type=\"math/tex; mode=display\">\nJ(w) = -L(w) = -y \\ln a - (1-y) \\ln (1-a)</script><p></p>\n<script type=\"math/tex; mode=display\">\nL(w) = y \\ln a + (1-y) \\ln (1-a)</script><p> log-likelihood</p>\n<p></p>\n<script type=\"math/tex; mode=display\">\n\\delta = \\frac{\\partial J}{\\partial net} = \\frac{\\partial J}{\\partial a} \\frac{\\partial a}{\\partial net} = \\frac{a-y}{(1-a)a}(1-a)a = a-y</script><h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> softmax $a=\\mathrm{softmax}(net)=\\frac{\\exp(net_{i})}{\\sum_{j=1}^{N}\\exp(net_{j})}$ </p>\n<p></p>\n<script type=\"math/tex; mode=display\">\nJ(w) = -\\sum_{j=1}^{N} y_j \\ln a_j = \\sum_{j=1}^{N} y_j (\\ln \\sum_{k=1}^{N} e^{net_{k}}-net_{j})</script><p> $i$ </p>\n<script type=\"math/tex; mode=display\">\n\\delta_{i} = \\frac{\\partial J}{\\partial net_{i}} = \\sum_{j=1}^{N} y_j (\\frac{\\sum_{k=1}^{N} e^{net_{k}}\\frac{\\partial net_{k}}{\\partial net_{i}}}{\\sum_{k=1}^{N} e^{net_{k}}} - \\frac{\\partial net_{j}}{\\partial net_{i}}) = \\sum_{j=1}^{N} y_j  \\frac{e^{net_{i}}}{\\sum_{k=1}^{N} e^{net_{k}}} - \\sum_{j=1}^{N} y_j  \\frac{\\partial net_{j}}{\\partial net_{i}} = a_{i}-y_{i}</script>"},{"title":" PCA  LFM ","date":"2017-08-08T12:25:59.000Z","_content":"\n\nPCAPrincipal Component AnalysisLFMLatent Factor Model\nPCA  LFM \n\n PCA  LFM  user-item \n\n\n\n<!-- more -->\n\n1. PCA\n\n   PCA $p$  $q$ \n    $\\lbrace X_{1}, X_{2}, \\cdots, X_{p} \\rbrace$  $\\lbrace Y_{1}, Y_{2}, \\cdots, Y_{q} \\rbrace$ \n   $$\n   Y_{i} = \\sum_{j=1}^{p} \\alpha_{j}^{(i)} X_{j}\n   $$\n   PCAcorrelation  correlation \n\n    $Y_{i}$  $X_{j}$  $\\alpha_{j}^{(i)}$  $\\alpha_{j}^{(i)}$  $0$  $0$  $Y_{i}$  $X_{j}$  $\\alpha_{j}^{(i)} $  $0$  $p$  $q$ \n\n   PCA  $q$ eigenvector $q$  $q$ eigenvalue $q$ \n\n   PCA \n\n2. LFM\n\n    $\\lbrace X_{1}, X_{2}, \\cdots, X_{p} \\rbrace$  $Z_{k}$\n   $$\n   \\forall X_{i}, \\text{  } \\beta_{k}^{(i)}, \\text{  } X_{i} = \\sum_{k=1}^{r} \\beta_{k}^{(i)} Z_{k}\n   $$\n    $\\lbrace Z_{1}, Z_{2}, \\cdots, Z_{r} \\rbrace$ \n    $\\beta^{\\mathsf{T}}$  user  $Z$  item  user-item LFM\n\n    $X$  $\\beta$  $X$ \n   \n\n   \n\n****\nPCA\nLFM\n","source":"_posts/compare-between-PCA-and-LFM.md","raw":"---\ntitle:  PCA  LFM \ndate: 2017-08-08 20:25:59\ncategories: ML\ntags: \n      - PCA\n      - LFM\n---\n\n\nPCAPrincipal Component AnalysisLFMLatent Factor Model\nPCA  LFM \n\n PCA  LFM  user-item \n\n\n\n<!-- more -->\n\n1. PCA\n\n   PCA $p$  $q$ \n    $\\lbrace X_{1}, X_{2}, \\cdots, X_{p} \\rbrace$  $\\lbrace Y_{1}, Y_{2}, \\cdots, Y_{q} \\rbrace$ \n   $$\n   Y_{i} = \\sum_{j=1}^{p} \\alpha_{j}^{(i)} X_{j}\n   $$\n   PCAcorrelation  correlation \n\n    $Y_{i}$  $X_{j}$  $\\alpha_{j}^{(i)}$  $\\alpha_{j}^{(i)}$  $0$  $0$  $Y_{i}$  $X_{j}$  $\\alpha_{j}^{(i)} $  $0$  $p$  $q$ \n\n   PCA  $q$ eigenvector $q$  $q$ eigenvalue $q$ \n\n   PCA \n\n2. LFM\n\n    $\\lbrace X_{1}, X_{2}, \\cdots, X_{p} \\rbrace$  $Z_{k}$\n   $$\n   \\forall X_{i}, \\text{  } \\beta_{k}^{(i)}, \\text{  } X_{i} = \\sum_{k=1}^{r} \\beta_{k}^{(i)} Z_{k}\n   $$\n    $\\lbrace Z_{1}, Z_{2}, \\cdots, Z_{r} \\rbrace$ \n    $\\beta^{\\mathsf{T}}$  user  $Z$  item  user-item LFM\n\n    $X$  $\\beta$  $X$ \n   \n\n   \n\n****\nPCA\nLFM\n","slug":"compare-between-PCA-and-LFM","published":1,"updated":"2021-07-19T13:03:31.696Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckwrktiva0009qxt6bpod90eh","content":"<p><br>PCAPrincipal Component AnalysisLFMLatent Factor Model<br>PCA  LFM <br><br> PCA  LFM  user-item </p>\n<p></p>\n<a id=\"more\"></a>\n<ol>\n<li><p>PCA</p>\n<p>PCA $p$  $q$ <br> $\\lbrace X_{1}, X_{2}, \\cdots, X_{p} \\rbrace$  $\\lbrace Y_{1}, Y_{2}, \\cdots, Y_{q} \\rbrace$ </p>\n<script type=\"math/tex; mode=display\">\nY_{i} = \\sum_{j=1}^{p} \\alpha_{j}^{(i)} X_{j}</script><p>PCAcorrelation  correlation </p>\n<p> $Y_{i}$  $X_{j}$  $\\alpha_{j}^{(i)}$  $\\alpha_{j}^{(i)}$  $0$  $0$  $Y_{i}$  $X_{j}$  $\\alpha_{j}^{(i)} $  $0$  $p$  $q$ </p>\n<p>PCA  $q$ eigenvector $q$  $q$ eigenvalue $q$ </p>\n<p>PCA </p>\n</li>\n<li><p>LFM</p>\n<p> $\\lbrace X_{1}, X_{2}, \\cdots, X_{p} \\rbrace$  $Z_{k}$</p>\n<script type=\"math/tex; mode=display\">\n\\forall X_{i}, \\text{  } \\beta_{k}^{(i)}, \\text{  } X_{i} = \\sum_{k=1}^{r} \\beta_{k}^{(i)} Z_{k}</script><p> $\\lbrace Z_{1}, Z_{2}, \\cdots, Z_{r} \\rbrace$ <br> $\\beta^{\\mathsf{T}}$  user  $Z$  item  user-item LFM</p>\n<p> $X$  $\\beta$  $X$ <br></p>\n<p></p>\n</li>\n</ol>\n<p><strong></strong><br>PCA<br>LFM</p>\n","site":{"data":{}},"excerpt":"<p><br>PCAPrincipal Component AnalysisLFMLatent Factor Model<br>PCA  LFM <br><br> PCA  LFM  user-item </p>\n<p></p>","more":"<ol>\n<li><p>PCA</p>\n<p>PCA $p$  $q$ <br> $\\lbrace X_{1}, X_{2}, \\cdots, X_{p} \\rbrace$  $\\lbrace Y_{1}, Y_{2}, \\cdots, Y_{q} \\rbrace$ </p>\n<script type=\"math/tex; mode=display\">\nY_{i} = \\sum_{j=1}^{p} \\alpha_{j}^{(i)} X_{j}</script><p>PCAcorrelation  correlation </p>\n<p> $Y_{i}$  $X_{j}$  $\\alpha_{j}^{(i)}$  $\\alpha_{j}^{(i)}$  $0$  $0$  $Y_{i}$  $X_{j}$  $\\alpha_{j}^{(i)} $  $0$  $p$  $q$ </p>\n<p>PCA  $q$ eigenvector $q$  $q$ eigenvalue $q$ </p>\n<p>PCA </p>\n</li>\n<li><p>LFM</p>\n<p> $\\lbrace X_{1}, X_{2}, \\cdots, X_{p} \\rbrace$  $Z_{k}$</p>\n<script type=\"math/tex; mode=display\">\n\\forall X_{i}, \\text{  } \\beta_{k}^{(i)}, \\text{  } X_{i} = \\sum_{k=1}^{r} \\beta_{k}^{(i)} Z_{k}</script><p> $\\lbrace Z_{1}, Z_{2}, \\cdots, Z_{r} \\rbrace$ <br> $\\beta^{\\mathsf{T}}$  user  $Z$  item  user-item LFM</p>\n<p> $X$  $\\beta$  $X$ <br></p>\n<p></p>\n</li>\n</ol>\n<p><strong></strong><br>PCA<br>LFM</p>"},{"title":"","date":"2018-02-04T14:17:38.000Z","description":"","_content":"\n# \n\n**Discriminative model** $+1$  $-1$ \n\nLinear RegressionLogistic RegressionSVMTraditional Neural Networks)Linear Discriminative AnalysisConditional Random Field\n\n**Generative model**\n\nNaive BayesHidden Markov ModelBayes NetworksLatent Dirichlet Allocation\n\n# \n\n\n\n-  $(C, X)$ $C=\\{c_1, c_2, \\cdots, c_n\\}$ n$X=\\{x_1, x_2, \\cdots, x_n\\}$ n\n-  $(\\tilde{c}, \\tilde{x})$ $\\tilde{c}$ $\\tilde{x}$ \n\n## \n\n $P(\\tilde{c} | \\tilde{x}) = P(\\tilde{c} | \\tilde{x}, C, X)$  $\\theta$  $P(\\tilde{c} | \\tilde{x},\\theta)$ \n$$\n\\begin{aligned}\n&P(\\tilde{c} | \\tilde{x}) \\\\\n=&P(\\tilde{c} | \\tilde{x}, C, X) \\\\\n=&\\int P(\\tilde{c}, \\theta | \\tilde{x}, C, X) \\, d \\theta \\\\\n=&\\int P(\\tilde{c} | \\tilde{x}, \\theta) \\cdot P(\\theta | C, X) \\, d \\theta \\\\\n=&\\int P(\\tilde{c} | \\tilde{x}, \\theta) \\cdot \\frac{P(C | X, \\theta) \\cdot P(\\theta)}{\\int P(C | X, \\theta) \\cdot P(\\theta) \\, d \\theta} \\, d \\theta\n\\end{aligned}\n$$\n\n\n variational inference  $\\theta$  $\\theta_{MAP}$  $\\theta$ point estimate $P(\\tilde{c} | \\tilde{x}) = P(\\tilde{c} | \\tilde{x}, C, X) = P(\\tilde{c} | \\tilde{x},\\theta_{MAP})$ \n\n $P(C|X) = \\int P(C | X, \\theta) \\cdot P(\\theta) \\, d \\theta$  $P(C | X, \\theta) \\cdot P(\\theta)$ \n\n\n\n/\n\n## \n\n $P(\\tilde{x}, \\tilde{c})$ \n\n$P(\\tilde{x}, \\tilde{c}) = P(\\tilde{x} | \\tilde{c}) \\cdot P(\\tilde{c})$ \n\n\n\n# \n\n## \n\n\n\n1. \n2. \n3. \n\n\n\n\n\n## \n\n\n\n1. \n2. \n3. Gaussian Mixture Model\n\n\n\n1. ","source":"_posts/discriminative-or-generative-model.md","raw":"---\ntitle: \ndate: 2018-02-04 22:17:38\ncategories: ML\ntags:\n     - Discriminative model\n     - Generative model\ndescription: \n---\n\n# \n\n**Discriminative model** $+1$  $-1$ \n\nLinear RegressionLogistic RegressionSVMTraditional Neural Networks)Linear Discriminative AnalysisConditional Random Field\n\n**Generative model**\n\nNaive BayesHidden Markov ModelBayes NetworksLatent Dirichlet Allocation\n\n# \n\n\n\n-  $(C, X)$ $C=\\{c_1, c_2, \\cdots, c_n\\}$ n$X=\\{x_1, x_2, \\cdots, x_n\\}$ n\n-  $(\\tilde{c}, \\tilde{x})$ $\\tilde{c}$ $\\tilde{x}$ \n\n## \n\n $P(\\tilde{c} | \\tilde{x}) = P(\\tilde{c} | \\tilde{x}, C, X)$  $\\theta$  $P(\\tilde{c} | \\tilde{x},\\theta)$ \n$$\n\\begin{aligned}\n&P(\\tilde{c} | \\tilde{x}) \\\\\n=&P(\\tilde{c} | \\tilde{x}, C, X) \\\\\n=&\\int P(\\tilde{c}, \\theta | \\tilde{x}, C, X) \\, d \\theta \\\\\n=&\\int P(\\tilde{c} | \\tilde{x}, \\theta) \\cdot P(\\theta | C, X) \\, d \\theta \\\\\n=&\\int P(\\tilde{c} | \\tilde{x}, \\theta) \\cdot \\frac{P(C | X, \\theta) \\cdot P(\\theta)}{\\int P(C | X, \\theta) \\cdot P(\\theta) \\, d \\theta} \\, d \\theta\n\\end{aligned}\n$$\n\n\n variational inference  $\\theta$  $\\theta_{MAP}$  $\\theta$ point estimate $P(\\tilde{c} | \\tilde{x}) = P(\\tilde{c} | \\tilde{x}, C, X) = P(\\tilde{c} | \\tilde{x},\\theta_{MAP})$ \n\n $P(C|X) = \\int P(C | X, \\theta) \\cdot P(\\theta) \\, d \\theta$  $P(C | X, \\theta) \\cdot P(\\theta)$ \n\n\n\n/\n\n## \n\n $P(\\tilde{x}, \\tilde{c})$ \n\n$P(\\tilde{x}, \\tilde{c}) = P(\\tilde{x} | \\tilde{c}) \\cdot P(\\tilde{c})$ \n\n\n\n# \n\n## \n\n\n\n1. \n2. \n3. \n\n\n\n\n\n## \n\n\n\n1. \n2. \n3. Gaussian Mixture Model\n\n\n\n1. ","slug":"discriminative-or-generative-model","published":1,"updated":"2021-07-19T13:03:31.696Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckwrktivb000aqxt6iflya024","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p><strong>Discriminative model</strong> $+1$  $-1$ </p>\n<p>Linear RegressionLogistic RegressionSVMTraditional Neural Networks)Linear Discriminative AnalysisConditional Random Field</p>\n<p><strong>Generative model</strong></p>\n<p>Naive BayesHidden Markov ModelBayes NetworksLatent Dirichlet Allocation</p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n<ul>\n<li> $(C, X)$ $C=\\{c_1, c_2, \\cdots, c_n\\}$ n$X=\\{x_1, x_2, \\cdots, x_n\\}$ n</li>\n<li> $(\\tilde{c}, \\tilde{x})$ $\\tilde{c}$ $\\tilde{x}$ </li>\n</ul>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> $P(\\tilde{c} | \\tilde{x}) = P(\\tilde{c} | \\tilde{x}, C, X)$  $\\theta$  $P(\\tilde{c} | \\tilde{x},\\theta)$ </p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n&P(\\tilde{c} | \\tilde{x}) \\\\\n=&P(\\tilde{c} | \\tilde{x}, C, X) \\\\\n=&\\int P(\\tilde{c}, \\theta | \\tilde{x}, C, X) \\, d \\theta \\\\\n=&\\int P(\\tilde{c} | \\tilde{x}, \\theta) \\cdot P(\\theta | C, X) \\, d \\theta \\\\\n=&\\int P(\\tilde{c} | \\tilde{x}, \\theta) \\cdot \\frac{P(C | X, \\theta) \\cdot P(\\theta)}{\\int P(C | X, \\theta) \\cdot P(\\theta) \\, d \\theta} \\, d \\theta\n\\end{aligned}</script><p></p>\n<p> variational inference  $\\theta$  $\\theta_{MAP}$  $\\theta$ point estimate $P(\\tilde{c} | \\tilde{x}) = P(\\tilde{c} | \\tilde{x}, C, X) = P(\\tilde{c} | \\tilde{x},\\theta_{MAP})$ </p>\n<p> $P(C|X) = \\int P(C | X, \\theta) \\cdot P(\\theta) \\, d \\theta$  $P(C | X, \\theta) \\cdot P(\\theta)$ </p>\n<p></p>\n<p>/</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> $P(\\tilde{x}, \\tilde{c})$ </p>\n<p>$P(\\tilde{x}, \\tilde{c}) = P(\\tilde{x} | \\tilde{c}) \\cdot P(\\tilde{c})$ </p>\n<p></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><h2 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"\"></a></h2><p></p>\n<ol>\n<li></li>\n<li></li>\n<li></li>\n</ol>\n<p></p>\n<p></p>\n<h2 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"\"></a></h2><p></p>\n<ol>\n<li></li>\n<li></li>\n<li>Gaussian Mixture Model</li>\n</ol>\n<p></p>\n<ol>\n<li></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p><strong>Discriminative model</strong> $+1$  $-1$ </p>\n<p>Linear RegressionLogistic RegressionSVMTraditional Neural Networks)Linear Discriminative AnalysisConditional Random Field</p>\n<p><strong>Generative model</strong></p>\n<p>Naive BayesHidden Markov ModelBayes NetworksLatent Dirichlet Allocation</p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n<ul>\n<li> $(C, X)$ $C=\\{c_1, c_2, \\cdots, c_n\\}$ n$X=\\{x_1, x_2, \\cdots, x_n\\}$ n</li>\n<li> $(\\tilde{c}, \\tilde{x})$ $\\tilde{c}$ $\\tilde{x}$ </li>\n</ul>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> $P(\\tilde{c} | \\tilde{x}) = P(\\tilde{c} | \\tilde{x}, C, X)$  $\\theta$  $P(\\tilde{c} | \\tilde{x},\\theta)$ </p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n&P(\\tilde{c} | \\tilde{x}) \\\\\n=&P(\\tilde{c} | \\tilde{x}, C, X) \\\\\n=&\\int P(\\tilde{c}, \\theta | \\tilde{x}, C, X) \\, d \\theta \\\\\n=&\\int P(\\tilde{c} | \\tilde{x}, \\theta) \\cdot P(\\theta | C, X) \\, d \\theta \\\\\n=&\\int P(\\tilde{c} | \\tilde{x}, \\theta) \\cdot \\frac{P(C | X, \\theta) \\cdot P(\\theta)}{\\int P(C | X, \\theta) \\cdot P(\\theta) \\, d \\theta} \\, d \\theta\n\\end{aligned}</script><p></p>\n<p> variational inference  $\\theta$  $\\theta_{MAP}$  $\\theta$ point estimate $P(\\tilde{c} | \\tilde{x}) = P(\\tilde{c} | \\tilde{x}, C, X) = P(\\tilde{c} | \\tilde{x},\\theta_{MAP})$ </p>\n<p> $P(C|X) = \\int P(C | X, \\theta) \\cdot P(\\theta) \\, d \\theta$  $P(C | X, \\theta) \\cdot P(\\theta)$ </p>\n<p></p>\n<p>/</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> $P(\\tilde{x}, \\tilde{c})$ </p>\n<p>$P(\\tilde{x}, \\tilde{c}) = P(\\tilde{x} | \\tilde{c}) \\cdot P(\\tilde{c})$ </p>\n<p></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><h2 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"\"></a></h2><p></p>\n<ol>\n<li></li>\n<li></li>\n<li></li>\n</ol>\n<p></p>\n<p></p>\n<h2 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"\"></a></h2><p></p>\n<ol>\n<li></li>\n<li></li>\n<li>Gaussian Mixture Model</li>\n</ol>\n<p></p>\n<ol>\n<li></li>\n</ol>\n"},{"title":" boostAdaboost  ","date":"2017-10-15T03:31:35.000Z","description":" boost  Adaboost ","_content":"\n# \n\n1. boost =  + \n2. Adaboost = boost + \n3.  = boost + \n\n\n\n1. boost \n2. Adaboost  boost  boost \n\n# \n\n\n\n\n\n# \n\n Adaboost \n\n BRT \n\n GBDT/GBRT \n\n# \n\nBDT +  Adaboost \n\nBRT + \n\nGBDT + \n\nGBRT + \n\n GBRT  BRT","source":"_posts/from-boost-to-Adaboost-to-GBT.md","raw":"---\ntitle:  boostAdaboost  \ndate: 2017-10-15 11:31:35\ncategories: ML\ntags:\n     - Ensemble\n     - Boost\n     - GBDT/GBRT\ndescription:  boost  Adaboost \n---\n\n# \n\n1. boost =  + \n2. Adaboost = boost + \n3.  = boost + \n\n\n\n1. boost \n2. Adaboost  boost  boost \n\n# \n\n\n\n\n\n# \n\n Adaboost \n\n BRT \n\n GBDT/GBRT \n\n# \n\nBDT +  Adaboost \n\nBRT + \n\nGBDT + \n\nGBRT + \n\n GBRT  BRT","slug":"from-boost-to-Adaboost-to-GBT","published":1,"updated":"2021-07-19T13:03:31.697Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckwrktivc000bqxt6rmriycof","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><ol>\n<li>boost =  + </li>\n<li>Adaboost = boost + </li>\n<li> = boost + </li>\n</ol>\n<p></p>\n<ol>\n<li>boost </li>\n<li>Adaboost  boost  boost </li>\n</ol>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n<p></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> Adaboost </p>\n<p> BRT </p>\n<p> GBDT/GBRT </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>BDT +  Adaboost </p>\n<p>BRT + </p>\n<p>GBDT + </p>\n<p>GBRT + </p>\n<p> GBRT  BRT</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><ol>\n<li>boost =  + </li>\n<li>Adaboost = boost + </li>\n<li> = boost + </li>\n</ol>\n<p></p>\n<ol>\n<li>boost </li>\n<li>Adaboost  boost  boost </li>\n</ol>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n<p></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> Adaboost </p>\n<p> BRT </p>\n<p> GBDT/GBRT </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>BDT +  Adaboost </p>\n<p>BRT + </p>\n<p>GBDT + </p>\n<p>GBRT + </p>\n<p> GBRT  BRT</p>\n"},{"title":"","date":"2017-08-07T14:28:18.000Z","_content":"\n\n\n<!-- more -->\n\n[](http://txshi-mt.com)\n\n\n\n\n\n\n\n\n\n\nAI\n","source":"_posts/init-blog.md","raw":"---\ntitle: \ndate: 2017-08-07 22:28:18\ncategories: Life\ntags:\n---\n\n\n\n<!-- more -->\n\n[](http://txshi-mt.com)\n\n\n\n\n\n\n\n\n\n\nAI\n","slug":"init-blog","published":1,"updated":"2021-07-19T13:03:31.697Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckwrktivd000cqxt6tjtqoylw","content":"<p></p>\n<a id=\"more\"></a>\n<p><a href=\"http://txshi-mt.com\" target=\"_blank\" rel=\"noopener\"></a><br><br><br></p>\n<p><br><br><br></p>\n<p><br>AI</p>\n","site":{"data":{}},"excerpt":"<p></p>","more":"<p><a href=\"http://txshi-mt.com\" target=\"_blank\" rel=\"noopener\"></a><br><br><br></p>\n<p><br><br><br></p>\n<p><br>AI</p>"},{"title":"K-means  GMM ","date":"2017-10-23T15:23:56.000Z","description":" GMM  K-means  EM ","_content":"\n#  GMM \n\n K-means \n GMMGaussian Mixture Model K-means  EM  K-means  EM \n\n\n\n**** $x_{1}, x_{2}, \\cdots, x_{N} \\in \\mathbb{R}^{m}$  $K$  $K$  $\\mu_{1}, \\mu_{2}, \\cdots, \\mu_{K}$  $\\sum\\limits_{i=1}^{N} \\min\\limits_{1 \\le k \\le K} \\lVert x_{i} - \\mu_{k} \\rVert_{2}^{2}$ \n\n**** $x_{1}, x_{2}, \\cdots, x_{N} \\in \\mathbb{R}^{m}$  $K$ \n$$\np(x, z \\mid \\mu_{1}, \\mu_{2}, \\cdots, \\mu_{K}) \\propto \n\\left \\{\n\\begin{aligned}\n&\\exp \\left ( -\\lVert x-\\mu_{z} \\rVert_{2}^{2} \\right ) & \\qquad \\lVert x-\\mu_{z} \\rVert_{2} = \\min_{1 \\le k \\le K} \\lVert x-\\mu_{k} \\rVert_{2} \\\\\n&0 & \\qquad \\lVert x-\\mu_{z} \\rVert_{2} > \\min_{1 \\le k \\le K} \\lVert x-\\mu_{k} \\rVert_{2}\n\\end{aligned}\n\\right.\n$$\n $z \\in \\{1, 2, \\cdots, K\\}$ \n\n $K$  $K$ \n\n EM  K-means \n\n**E **\n\n1. \n   $$\n   p(z_{i} \\mid x_{i}, \\{\\mu_{k}\\}^{(t)}) \\propto \n   \\left \\{\n   \\begin{aligned}\n   &1 & \\qquad \\lVert x_{i}-\\mu_{z_{i}}^{(t)} \\rVert_{2} = \\min_{1 \\le k \\le K} \\lVert x_{i}-\\mu_{k}^{(t)} \\rVert_{2} \\\\\n   &0 & \\qquad \\lVert x_{i}-\\mu_{z_{i}}^{(t)} \\rVert_{2} > \\min_{1 \\le k \\le K} \\lVert x_{i}-\\mu_{k}^{(t)} \\rVert_{2}\n   \\end{aligned}\n   \\right.\n   $$\n   \n   $$\n   p(Z \\mid X, \\{ \\mu_{k} \\}^{(t)}) = \\prod_{i=1}^{N} p(z_{i} \\mid x_{i}, \\{ \\mu_{k} \\}^{(t)})\n   $$\n   \n\n    $y_{i}$ \n\n2. \n   $$\n   \\begin{aligned}\n   Q(\\{ \\mu_{k} \\} \\mid \\{ \\mu_{k} \\}^{(t)}) & = E_{Z \\mid X, \\{ \\mu_{k} \\}^{(t)}} [\\log p(X, Z \\mid \\{ \\mu_{k} \\})] \\\\\n   & = \\sum_{i=1}^{N} E_{z_{i} \\mid x_{i}, \\{ \\mu_{k} \\}^{(t)}} [\\log p(x_{i}, z_{i} \\mid \\{ \\mu_{k} \\})] \\\\\n   & = \\sum_{i=1}^{N} \\log p(x_{i}, z_{i}=y_{i} \\mid \\{ \\mu_{k} \\}) \\\\\n   & = \\mathrm{const} - \\sum_{i=1}^{N} \\lVert x_{i} - \\mu_{y_{i}} \\rVert_{2}^{2}\n   \\end{aligned}\n   $$\n    $Q'(\\{ \\mu_{k} \\} \\mid \\{ \\mu_{k} \\}^{(t)}) = \\sum\\limits_{i=1}^{N} \\lVert x_{i} - \\mu_{y_{i}} \\rVert_{2}^{2}$ \n\n**M **\n\n $\\{ \\mu_{k} \\}$  $Q'$ \n\n $\\{1, 2, \\cdots, K \\}$  $K$ $I_{k} = \\{ i \\mid y_{i}=k \\}$ \n$$\nQ'(\\{ \\mu_{k} \\} \\mid \\{ \\mu_{k} \\}^{(t)}) = \\sum\\limits_{i=1}^{K} \\sum_{i \\in I_{k}} \\lVert x_{i} - \\mu_{k} \\rVert_{2}^{2}\n$$\n $\\sum\\limits_{i \\in I_{k}} \\lVert x_{i} - \\mu_{k} \\rVert_{2}^{2}$ \n\n K-means \n\n# \n\n GMM K-means  GMM  $\\Sigma = \\sigma I$  $\\sigma \\to 0$  [](http://icml.cc/2012/papers/291.pdf)  2.1 ","source":"_posts/kmeans-is-a-gmm.md","raw":"---\ntitle: K-means  GMM \ndate: 2017-10-23 23:23:56\ncategories: ML\ntags:\n     - K-means\n     - GMM\n     - EM Algorithm\ndescription:  GMM  K-means  EM \n---\n\n#  GMM \n\n K-means \n GMMGaussian Mixture Model K-means  EM  K-means  EM \n\n\n\n**** $x_{1}, x_{2}, \\cdots, x_{N} \\in \\mathbb{R}^{m}$  $K$  $K$  $\\mu_{1}, \\mu_{2}, \\cdots, \\mu_{K}$  $\\sum\\limits_{i=1}^{N} \\min\\limits_{1 \\le k \\le K} \\lVert x_{i} - \\mu_{k} \\rVert_{2}^{2}$ \n\n**** $x_{1}, x_{2}, \\cdots, x_{N} \\in \\mathbb{R}^{m}$  $K$ \n$$\np(x, z \\mid \\mu_{1}, \\mu_{2}, \\cdots, \\mu_{K}) \\propto \n\\left \\{\n\\begin{aligned}\n&\\exp \\left ( -\\lVert x-\\mu_{z} \\rVert_{2}^{2} \\right ) & \\qquad \\lVert x-\\mu_{z} \\rVert_{2} = \\min_{1 \\le k \\le K} \\lVert x-\\mu_{k} \\rVert_{2} \\\\\n&0 & \\qquad \\lVert x-\\mu_{z} \\rVert_{2} > \\min_{1 \\le k \\le K} \\lVert x-\\mu_{k} \\rVert_{2}\n\\end{aligned}\n\\right.\n$$\n $z \\in \\{1, 2, \\cdots, K\\}$ \n\n $K$  $K$ \n\n EM  K-means \n\n**E **\n\n1. \n   $$\n   p(z_{i} \\mid x_{i}, \\{\\mu_{k}\\}^{(t)}) \\propto \n   \\left \\{\n   \\begin{aligned}\n   &1 & \\qquad \\lVert x_{i}-\\mu_{z_{i}}^{(t)} \\rVert_{2} = \\min_{1 \\le k \\le K} \\lVert x_{i}-\\mu_{k}^{(t)} \\rVert_{2} \\\\\n   &0 & \\qquad \\lVert x_{i}-\\mu_{z_{i}}^{(t)} \\rVert_{2} > \\min_{1 \\le k \\le K} \\lVert x_{i}-\\mu_{k}^{(t)} \\rVert_{2}\n   \\end{aligned}\n   \\right.\n   $$\n   \n   $$\n   p(Z \\mid X, \\{ \\mu_{k} \\}^{(t)}) = \\prod_{i=1}^{N} p(z_{i} \\mid x_{i}, \\{ \\mu_{k} \\}^{(t)})\n   $$\n   \n\n    $y_{i}$ \n\n2. \n   $$\n   \\begin{aligned}\n   Q(\\{ \\mu_{k} \\} \\mid \\{ \\mu_{k} \\}^{(t)}) & = E_{Z \\mid X, \\{ \\mu_{k} \\}^{(t)}} [\\log p(X, Z \\mid \\{ \\mu_{k} \\})] \\\\\n   & = \\sum_{i=1}^{N} E_{z_{i} \\mid x_{i}, \\{ \\mu_{k} \\}^{(t)}} [\\log p(x_{i}, z_{i} \\mid \\{ \\mu_{k} \\})] \\\\\n   & = \\sum_{i=1}^{N} \\log p(x_{i}, z_{i}=y_{i} \\mid \\{ \\mu_{k} \\}) \\\\\n   & = \\mathrm{const} - \\sum_{i=1}^{N} \\lVert x_{i} - \\mu_{y_{i}} \\rVert_{2}^{2}\n   \\end{aligned}\n   $$\n    $Q'(\\{ \\mu_{k} \\} \\mid \\{ \\mu_{k} \\}^{(t)}) = \\sum\\limits_{i=1}^{N} \\lVert x_{i} - \\mu_{y_{i}} \\rVert_{2}^{2}$ \n\n**M **\n\n $\\{ \\mu_{k} \\}$  $Q'$ \n\n $\\{1, 2, \\cdots, K \\}$  $K$ $I_{k} = \\{ i \\mid y_{i}=k \\}$ \n$$\nQ'(\\{ \\mu_{k} \\} \\mid \\{ \\mu_{k} \\}^{(t)}) = \\sum\\limits_{i=1}^{K} \\sum_{i \\in I_{k}} \\lVert x_{i} - \\mu_{k} \\rVert_{2}^{2}\n$$\n $\\sum\\limits_{i \\in I_{k}} \\lVert x_{i} - \\mu_{k} \\rVert_{2}^{2}$ \n\n K-means \n\n# \n\n GMM K-means  GMM  $\\Sigma = \\sigma I$  $\\sigma \\to 0$  [](http://icml.cc/2012/papers/291.pdf)  2.1 ","slug":"kmeans-is-a-gmm","published":1,"updated":"2021-07-19T13:03:31.697Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckwrktive000dqxt6y2wq3vso","content":"<h1 id=\"-GMM-\"><a href=\"#-GMM-\" class=\"headerlink\" title=\" GMM \"></a> GMM </h1><p> K-means <br> GMMGaussian Mixture Model K-means  EM  K-means  EM </p>\n<p></p>\n<p><strong></strong> $x_{1}, x_{2}, \\cdots, x_{N} \\in \\mathbb{R}^{m}$  $K$  $K$  $\\mu_{1}, \\mu_{2}, \\cdots, \\mu_{K}$  $\\sum\\limits_{i=1}^{N} \\min\\limits_{1 \\le k \\le K} \\lVert x_{i} - \\mu_{k} \\rVert_{2}^{2}$ </p>\n<p><strong></strong> $x_{1}, x_{2}, \\cdots, x_{N} \\in \\mathbb{R}^{m}$  $K$ </p>\n<script type=\"math/tex; mode=display\">\np(x, z \\mid \\mu_{1}, \\mu_{2}, \\cdots, \\mu_{K}) \\propto \n\\left \\{\n\\begin{aligned}\n&\\exp \\left ( -\\lVert x-\\mu_{z} \\rVert_{2}^{2} \\right ) & \\qquad \\lVert x-\\mu_{z} \\rVert_{2} = \\min_{1 \\le k \\le K} \\lVert x-\\mu_{k} \\rVert_{2} \\\\\n&0 & \\qquad \\lVert x-\\mu_{z} \\rVert_{2} > \\min_{1 \\le k \\le K} \\lVert x-\\mu_{k} \\rVert_{2}\n\\end{aligned}\n\\right.</script><p> $z \\in \\{1, 2, \\cdots, K\\}$ </p>\n<p> $K$  $K$ </p>\n<p> EM  K-means </p>\n<p><strong>E </strong></p>\n<ol>\n<li><p></p>\n<script type=\"math/tex; mode=display\">\np(z_{i} \\mid x_{i}, \\{\\mu_{k}\\}^{(t)}) \\propto \n\\left \\{\n\\begin{aligned}\n&1 & \\qquad \\lVert x_{i}-\\mu_{z_{i}}^{(t)} \\rVert_{2} = \\min_{1 \\le k \\le K} \\lVert x_{i}-\\mu_{k}^{(t)} \\rVert_{2} \\\\\n&0 & \\qquad \\lVert x_{i}-\\mu_{z_{i}}^{(t)} \\rVert_{2} > \\min_{1 \\le k \\le K} \\lVert x_{i}-\\mu_{k}^{(t)} \\rVert_{2}\n\\end{aligned}\n\\right.</script><p></p>\n<script type=\"math/tex; mode=display\">\np(Z \\mid X, \\{ \\mu_{k} \\}^{(t)}) = \\prod_{i=1}^{N} p(z_{i} \\mid x_{i}, \\{ \\mu_{k} \\}^{(t)})</script><p></p>\n<p> $y_{i}$ </p>\n</li>\n<li><p></p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nQ(\\{ \\mu_{k} \\} \\mid \\{ \\mu_{k} \\}^{(t)}) & = E_{Z \\mid X, \\{ \\mu_{k} \\}^{(t)}} [\\log p(X, Z \\mid \\{ \\mu_{k} \\})] \\\\\n& = \\sum_{i=1}^{N} E_{z_{i} \\mid x_{i}, \\{ \\mu_{k} \\}^{(t)}} [\\log p(x_{i}, z_{i} \\mid \\{ \\mu_{k} \\})] \\\\\n& = \\sum_{i=1}^{N} \\log p(x_{i}, z_{i}=y_{i} \\mid \\{ \\mu_{k} \\}) \\\\\n& = \\mathrm{const} - \\sum_{i=1}^{N} \\lVert x_{i} - \\mu_{y_{i}} \\rVert_{2}^{2}\n\\end{aligned}</script><p> $Q(\\{ \\mu_{k} \\} \\mid \\{ \\mu_{k} \\}^{(t)}) = \\sum\\limits_{i=1}^{N} \\lVert x_{i} - \\mu_{y_{i}} \\rVert_{2}^{2}$ </p>\n</li>\n</ol>\n<p><strong>M </strong></p>\n<p> $\\{ \\mu_{k} \\}$  $Q$ </p>\n<p> $\\{1, 2, \\cdots, K \\}$  $K$ $I_{k} = \\{ i \\mid y_{i}=k \\}$ </p>\n<script type=\"math/tex; mode=display\">\nQ'(\\{ \\mu_{k} \\} \\mid \\{ \\mu_{k} \\}^{(t)}) = \\sum\\limits_{i=1}^{K} \\sum_{i \\in I_{k}} \\lVert x_{i} - \\mu_{k} \\rVert_{2}^{2}</script><p> $\\sum\\limits_{i \\in I_{k}} \\lVert x_{i} - \\mu_{k} \\rVert_{2}^{2}$ </p>\n<p> K-means </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> GMM K-means  GMM  $\\Sigma = \\sigma I$  $\\sigma \\to 0$  <a href=\"http://icml.cc/2012/papers/291.pdf\" target=\"_blank\" rel=\"noopener\"></a>  2.1 </p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"-GMM-\"><a href=\"#-GMM-\" class=\"headerlink\" title=\" GMM \"></a> GMM </h1><p> K-means <br> GMMGaussian Mixture Model K-means  EM  K-means  EM </p>\n<p></p>\n<p><strong></strong> $x_{1}, x_{2}, \\cdots, x_{N} \\in \\mathbb{R}^{m}$  $K$  $K$  $\\mu_{1}, \\mu_{2}, \\cdots, \\mu_{K}$  $\\sum\\limits_{i=1}^{N} \\min\\limits_{1 \\le k \\le K} \\lVert x_{i} - \\mu_{k} \\rVert_{2}^{2}$ </p>\n<p><strong></strong> $x_{1}, x_{2}, \\cdots, x_{N} \\in \\mathbb{R}^{m}$  $K$ </p>\n<script type=\"math/tex; mode=display\">\np(x, z \\mid \\mu_{1}, \\mu_{2}, \\cdots, \\mu_{K}) \\propto \n\\left \\{\n\\begin{aligned}\n&\\exp \\left ( -\\lVert x-\\mu_{z} \\rVert_{2}^{2} \\right ) & \\qquad \\lVert x-\\mu_{z} \\rVert_{2} = \\min_{1 \\le k \\le K} \\lVert x-\\mu_{k} \\rVert_{2} \\\\\n&0 & \\qquad \\lVert x-\\mu_{z} \\rVert_{2} > \\min_{1 \\le k \\le K} \\lVert x-\\mu_{k} \\rVert_{2}\n\\end{aligned}\n\\right.</script><p> $z \\in \\{1, 2, \\cdots, K\\}$ </p>\n<p> $K$  $K$ </p>\n<p> EM  K-means </p>\n<p><strong>E </strong></p>\n<ol>\n<li><p></p>\n<script type=\"math/tex; mode=display\">\np(z_{i} \\mid x_{i}, \\{\\mu_{k}\\}^{(t)}) \\propto \n\\left \\{\n\\begin{aligned}\n&1 & \\qquad \\lVert x_{i}-\\mu_{z_{i}}^{(t)} \\rVert_{2} = \\min_{1 \\le k \\le K} \\lVert x_{i}-\\mu_{k}^{(t)} \\rVert_{2} \\\\\n&0 & \\qquad \\lVert x_{i}-\\mu_{z_{i}}^{(t)} \\rVert_{2} > \\min_{1 \\le k \\le K} \\lVert x_{i}-\\mu_{k}^{(t)} \\rVert_{2}\n\\end{aligned}\n\\right.</script><p></p>\n<script type=\"math/tex; mode=display\">\np(Z \\mid X, \\{ \\mu_{k} \\}^{(t)}) = \\prod_{i=1}^{N} p(z_{i} \\mid x_{i}, \\{ \\mu_{k} \\}^{(t)})</script><p></p>\n<p> $y_{i}$ </p>\n</li>\n<li><p></p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nQ(\\{ \\mu_{k} \\} \\mid \\{ \\mu_{k} \\}^{(t)}) & = E_{Z \\mid X, \\{ \\mu_{k} \\}^{(t)}} [\\log p(X, Z \\mid \\{ \\mu_{k} \\})] \\\\\n& = \\sum_{i=1}^{N} E_{z_{i} \\mid x_{i}, \\{ \\mu_{k} \\}^{(t)}} [\\log p(x_{i}, z_{i} \\mid \\{ \\mu_{k} \\})] \\\\\n& = \\sum_{i=1}^{N} \\log p(x_{i}, z_{i}=y_{i} \\mid \\{ \\mu_{k} \\}) \\\\\n& = \\mathrm{const} - \\sum_{i=1}^{N} \\lVert x_{i} - \\mu_{y_{i}} \\rVert_{2}^{2}\n\\end{aligned}</script><p> $Q(\\{ \\mu_{k} \\} \\mid \\{ \\mu_{k} \\}^{(t)}) = \\sum\\limits_{i=1}^{N} \\lVert x_{i} - \\mu_{y_{i}} \\rVert_{2}^{2}$ </p>\n</li>\n</ol>\n<p><strong>M </strong></p>\n<p> $\\{ \\mu_{k} \\}$  $Q$ </p>\n<p> $\\{1, 2, \\cdots, K \\}$  $K$ $I_{k} = \\{ i \\mid y_{i}=k \\}$ </p>\n<script type=\"math/tex; mode=display\">\nQ'(\\{ \\mu_{k} \\} \\mid \\{ \\mu_{k} \\}^{(t)}) = \\sum\\limits_{i=1}^{K} \\sum_{i \\in I_{k}} \\lVert x_{i} - \\mu_{k} \\rVert_{2}^{2}</script><p> $\\sum\\limits_{i \\in I_{k}} \\lVert x_{i} - \\mu_{k} \\rVert_{2}^{2}$ </p>\n<p> K-means </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> GMM K-means  GMM  $\\Sigma = \\sigma I$  $\\sigma \\to 0$  <a href=\"http://icml.cc/2012/papers/291.pdf\" target=\"_blank\" rel=\"noopener\"></a>  2.1 </p>\n"},{"title":" L0 L1 L2 ","date":"2017-09-07T06:43:10.000Z","description":" L0 L1 L2 ","_content":"\n# \n\nMinimize your error while regularizing your parameters\nminimize error\"\nregularizing parameters Occam's Razor\n\n\n$$\nw^{\\ast} = \\mathop{\\arg\\min}_{w} \\sum_{i} L(y_{i}, f(x_{i};w)) + \\lambda\\Omega(w)\n$$\n\n\n- Square loss  \n- Hinge loss  SVM\n- Exp loss  Boosting\n- Log loss  Logistic regression\n\n\n loss \n\n $ \\Omega(w) $ \n\n# \n\n $w$ L0 L1 L2 Frobenius \n\n L0 L1   L2 \n\n\n\n## L0  L1 \n\n\n\nL0  0 \n NP-hard \n L0  L1 [^1]\n\n\n\n-  0 \n-  10  10 \n\n## L2 \n\n L1  L2 \n\n L2 \n\nill-condition\n\n $AX = b$  $A$  $b$  $X$ ill-conditionwell-condition\n\n\n\n $A$  $A$ condition number\n$$\n\\kappa(A) = \\lVert A \\rVert \\lVert A^{-1} \\rVert\n$$\n A\n$$\n\\frac{\\lVert \\Delta x \\rVert}{\\lVert x \\rVert} \\le \\kappa(A) \\cdot \\frac{\\lVert \\Delta b \\rVert}{\\lVert b \\rVert} \\\\\n\\frac{\\lVert \\Delta x \\rVert}{\\lVert x + \\Delta x \\rVert} \\le \\kappa(A) \\cdot \\frac{\\lVert \\Delta A \\rVert}{\\lVert A \\rVert}\n$$\ncondition number \n condition number  1  well-condition  ill-condition \n\n\n$$\nw^{\\ast} = (X^{\\mathsf{T}}X)^{-1}X^{\\mathsf{T}}y\n$$\n $ X^{\\mathsf{T}}X $ \n L2 \n$$\nw^{\\ast} = (X^{\\mathsf{T}}X + \\lambda I)^{-1}X^{\\mathsf{T}}y\n$$\n\n\n\n $ \\lambda $ $\\lambda$ -strongly convex\n\n $f$ \n$$\nf(y) \\ge f(x) + \\langle \\nabla f(x), y-x \\rangle + \\frac{\\lambda}{2} \\lVert y-x \\rVert ^{2}\n$$\n $f$  $ \\lambda $ $\\lambda$ -strongly convex\n\n\n$$\nf(y) \\ge f(x) + \\langle \\nabla f(x), y-x \\rangle + o (\\lVert y-x \\rVert)\n$$\n\n\n\n[^1]: [Ramirez, C. & V. Kreinovich & M. Argaez. Why l1 is a good approximation to l0: A geometric explanation [J]. *Engineering*, 2013, 7 (3): 203-207](https://www.researchgate.net/publication/290729378_Why_l1_is_a_good_approximation_to_l0_A_geometric_explanation)\n\n","source":"_posts/norm-regularization-01.md","raw":"---\ntitle:  L0 L1 L2 \ndate: 2017-09-07 14:43:10\ncategories: ML\ntags:\n      - Norm regularization\n      - Convex optimization\ndescription:  L0 L1 L2 \n---\n\n# \n\nMinimize your error while regularizing your parameters\nminimize error\"\nregularizing parameters Occam's Razor\n\n\n$$\nw^{\\ast} = \\mathop{\\arg\\min}_{w} \\sum_{i} L(y_{i}, f(x_{i};w)) + \\lambda\\Omega(w)\n$$\n\n\n- Square loss  \n- Hinge loss  SVM\n- Exp loss  Boosting\n- Log loss  Logistic regression\n\n\n loss \n\n $ \\Omega(w) $ \n\n# \n\n $w$ L0 L1 L2 Frobenius \n\n L0 L1   L2 \n\n\n\n## L0  L1 \n\n\n\nL0  0 \n NP-hard \n L0  L1 [^1]\n\n\n\n-  0 \n-  10  10 \n\n## L2 \n\n L1  L2 \n\n L2 \n\nill-condition\n\n $AX = b$  $A$  $b$  $X$ ill-conditionwell-condition\n\n\n\n $A$  $A$ condition number\n$$\n\\kappa(A) = \\lVert A \\rVert \\lVert A^{-1} \\rVert\n$$\n A\n$$\n\\frac{\\lVert \\Delta x \\rVert}{\\lVert x \\rVert} \\le \\kappa(A) \\cdot \\frac{\\lVert \\Delta b \\rVert}{\\lVert b \\rVert} \\\\\n\\frac{\\lVert \\Delta x \\rVert}{\\lVert x + \\Delta x \\rVert} \\le \\kappa(A) \\cdot \\frac{\\lVert \\Delta A \\rVert}{\\lVert A \\rVert}\n$$\ncondition number \n condition number  1  well-condition  ill-condition \n\n\n$$\nw^{\\ast} = (X^{\\mathsf{T}}X)^{-1}X^{\\mathsf{T}}y\n$$\n $ X^{\\mathsf{T}}X $ \n L2 \n$$\nw^{\\ast} = (X^{\\mathsf{T}}X + \\lambda I)^{-1}X^{\\mathsf{T}}y\n$$\n\n\n\n $ \\lambda $ $\\lambda$ -strongly convex\n\n $f$ \n$$\nf(y) \\ge f(x) + \\langle \\nabla f(x), y-x \\rangle + \\frac{\\lambda}{2} \\lVert y-x \\rVert ^{2}\n$$\n $f$  $ \\lambda $ $\\lambda$ -strongly convex\n\n\n$$\nf(y) \\ge f(x) + \\langle \\nabla f(x), y-x \\rangle + o (\\lVert y-x \\rVert)\n$$\n\n\n\n[^1]: [Ramirez, C. & V. Kreinovich & M. Argaez. Why l1 is a good approximation to l0: A geometric explanation [J]. *Engineering*, 2013, 7 (3): 203-207](https://www.researchgate.net/publication/290729378_Why_l1_is_a_good_approximation_to_l0_A_geometric_explanation)\n\n","slug":"norm-regularization-01","published":1,"updated":"2021-07-19T13:03:31.697Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckwrktivf000eqxt6fxi9az8z","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>Minimize your error while regularizing your parameters<br>minimize error<br>regularizing parameters Occams Razor</p>\n<p></p>\n<script type=\"math/tex; mode=display\">\nw^{\\ast} = \\mathop{\\arg\\min}_{w} \\sum_{i} L(y_{i}, f(x_{i};w)) + \\lambda\\Omega(w)</script><p></p>\n<ul>\n<li>Square loss  </li>\n<li>Hinge loss  SVM</li>\n<li>Exp loss  Boosting</li>\n<li>Log loss  Logistic regression</li>\n</ul>\n<p><br> loss </p>\n<p> $ \\Omega(w) $ </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> $w$ L0 L1 L2 Frobenius </p>\n<p> L0 L1   L2 </p>\n<p></p>\n<h2 id=\"L0--L1-\"><a href=\"#L0--L1-\" class=\"headerlink\" title=\"L0  L1 \"></a>L0  L1 </h2><p></p>\n<p>L0  0 <br> NP-hard <br> L0  L1 <sup id=\"fnref:1\"><a href=\"#fn:1\" rel=\"footnote\">1</a></sup></p>\n<p></p>\n<ul>\n<li> 0 </li>\n<li> 10  10 </li>\n</ul>\n<h2 id=\"L2-\"><a href=\"#L2-\" class=\"headerlink\" title=\"L2 \"></a>L2 </h2><p> L1  L2 </p>\n<p> L2 </p>\n<p>ill-condition<br><br> $AX = b$  $A$  $b$  $X$ ill-conditionwell-condition</p>\n<p></p>\n<p> $A$  $A$ condition number</p>\n<script type=\"math/tex; mode=display\">\n\\kappa(A) = \\lVert A \\rVert \\lVert A^{-1} \\rVert</script><p> A</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\lVert \\Delta x \\rVert}{\\lVert x \\rVert} \\le \\kappa(A) \\cdot \\frac{\\lVert \\Delta b \\rVert}{\\lVert b \\rVert} \\\\\n\\frac{\\lVert \\Delta x \\rVert}{\\lVert x + \\Delta x \\rVert} \\le \\kappa(A) \\cdot \\frac{\\lVert \\Delta A \\rVert}{\\lVert A \\rVert}</script><p>condition number <br> condition number  1  well-condition  ill-condition </p>\n<p></p>\n<script type=\"math/tex; mode=display\">\nw^{\\ast} = (X^{\\mathsf{T}}X)^{-1}X^{\\mathsf{T}}y</script><p> $ X^{\\mathsf{T}}X $ <br> L2 </p>\n<script type=\"math/tex; mode=display\">\nw^{\\ast} = (X^{\\mathsf{T}}X + \\lambda I)^{-1}X^{\\mathsf{T}}y</script><p></p>\n<p><br> $ \\lambda $ $\\lambda$ -strongly convex</p>\n<p> $f$ </p>\n<script type=\"math/tex; mode=display\">\nf(y) \\ge f(x) + \\langle \\nabla f(x), y-x \\rangle + \\frac{\\lambda}{2} \\lVert y-x \\rVert ^{2}</script><p> $f$  $ \\lambda $ $\\lambda$ -strongly convex</p>\n<p></p>\n<script type=\"math/tex; mode=display\">\nf(y) \\ge f(x) + \\langle \\nabla f(x), y-x \\rangle + o (\\lVert y-x \\rVert)</script><p><br></p>\n<div id=\"footnotes\"><hr><div id=\"footnotelist\"><ol style=\"list-style:none; padding-left: 0;\"><li id=\"fn:1\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px;\">1.</span><span style=\"display: inline-block; vertical-align: top;\"><a href=\"https://www.researchgate.net/publication/290729378_Why_l1_is_a_good_approximation_to_l0_A_geometric_explanation\" target=\"_blank\" rel=\"noopener\">Ramirez, C. &amp; V. Kreinovich &amp; M. Argaez. Why l1 is a good approximation to l0: A geometric explanation [J]. <em>Engineering</em>, 2013, 7 (3): 203-207</a></span><a href=\"#fnref:1\" rev=\"footnote\"> </a></li></ol></div></div>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>Minimize your error while regularizing your parameters<br>minimize error<br>regularizing parameters Occams Razor</p>\n<p></p>\n<script type=\"math/tex; mode=display\">\nw^{\\ast} = \\mathop{\\arg\\min}_{w} \\sum_{i} L(y_{i}, f(x_{i};w)) + \\lambda\\Omega(w)</script><p></p>\n<ul>\n<li>Square loss  </li>\n<li>Hinge loss  SVM</li>\n<li>Exp loss  Boosting</li>\n<li>Log loss  Logistic regression</li>\n</ul>\n<p><br> loss </p>\n<p> $ \\Omega(w) $ </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> $w$ L0 L1 L2 Frobenius </p>\n<p> L0 L1   L2 </p>\n<p></p>\n<h2 id=\"L0--L1-\"><a href=\"#L0--L1-\" class=\"headerlink\" title=\"L0  L1 \"></a>L0  L1 </h2><p></p>\n<p>L0  0 <br> NP-hard <br> L0  L1 <sup id=\"fnref:1\"><a href=\"#fn:1\" rel=\"footnote\">1</a></sup></p>\n<p></p>\n<ul>\n<li> 0 </li>\n<li> 10  10 </li>\n</ul>\n<h2 id=\"L2-\"><a href=\"#L2-\" class=\"headerlink\" title=\"L2 \"></a>L2 </h2><p> L1  L2 </p>\n<p> L2 </p>\n<p>ill-condition<br><br> $AX = b$  $A$  $b$  $X$ ill-conditionwell-condition</p>\n<p></p>\n<p> $A$  $A$ condition number</p>\n<script type=\"math/tex; mode=display\">\n\\kappa(A) = \\lVert A \\rVert \\lVert A^{-1} \\rVert</script><p> A</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\lVert \\Delta x \\rVert}{\\lVert x \\rVert} \\le \\kappa(A) \\cdot \\frac{\\lVert \\Delta b \\rVert}{\\lVert b \\rVert} \\\\\n\\frac{\\lVert \\Delta x \\rVert}{\\lVert x + \\Delta x \\rVert} \\le \\kappa(A) \\cdot \\frac{\\lVert \\Delta A \\rVert}{\\lVert A \\rVert}</script><p>condition number <br> condition number  1  well-condition  ill-condition </p>\n<p></p>\n<script type=\"math/tex; mode=display\">\nw^{\\ast} = (X^{\\mathsf{T}}X)^{-1}X^{\\mathsf{T}}y</script><p> $ X^{\\mathsf{T}}X $ <br> L2 </p>\n<script type=\"math/tex; mode=display\">\nw^{\\ast} = (X^{\\mathsf{T}}X + \\lambda I)^{-1}X^{\\mathsf{T}}y</script><p></p>\n<p><br> $ \\lambda $ $\\lambda$ -strongly convex</p>\n<p> $f$ </p>\n<script type=\"math/tex; mode=display\">\nf(y) \\ge f(x) + \\langle \\nabla f(x), y-x \\rangle + \\frac{\\lambda}{2} \\lVert y-x \\rVert ^{2}</script><p> $f$  $ \\lambda $ $\\lambda$ -strongly convex</p>\n<p></p>\n<script type=\"math/tex; mode=display\">\nf(y) \\ge f(x) + \\langle \\nabla f(x), y-x \\rangle + o (\\lVert y-x \\rVert)</script><p><br></p>\n<div id=\"footnotes\"><hr><div id=\"footnotelist\"><ol style=\"list-style:none; padding-left: 0;\"><li id=\"fn:1\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px;\">1.</span><span style=\"display: inline-block; vertical-align: top;\"><a href=\"https://www.researchgate.net/publication/290729378_Why_l1_is_a_good_approximation_to_l0_A_geometric_explanation\" target=\"_blank\" rel=\"noopener\">Ramirez, C. &amp; V. Kreinovich &amp; M. Argaez. Why l1 is a good approximation to l0: A geometric explanation [J]. <em>Engineering</em>, 2013, 7 (3): 203-207</a></span><a href=\"#fnref:1\" rev=\"footnote\"> </a></li></ol></div></div>"},{"title":"","date":"2021-07-20T13:37:01.000Z","description":"","_content":"\n# \n\nGarbage CollectionGC****\n\n\n\n1. \n2. \n\n A  A  B B  B  A \n\n# -Marking and Sweeping\n\n- John McCarthy  Lisp \n\n1.  bit  true\n2. \n\n\n\n# \n\nVirtual MachineVM\n\n```c\ntypedef enum {\n  OBJ_INT,\n  OBJ_PAIR\n} ObjectType;\n```\n\n OBJ_PAIR  INT  INT  PAIR  PAIR \n\n PAIR  LIST  LIST  PAIR\n\n```\nLIST = (LIST , LIST )\nLIST  = (LIST , LIST )\n```\n\n PAIR \n\n VM \n\n```c\ntypedef struct sObject {\n  ObjectType type;\n\n  union {\n    /* OBJ_INT */\n    int value;\n\n    /* OBJ_PAIR */\n    struct {\n      struct sObject* head;\n      struct sObject* tail;\n    };\n  };\n} Object;\n```\n\n#  VM\n\n VM  VM \n\n```c\n#define STACK_MAX 256\n\ntypedef struct {\n  Object* stack[STACK_MAX];\n  int stackSize;\n} VM;\n```\n\n new push  pop \n\n```c\nVM* newVM() {\n  VM* vm = malloc(sizeof(VM));\n  vm->stackSize = 0;\n  return vm;\n}\n\nvoid push(VM* vm, Object* value) {\n  assert(vm->stackSize < STACK_MAX, \"Stack overflow!\");\n  vm->stack[vm->stackSize++] = value;\n}\n\nObject* pop(VM* vm) {\n  assert(vm->stackSize > 0, \"Stack underflow!\");\n  return vm->stack[--vm->stackSize];\n}\n```\n\n VM \n\n```c\nObject* newObject(VM* vm, ObjectType type) {\n  Object* object = malloc(sizeof(Object));\n  object->type = type;\n  return object;\n}\n```\n\n PAIR  PAIR  PAIR  PAIR \n\n```c\nvoid pushInt(VM* vm, int intValue) {\n  Object* object = newObject(vm, OBJ_INT);\n  object->value = intValue;\n  push(vm, object);\n}\n\nObject* pushPair(VM* vm) {\n  Object* object = newObject(vm, OBJ_PAIR);\n  object->tail = pop(vm);\n  object->head = pop(vm);\n\n  push(vm, object);\n  return object;\n}\n```\n\nVM \n\n# \n\n\n\n```c\ntypedef struct sObject {\n  unsigned char marked;\n  /* Previous stuff... */\n} Object;\n```\n\n- VM PAIR  PAIR \n\n```c\nvoid mark(Object* object) {\n  object->marked = 1;\n\n  if (object->type == OBJ_PAIR) {\n    mark(object->head);\n    mark(object->tail);\n  }\n}\n\nvoid markAll(VM* vm)\n{\n  for (int i = 0; i < vm->stackSize; i++) {\n    mark(vm->stack[i]);\n  }\n}\n```\n\n mark \n\n```c\nvoid mark(Object* object) {\n  /* If already marked, we're done. Check this first\n     to avoid recursing on cycles in the object graph. */\n  if (object->marked) return;\n\n  object->marked = 1;\n\n  if (object->type == OBJ_PAIR) {\n    mark(object->head);\n    mark(object->tail);\n  }\n}\n```\n\n# \n\n-\n\n C \n\n VM \n\n```c\ntypedef struct sObject {\n  /* The next object in the list of all objects. */\n  struct sObject* next;\n\n  /* Previous stuff... */\n} Object;\n\ntypedef struct {\n  /* The first object in the list of all objects. */\n  Object* firstObject;\n\n  /* Previous stuff... */\n} VM;\n```\n\n VM  new  firstObject  NULL\n\n```c\nObject* newObject(VM* vm, ObjectType type) {\n  Object* object = malloc(sizeof(Object));\n  object->type = type;\n  object->marked = 0;\n\n  /* Insert it into the list of allocated objects. */\n  object->next = vm->firstObject;\n  vm->firstObject = object;\n\n  return object;\n}\n```\n\n Linux \n\n```c\nvoid sweep(VM* vm)\n{\n  Object** object = &vm->firstObject;\n  while (*object) {\n    if (!(*object)->marked) {\n      /* This object wasn't reached, so remove it from the list\n         and free it. */\n      Object* unreached = *object;\n\n      *object = unreached->next;\n      free(unreached);\n    } else {\n      /* This object was reached, so unmark it (for the next GC)\n         and move on to the next. */\n      (*object)->marked = 0;\n      object = &(*object)->next;\n    }\n  }\n}\n```\n\n\n\n```c\nvoid gc(VM* vm) {\n  markAll(vm);\n  sweep(vm);\n}\n```\n\n# \n\n VM \n\n\n\n```c\ntypedef struct {\n  /* The total number of currently allocated objects. */\n  int numObjects;\n\n  /* The number of objects required to trigger a GC. */\n  int maxObjects;\n\n  /* Previous stuff... */\n} VM;\n\n--------------------\n\nVM* newVM() {\n  /* Previous stuff... */\n\n  vm->numObjects = 0;\n  vm->maxObjects = INITIAL_GC_THRESHOLD;\n  return vm;\n}\n\n--------------------\n\nObject* newObject(VM* vm, ObjectType type) {\n  if (vm->numObjects == vm->maxObjects) gc(vm);\n\n  /* Create object... */\n\n  vm->numObjects++;\n  return object;\n}\n\n--------------------\n\nvoid gc(VM* vm) {\n  int numObjects = vm->numObjects;\n\n  markAll(vm);\n  sweep(vm);\n\n  vm->maxObjects = vm->numObjects * 2;\n}\n```\n\n 2  8  8 16 \n\n# \n\n OK \n\n GitHub[SakigamiYang/baby-gc](https://github.com/SakigamiYang/baby-gc)\n\n","source":"_posts/baby-gc.md","raw":"---\ntitle: \ndate: 2021-07-20 21:37:01\ncategories: Develop\ntags:\n- GC\ndescription: \n---\n\n# \n\nGarbage CollectionGC****\n\n\n\n1. \n2. \n\n A  A  B B  B  A \n\n# -Marking and Sweeping\n\n- John McCarthy  Lisp \n\n1.  bit  true\n2. \n\n\n\n# \n\nVirtual MachineVM\n\n```c\ntypedef enum {\n  OBJ_INT,\n  OBJ_PAIR\n} ObjectType;\n```\n\n OBJ_PAIR  INT  INT  PAIR  PAIR \n\n PAIR  LIST  LIST  PAIR\n\n```\nLIST = (LIST , LIST )\nLIST  = (LIST , LIST )\n```\n\n PAIR \n\n VM \n\n```c\ntypedef struct sObject {\n  ObjectType type;\n\n  union {\n    /* OBJ_INT */\n    int value;\n\n    /* OBJ_PAIR */\n    struct {\n      struct sObject* head;\n      struct sObject* tail;\n    };\n  };\n} Object;\n```\n\n#  VM\n\n VM  VM \n\n```c\n#define STACK_MAX 256\n\ntypedef struct {\n  Object* stack[STACK_MAX];\n  int stackSize;\n} VM;\n```\n\n new push  pop \n\n```c\nVM* newVM() {\n  VM* vm = malloc(sizeof(VM));\n  vm->stackSize = 0;\n  return vm;\n}\n\nvoid push(VM* vm, Object* value) {\n  assert(vm->stackSize < STACK_MAX, \"Stack overflow!\");\n  vm->stack[vm->stackSize++] = value;\n}\n\nObject* pop(VM* vm) {\n  assert(vm->stackSize > 0, \"Stack underflow!\");\n  return vm->stack[--vm->stackSize];\n}\n```\n\n VM \n\n```c\nObject* newObject(VM* vm, ObjectType type) {\n  Object* object = malloc(sizeof(Object));\n  object->type = type;\n  return object;\n}\n```\n\n PAIR  PAIR  PAIR  PAIR \n\n```c\nvoid pushInt(VM* vm, int intValue) {\n  Object* object = newObject(vm, OBJ_INT);\n  object->value = intValue;\n  push(vm, object);\n}\n\nObject* pushPair(VM* vm) {\n  Object* object = newObject(vm, OBJ_PAIR);\n  object->tail = pop(vm);\n  object->head = pop(vm);\n\n  push(vm, object);\n  return object;\n}\n```\n\nVM \n\n# \n\n\n\n```c\ntypedef struct sObject {\n  unsigned char marked;\n  /* Previous stuff... */\n} Object;\n```\n\n- VM PAIR  PAIR \n\n```c\nvoid mark(Object* object) {\n  object->marked = 1;\n\n  if (object->type == OBJ_PAIR) {\n    mark(object->head);\n    mark(object->tail);\n  }\n}\n\nvoid markAll(VM* vm)\n{\n  for (int i = 0; i < vm->stackSize; i++) {\n    mark(vm->stack[i]);\n  }\n}\n```\n\n mark \n\n```c\nvoid mark(Object* object) {\n  /* If already marked, we're done. Check this first\n     to avoid recursing on cycles in the object graph. */\n  if (object->marked) return;\n\n  object->marked = 1;\n\n  if (object->type == OBJ_PAIR) {\n    mark(object->head);\n    mark(object->tail);\n  }\n}\n```\n\n# \n\n-\n\n C \n\n VM \n\n```c\ntypedef struct sObject {\n  /* The next object in the list of all objects. */\n  struct sObject* next;\n\n  /* Previous stuff... */\n} Object;\n\ntypedef struct {\n  /* The first object in the list of all objects. */\n  Object* firstObject;\n\n  /* Previous stuff... */\n} VM;\n```\n\n VM  new  firstObject  NULL\n\n```c\nObject* newObject(VM* vm, ObjectType type) {\n  Object* object = malloc(sizeof(Object));\n  object->type = type;\n  object->marked = 0;\n\n  /* Insert it into the list of allocated objects. */\n  object->next = vm->firstObject;\n  vm->firstObject = object;\n\n  return object;\n}\n```\n\n Linux \n\n```c\nvoid sweep(VM* vm)\n{\n  Object** object = &vm->firstObject;\n  while (*object) {\n    if (!(*object)->marked) {\n      /* This object wasn't reached, so remove it from the list\n         and free it. */\n      Object* unreached = *object;\n\n      *object = unreached->next;\n      free(unreached);\n    } else {\n      /* This object was reached, so unmark it (for the next GC)\n         and move on to the next. */\n      (*object)->marked = 0;\n      object = &(*object)->next;\n    }\n  }\n}\n```\n\n\n\n```c\nvoid gc(VM* vm) {\n  markAll(vm);\n  sweep(vm);\n}\n```\n\n# \n\n VM \n\n\n\n```c\ntypedef struct {\n  /* The total number of currently allocated objects. */\n  int numObjects;\n\n  /* The number of objects required to trigger a GC. */\n  int maxObjects;\n\n  /* Previous stuff... */\n} VM;\n\n--------------------\n\nVM* newVM() {\n  /* Previous stuff... */\n\n  vm->numObjects = 0;\n  vm->maxObjects = INITIAL_GC_THRESHOLD;\n  return vm;\n}\n\n--------------------\n\nObject* newObject(VM* vm, ObjectType type) {\n  if (vm->numObjects == vm->maxObjects) gc(vm);\n\n  /* Create object... */\n\n  vm->numObjects++;\n  return object;\n}\n\n--------------------\n\nvoid gc(VM* vm) {\n  int numObjects = vm->numObjects;\n\n  markAll(vm);\n  sweep(vm);\n\n  vm->maxObjects = vm->numObjects * 2;\n}\n```\n\n 2  8  8 16 \n\n# \n\n OK \n\n GitHub[SakigamiYang/baby-gc](https://github.com/SakigamiYang/baby-gc)\n\n","slug":"baby-gc","published":1,"updated":"2021-07-20T14:29:13.794Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckwrktivf000fqxt6livwxo6n","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>Garbage CollectionGC<strong></strong></p>\n<p></p>\n<ol>\n<li></li>\n<li></li>\n</ol>\n<p> A  A  B B  B  A </p>\n<h1 id=\"-Marking-and-Sweeping\"><a href=\"#-Marking-and-Sweeping\" class=\"headerlink\" title=\"-Marking and Sweeping\"></a>-Marking and Sweeping</h1><p>- John McCarthy  Lisp </p>\n<ol>\n<li> bit  true</li>\n<li></li>\n</ol>\n<p></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>Virtual MachineVM</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"keyword\">enum</span> &#123;</span><br><span class=\"line\">  OBJ_INT,</span><br><span class=\"line\">  OBJ_PAIR</span><br><span class=\"line\">&#125; ObjectType;</span><br></pre></td></tr></table></figure>\n<p> OBJ_PAIR  INT  INT  PAIR  PAIR </p>\n<p> PAIR  LIST  LIST  PAIR</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">LIST = (LIST , LIST )</span><br><span class=\"line\">LIST  = (LIST , LIST )</span><br></pre></td></tr></table></figure>\n<p> PAIR </p>\n<p> VM </p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">sObject</span> &#123;</span></span><br><span class=\"line\">  ObjectType type;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">union</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">/* OBJ_INT */</span></span><br><span class=\"line\">    <span class=\"keyword\">int</span> value;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/* OBJ_PAIR */</span></span><br><span class=\"line\">    <span class=\"class\"><span class=\"keyword\">struct</span> &#123;</span></span><br><span class=\"line\">      <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">sObject</span>* <span class=\"title\">head</span>;</span></span><br><span class=\"line\">      <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">sObject</span>* <span class=\"title\">tail</span>;</span></span><br><span class=\"line\">    &#125;;</span><br><span class=\"line\">  &#125;;</span><br><span class=\"line\">&#125; Object;</span><br></pre></td></tr></table></figure>\n<h1 id=\"-VM\"><a href=\"#-VM\" class=\"headerlink\" title=\" VM\"></a> VM</h1><p> VM  VM </p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">define</span> STACK_MAX 256</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> &#123;</span></span><br><span class=\"line\">  Object* <span class=\"built_in\">stack</span>[STACK_MAX];</span><br><span class=\"line\">  <span class=\"keyword\">int</span> stackSize;</span><br><span class=\"line\">&#125; VM;</span><br></pre></td></tr></table></figure>\n<p> new push  pop </p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">VM* <span class=\"title\">newVM</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">  VM* vm = <span class=\"built_in\">malloc</span>(<span class=\"keyword\">sizeof</span>(VM));</span><br><span class=\"line\">  vm-&gt;stackSize = <span class=\"number\">0</span>;</span><br><span class=\"line\">  <span class=\"keyword\">return</span> vm;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">push</span><span class=\"params\">(VM* vm, Object* value)</span> </span>&#123;</span><br><span class=\"line\">  assert(vm-&gt;stackSize &lt; STACK_MAX, <span class=\"string\">\"Stack overflow!\"</span>);</span><br><span class=\"line\">  vm-&gt;<span class=\"built_in\">stack</span>[vm-&gt;stackSize++] = value;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\">Object* <span class=\"title\">pop</span><span class=\"params\">(VM* vm)</span> </span>&#123;</span><br><span class=\"line\">  assert(vm-&gt;stackSize &gt; <span class=\"number\">0</span>, <span class=\"string\">\"Stack underflow!\"</span>);</span><br><span class=\"line\">  <span class=\"keyword\">return</span> vm-&gt;<span class=\"built_in\">stack</span>[--vm-&gt;stackSize];</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p> VM </p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">Object* <span class=\"title\">newObject</span><span class=\"params\">(VM* vm, ObjectType type)</span> </span>&#123;</span><br><span class=\"line\">  Object* object = <span class=\"built_in\">malloc</span>(<span class=\"keyword\">sizeof</span>(Object));</span><br><span class=\"line\">  object-&gt;type = type;</span><br><span class=\"line\">  <span class=\"keyword\">return</span> object;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p> PAIR  PAIR  PAIR  PAIR </p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">pushInt</span><span class=\"params\">(VM* vm, <span class=\"keyword\">int</span> intValue)</span> </span>&#123;</span><br><span class=\"line\">  Object* object = newObject(vm, OBJ_INT);</span><br><span class=\"line\">  object-&gt;value = intValue;</span><br><span class=\"line\">  push(vm, object);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\">Object* <span class=\"title\">pushPair</span><span class=\"params\">(VM* vm)</span> </span>&#123;</span><br><span class=\"line\">  Object* object = newObject(vm, OBJ_PAIR);</span><br><span class=\"line\">  object-&gt;tail = pop(vm);</span><br><span class=\"line\">  object-&gt;head = pop(vm);</span><br><span class=\"line\"></span><br><span class=\"line\">  push(vm, object);</span><br><span class=\"line\">  <span class=\"keyword\">return</span> object;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>VM </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">sObject</span> &#123;</span></span><br><span class=\"line\">  <span class=\"keyword\">unsigned</span> <span class=\"keyword\">char</span> marked;</span><br><span class=\"line\">  <span class=\"comment\">/* Previous stuff... */</span></span><br><span class=\"line\">&#125; Object;</span><br></pre></td></tr></table></figure>\n<p>- VM PAIR  PAIR </p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">mark</span><span class=\"params\">(Object* object)</span> </span>&#123;</span><br><span class=\"line\">  object-&gt;marked = <span class=\"number\">1</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">if</span> (object-&gt;type == OBJ_PAIR) &#123;</span><br><span class=\"line\">    mark(object-&gt;head);</span><br><span class=\"line\">    mark(object-&gt;tail);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">markAll</span><span class=\"params\">(VM* vm)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; vm-&gt;stackSize; i++) &#123;</span><br><span class=\"line\">    mark(vm-&gt;<span class=\"built_in\">stack</span>[i]);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p> mark </p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">mark</span><span class=\"params\">(Object* object)</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"comment\">/* If already marked, we're done. Check this first</span></span><br><span class=\"line\"><span class=\"comment\">     to avoid recursing on cycles in the object graph. */</span></span><br><span class=\"line\">  <span class=\"keyword\">if</span> (object-&gt;marked) <span class=\"keyword\">return</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">  object-&gt;marked = <span class=\"number\">1</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">if</span> (object-&gt;type == OBJ_PAIR) &#123;</span><br><span class=\"line\">    mark(object-&gt;head);</span><br><span class=\"line\">    mark(object-&gt;tail);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>-</p>\n<p> C </p>\n<p> VM </p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">sObject</span> &#123;</span></span><br><span class=\"line\">  <span class=\"comment\">/* The next object in the list of all objects. */</span></span><br><span class=\"line\">  <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">sObject</span>* <span class=\"title\">next</span>;</span></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">/* Previous stuff... */</span></span><br><span class=\"line\">&#125; Object;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> &#123;</span></span><br><span class=\"line\">  <span class=\"comment\">/* The first object in the list of all objects. */</span></span><br><span class=\"line\">  Object* firstObject;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">/* Previous stuff... */</span></span><br><span class=\"line\">&#125; VM;</span><br></pre></td></tr></table></figure>\n<p> VM  new  firstObject  NULL</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">Object* <span class=\"title\">newObject</span><span class=\"params\">(VM* vm, ObjectType type)</span> </span>&#123;</span><br><span class=\"line\">  Object* object = <span class=\"built_in\">malloc</span>(<span class=\"keyword\">sizeof</span>(Object));</span><br><span class=\"line\">  object-&gt;type = type;</span><br><span class=\"line\">  object-&gt;marked = <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">/* Insert it into the list of allocated objects. */</span></span><br><span class=\"line\">  object-&gt;next = vm-&gt;firstObject;</span><br><span class=\"line\">  vm-&gt;firstObject = object;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">return</span> object;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p> Linux </p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">sweep</span><span class=\"params\">(VM* vm)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">  Object** object = &amp;vm-&gt;firstObject;</span><br><span class=\"line\">  <span class=\"keyword\">while</span> (*object) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (!(*object)-&gt;marked) &#123;</span><br><span class=\"line\">      <span class=\"comment\">/* This object wasn't reached, so remove it from the list</span></span><br><span class=\"line\"><span class=\"comment\">         and free it. */</span></span><br><span class=\"line\">      Object* unreached = *object;</span><br><span class=\"line\"></span><br><span class=\"line\">      *object = unreached-&gt;next;</span><br><span class=\"line\">      <span class=\"built_in\">free</span>(unreached);</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">      <span class=\"comment\">/* This object was reached, so unmark it (for the next GC)</span></span><br><span class=\"line\"><span class=\"comment\">         and move on to the next. */</span></span><br><span class=\"line\">      (*object)-&gt;marked = <span class=\"number\">0</span>;</span><br><span class=\"line\">      object = &amp;(*object)-&gt;next;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">gc</span><span class=\"params\">(VM* vm)</span> </span>&#123;</span><br><span class=\"line\">  markAll(vm);</span><br><span class=\"line\">  sweep(vm);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> VM </p>\n<p></p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> &#123;</span></span><br><span class=\"line\">  <span class=\"comment\">/* The total number of currently allocated objects. */</span></span><br><span class=\"line\">  <span class=\"keyword\">int</span> numObjects;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">/* The number of objects required to trigger a GC. */</span></span><br><span class=\"line\">  <span class=\"keyword\">int</span> maxObjects;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">/* Previous stuff... */</span></span><br><span class=\"line\">&#125; VM;</span><br><span class=\"line\"></span><br><span class=\"line\">--------------------</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\">VM* <span class=\"title\">newVM</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"comment\">/* Previous stuff... */</span></span><br><span class=\"line\"></span><br><span class=\"line\">  vm-&gt;numObjects = <span class=\"number\">0</span>;</span><br><span class=\"line\">  vm-&gt;maxObjects = INITIAL_GC_THRESHOLD;</span><br><span class=\"line\">  <span class=\"keyword\">return</span> vm;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">--------------------</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\">Object* <span class=\"title\">newObject</span><span class=\"params\">(VM* vm, ObjectType type)</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (vm-&gt;numObjects == vm-&gt;maxObjects) gc(vm);</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">/* Create object... */</span></span><br><span class=\"line\"></span><br><span class=\"line\">  vm-&gt;numObjects++;</span><br><span class=\"line\">  <span class=\"keyword\">return</span> object;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">--------------------</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">gc</span><span class=\"params\">(VM* vm)</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">int</span> numObjects = vm-&gt;numObjects;</span><br><span class=\"line\"></span><br><span class=\"line\">  markAll(vm);</span><br><span class=\"line\">  sweep(vm);</span><br><span class=\"line\"></span><br><span class=\"line\">  vm-&gt;maxObjects = vm-&gt;numObjects * <span class=\"number\">2</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p> 2  8  8 16 </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> OK </p>\n<p> GitHub<a href=\"https://github.com/SakigamiYang/baby-gc\" target=\"_blank\" rel=\"noopener\">SakigamiYang/baby-gc</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>Garbage CollectionGC<strong></strong></p>\n<p></p>\n<ol>\n<li></li>\n<li></li>\n</ol>\n<p> A  A  B B  B  A </p>\n<h1 id=\"-Marking-and-Sweeping\"><a href=\"#-Marking-and-Sweeping\" class=\"headerlink\" title=\"-Marking and Sweeping\"></a>-Marking and Sweeping</h1><p>- John McCarthy  Lisp </p>\n<ol>\n<li> bit  true</li>\n<li></li>\n</ol>\n<p></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>Virtual MachineVM</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"keyword\">enum</span> &#123;</span><br><span class=\"line\">  OBJ_INT,</span><br><span class=\"line\">  OBJ_PAIR</span><br><span class=\"line\">&#125; ObjectType;</span><br></pre></td></tr></table></figure>\n<p> OBJ_PAIR  INT  INT  PAIR  PAIR </p>\n<p> PAIR  LIST  LIST  PAIR</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">LIST = (LIST , LIST )</span><br><span class=\"line\">LIST  = (LIST , LIST )</span><br></pre></td></tr></table></figure>\n<p> PAIR </p>\n<p> VM </p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">sObject</span> &#123;</span></span><br><span class=\"line\">  ObjectType type;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">union</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">/* OBJ_INT */</span></span><br><span class=\"line\">    <span class=\"keyword\">int</span> value;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/* OBJ_PAIR */</span></span><br><span class=\"line\">    <span class=\"class\"><span class=\"keyword\">struct</span> &#123;</span></span><br><span class=\"line\">      <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">sObject</span>* <span class=\"title\">head</span>;</span></span><br><span class=\"line\">      <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">sObject</span>* <span class=\"title\">tail</span>;</span></span><br><span class=\"line\">    &#125;;</span><br><span class=\"line\">  &#125;;</span><br><span class=\"line\">&#125; Object;</span><br></pre></td></tr></table></figure>\n<h1 id=\"-VM\"><a href=\"#-VM\" class=\"headerlink\" title=\" VM\"></a> VM</h1><p> VM  VM </p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">define</span> STACK_MAX 256</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> &#123;</span></span><br><span class=\"line\">  Object* <span class=\"built_in\">stack</span>[STACK_MAX];</span><br><span class=\"line\">  <span class=\"keyword\">int</span> stackSize;</span><br><span class=\"line\">&#125; VM;</span><br></pre></td></tr></table></figure>\n<p> new push  pop </p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">VM* <span class=\"title\">newVM</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">  VM* vm = <span class=\"built_in\">malloc</span>(<span class=\"keyword\">sizeof</span>(VM));</span><br><span class=\"line\">  vm-&gt;stackSize = <span class=\"number\">0</span>;</span><br><span class=\"line\">  <span class=\"keyword\">return</span> vm;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">push</span><span class=\"params\">(VM* vm, Object* value)</span> </span>&#123;</span><br><span class=\"line\">  assert(vm-&gt;stackSize &lt; STACK_MAX, <span class=\"string\">\"Stack overflow!\"</span>);</span><br><span class=\"line\">  vm-&gt;<span class=\"built_in\">stack</span>[vm-&gt;stackSize++] = value;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\">Object* <span class=\"title\">pop</span><span class=\"params\">(VM* vm)</span> </span>&#123;</span><br><span class=\"line\">  assert(vm-&gt;stackSize &gt; <span class=\"number\">0</span>, <span class=\"string\">\"Stack underflow!\"</span>);</span><br><span class=\"line\">  <span class=\"keyword\">return</span> vm-&gt;<span class=\"built_in\">stack</span>[--vm-&gt;stackSize];</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p> VM </p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">Object* <span class=\"title\">newObject</span><span class=\"params\">(VM* vm, ObjectType type)</span> </span>&#123;</span><br><span class=\"line\">  Object* object = <span class=\"built_in\">malloc</span>(<span class=\"keyword\">sizeof</span>(Object));</span><br><span class=\"line\">  object-&gt;type = type;</span><br><span class=\"line\">  <span class=\"keyword\">return</span> object;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p> PAIR  PAIR  PAIR  PAIR </p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">pushInt</span><span class=\"params\">(VM* vm, <span class=\"keyword\">int</span> intValue)</span> </span>&#123;</span><br><span class=\"line\">  Object* object = newObject(vm, OBJ_INT);</span><br><span class=\"line\">  object-&gt;value = intValue;</span><br><span class=\"line\">  push(vm, object);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\">Object* <span class=\"title\">pushPair</span><span class=\"params\">(VM* vm)</span> </span>&#123;</span><br><span class=\"line\">  Object* object = newObject(vm, OBJ_PAIR);</span><br><span class=\"line\">  object-&gt;tail = pop(vm);</span><br><span class=\"line\">  object-&gt;head = pop(vm);</span><br><span class=\"line\"></span><br><span class=\"line\">  push(vm, object);</span><br><span class=\"line\">  <span class=\"keyword\">return</span> object;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>VM </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">sObject</span> &#123;</span></span><br><span class=\"line\">  <span class=\"keyword\">unsigned</span> <span class=\"keyword\">char</span> marked;</span><br><span class=\"line\">  <span class=\"comment\">/* Previous stuff... */</span></span><br><span class=\"line\">&#125; Object;</span><br></pre></td></tr></table></figure>\n<p>- VM PAIR  PAIR </p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">mark</span><span class=\"params\">(Object* object)</span> </span>&#123;</span><br><span class=\"line\">  object-&gt;marked = <span class=\"number\">1</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">if</span> (object-&gt;type == OBJ_PAIR) &#123;</span><br><span class=\"line\">    mark(object-&gt;head);</span><br><span class=\"line\">    mark(object-&gt;tail);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">markAll</span><span class=\"params\">(VM* vm)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; vm-&gt;stackSize; i++) &#123;</span><br><span class=\"line\">    mark(vm-&gt;<span class=\"built_in\">stack</span>[i]);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p> mark </p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">mark</span><span class=\"params\">(Object* object)</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"comment\">/* If already marked, we're done. Check this first</span></span><br><span class=\"line\"><span class=\"comment\">     to avoid recursing on cycles in the object graph. */</span></span><br><span class=\"line\">  <span class=\"keyword\">if</span> (object-&gt;marked) <span class=\"keyword\">return</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">  object-&gt;marked = <span class=\"number\">1</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">if</span> (object-&gt;type == OBJ_PAIR) &#123;</span><br><span class=\"line\">    mark(object-&gt;head);</span><br><span class=\"line\">    mark(object-&gt;tail);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>-</p>\n<p> C </p>\n<p> VM </p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">sObject</span> &#123;</span></span><br><span class=\"line\">  <span class=\"comment\">/* The next object in the list of all objects. */</span></span><br><span class=\"line\">  <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">sObject</span>* <span class=\"title\">next</span>;</span></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">/* Previous stuff... */</span></span><br><span class=\"line\">&#125; Object;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> &#123;</span></span><br><span class=\"line\">  <span class=\"comment\">/* The first object in the list of all objects. */</span></span><br><span class=\"line\">  Object* firstObject;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">/* Previous stuff... */</span></span><br><span class=\"line\">&#125; VM;</span><br></pre></td></tr></table></figure>\n<p> VM  new  firstObject  NULL</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">Object* <span class=\"title\">newObject</span><span class=\"params\">(VM* vm, ObjectType type)</span> </span>&#123;</span><br><span class=\"line\">  Object* object = <span class=\"built_in\">malloc</span>(<span class=\"keyword\">sizeof</span>(Object));</span><br><span class=\"line\">  object-&gt;type = type;</span><br><span class=\"line\">  object-&gt;marked = <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">/* Insert it into the list of allocated objects. */</span></span><br><span class=\"line\">  object-&gt;next = vm-&gt;firstObject;</span><br><span class=\"line\">  vm-&gt;firstObject = object;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">return</span> object;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p> Linux </p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">sweep</span><span class=\"params\">(VM* vm)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">  Object** object = &amp;vm-&gt;firstObject;</span><br><span class=\"line\">  <span class=\"keyword\">while</span> (*object) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (!(*object)-&gt;marked) &#123;</span><br><span class=\"line\">      <span class=\"comment\">/* This object wasn't reached, so remove it from the list</span></span><br><span class=\"line\"><span class=\"comment\">         and free it. */</span></span><br><span class=\"line\">      Object* unreached = *object;</span><br><span class=\"line\"></span><br><span class=\"line\">      *object = unreached-&gt;next;</span><br><span class=\"line\">      <span class=\"built_in\">free</span>(unreached);</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">      <span class=\"comment\">/* This object was reached, so unmark it (for the next GC)</span></span><br><span class=\"line\"><span class=\"comment\">         and move on to the next. */</span></span><br><span class=\"line\">      (*object)-&gt;marked = <span class=\"number\">0</span>;</span><br><span class=\"line\">      object = &amp;(*object)-&gt;next;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p></p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">gc</span><span class=\"params\">(VM* vm)</span> </span>&#123;</span><br><span class=\"line\">  markAll(vm);</span><br><span class=\"line\">  sweep(vm);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> VM </p>\n<p></p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> &#123;</span></span><br><span class=\"line\">  <span class=\"comment\">/* The total number of currently allocated objects. */</span></span><br><span class=\"line\">  <span class=\"keyword\">int</span> numObjects;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">/* The number of objects required to trigger a GC. */</span></span><br><span class=\"line\">  <span class=\"keyword\">int</span> maxObjects;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">/* Previous stuff... */</span></span><br><span class=\"line\">&#125; VM;</span><br><span class=\"line\"></span><br><span class=\"line\">--------------------</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\">VM* <span class=\"title\">newVM</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"comment\">/* Previous stuff... */</span></span><br><span class=\"line\"></span><br><span class=\"line\">  vm-&gt;numObjects = <span class=\"number\">0</span>;</span><br><span class=\"line\">  vm-&gt;maxObjects = INITIAL_GC_THRESHOLD;</span><br><span class=\"line\">  <span class=\"keyword\">return</span> vm;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">--------------------</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\">Object* <span class=\"title\">newObject</span><span class=\"params\">(VM* vm, ObjectType type)</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (vm-&gt;numObjects == vm-&gt;maxObjects) gc(vm);</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">/* Create object... */</span></span><br><span class=\"line\"></span><br><span class=\"line\">  vm-&gt;numObjects++;</span><br><span class=\"line\">  <span class=\"keyword\">return</span> object;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">--------------------</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">gc</span><span class=\"params\">(VM* vm)</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">int</span> numObjects = vm-&gt;numObjects;</span><br><span class=\"line\"></span><br><span class=\"line\">  markAll(vm);</span><br><span class=\"line\">  sweep(vm);</span><br><span class=\"line\"></span><br><span class=\"line\">  vm-&gt;maxObjects = vm-&gt;numObjects * <span class=\"number\">2</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p> 2  8  8 16 </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> OK </p>\n<p> GitHub<a href=\"https://github.com/SakigamiYang/baby-gc\" target=\"_blank\" rel=\"noopener\">SakigamiYang/baby-gc</a></p>\n"},{"title":"","date":"2017-09-13T03:14:14.000Z","description":"","_content":"\n#  A $\\kappa({A})$\n\n\n\n $AX = b$  $x$ \n$$\n\\begin{aligned}\n& \\quad (A+\\Delta A)(x + \\Delta x) = b+\\Delta b \\\\\n& \\Rightarrow \\Delta x = A^{-1}[\\Delta b - (\\Delta A) x - (\\Delta A)(\\Delta x)] \\\\\n& \\Rightarrow \\lVert \\Delta x \\rVert \\le \\lVert A^{-1} \\rVert (\\lVert \\Delta b \\rVert + \\lVert \\Delta A \\rVert \\lVert x \\rVert + \\lVert \\Delta A\\rVert \\lVert \\Delta x\\rVert ) \\\\\n& \\Rightarrow (1-\\lVert A^{-1} \\rVert \\lVert \\Delta A \\rVert) \\lVert \\Delta x \\rVert \\le \\lVert A^{-1} \\rVert (\\lVert \\Delta b \\rVert + \\lVert \\Delta A \\rVert \\lVert x \\rVert) \\\\\n& \\Rightarrow \\lVert \\Delta x \\rVert \\le \\frac{\\lVert A^{-1} \\rVert}{1-\\lVert A^{-1} \\rVert \\lVert \\Delta A \\rVert} \\left( \\lVert \\Delta b \\rVert + \\lVert \\Delta A \\rVert \\lVert x \\rVert \\right) \\qquad (\\because \\, \\lVert A^{-1} \\rVert \\lVert \\Delta A \\rVert \\le 1) \\\\\n& \\Rightarrow \\frac {\\lVert \\Delta x \\rVert}{\\lVert x \\rVert} \\le \\frac{\\lVert A^{-1} \\rVert \\lVert A \\rVert}{1-\\lVert A^{-1} \\rVert \\lVert \\Delta A \\rVert} \\left( \\frac{\\lVert \\Delta b \\rVert}{\\lVert b \\rVert} + \\frac{\\lVert \\Delta A \\rVert}{\\lVert A \\rVert} \\right) \\qquad (\\because \\, \\lVert b \\rVert \\le \\lVert A \\rVert \\lVert x \\rVert)\n\\end{aligned}\n$$\n $\\Delta A = 0$ \n$$\n\\frac{\\lVert \\Delta x \\rVert}{\\lVert x \\rVert} \\le \\kappa(A) \\frac{\\lVert \\Delta b \\rVert}{\\lVert b \\rVert}\n$$\n $\\Delta b = 0$ \n$$\n\\frac{\\lVert \\Delta x \\rVert}{\\lVert x \\rVert} \\le \\kappa(A) \\frac{\\lVert \\Delta A \\rVert}{\\lVert A \\rVert}(1+o(\\lVert A^{-1} \\rVert))\n$$\n 2-\n$$\n\\kappa_{2}(A) = \\lVert A^{-1} \\rVert _{2} \\lVert A \\rVert _{2} = \\sqrt{\\frac{\\lambda _{max}}{\\lambda _{min}}}\n$$\n$\\lambda_{max}$  $\\lambda_{min}$  $A^{\\mathsf{H}}A$ \n\n\n","source":"_posts/norm-regularization-appendix.md","raw":"---\ntitle: \ndate: 2017-09-13 11:14:14\ncategories: ML\ntags:\n      - Norm regularization\n      - Matrix theory\ndescription: \n---\n\n#  A $\\kappa({A})$\n\n\n\n $AX = b$  $x$ \n$$\n\\begin{aligned}\n& \\quad (A+\\Delta A)(x + \\Delta x) = b+\\Delta b \\\\\n& \\Rightarrow \\Delta x = A^{-1}[\\Delta b - (\\Delta A) x - (\\Delta A)(\\Delta x)] \\\\\n& \\Rightarrow \\lVert \\Delta x \\rVert \\le \\lVert A^{-1} \\rVert (\\lVert \\Delta b \\rVert + \\lVert \\Delta A \\rVert \\lVert x \\rVert + \\lVert \\Delta A\\rVert \\lVert \\Delta x\\rVert ) \\\\\n& \\Rightarrow (1-\\lVert A^{-1} \\rVert \\lVert \\Delta A \\rVert) \\lVert \\Delta x \\rVert \\le \\lVert A^{-1} \\rVert (\\lVert \\Delta b \\rVert + \\lVert \\Delta A \\rVert \\lVert x \\rVert) \\\\\n& \\Rightarrow \\lVert \\Delta x \\rVert \\le \\frac{\\lVert A^{-1} \\rVert}{1-\\lVert A^{-1} \\rVert \\lVert \\Delta A \\rVert} \\left( \\lVert \\Delta b \\rVert + \\lVert \\Delta A \\rVert \\lVert x \\rVert \\right) \\qquad (\\because \\, \\lVert A^{-1} \\rVert \\lVert \\Delta A \\rVert \\le 1) \\\\\n& \\Rightarrow \\frac {\\lVert \\Delta x \\rVert}{\\lVert x \\rVert} \\le \\frac{\\lVert A^{-1} \\rVert \\lVert A \\rVert}{1-\\lVert A^{-1} \\rVert \\lVert \\Delta A \\rVert} \\left( \\frac{\\lVert \\Delta b \\rVert}{\\lVert b \\rVert} + \\frac{\\lVert \\Delta A \\rVert}{\\lVert A \\rVert} \\right) \\qquad (\\because \\, \\lVert b \\rVert \\le \\lVert A \\rVert \\lVert x \\rVert)\n\\end{aligned}\n$$\n $\\Delta A = 0$ \n$$\n\\frac{\\lVert \\Delta x \\rVert}{\\lVert x \\rVert} \\le \\kappa(A) \\frac{\\lVert \\Delta b \\rVert}{\\lVert b \\rVert}\n$$\n $\\Delta b = 0$ \n$$\n\\frac{\\lVert \\Delta x \\rVert}{\\lVert x \\rVert} \\le \\kappa(A) \\frac{\\lVert \\Delta A \\rVert}{\\lVert A \\rVert}(1+o(\\lVert A^{-1} \\rVert))\n$$\n 2-\n$$\n\\kappa_{2}(A) = \\lVert A^{-1} \\rVert _{2} \\lVert A \\rVert _{2} = \\sqrt{\\frac{\\lambda _{max}}{\\lambda _{min}}}\n$$\n$\\lambda_{max}$  $\\lambda_{min}$  $A^{\\mathsf{H}}A$ \n\n\n","slug":"norm-regularization-appendix","published":1,"updated":"2021-07-19T13:03:31.697Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckwrktivg000gqxt66mnw2k4u","content":"<h1 id=\"-A-kappa-A\"><a href=\"#-A-kappa-A\" class=\"headerlink\" title=\" A $\\kappa({A})$\"></a> A $\\kappa({A})$</h1><p></p>\n<p> $AX = b$  $x$ </p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n& \\quad (A+\\Delta A)(x + \\Delta x) = b+\\Delta b \\\\\n& \\Rightarrow \\Delta x = A^{-1}[\\Delta b - (\\Delta A) x - (\\Delta A)(\\Delta x)] \\\\\n& \\Rightarrow \\lVert \\Delta x \\rVert \\le \\lVert A^{-1} \\rVert (\\lVert \\Delta b \\rVert + \\lVert \\Delta A \\rVert \\lVert x \\rVert + \\lVert \\Delta A\\rVert \\lVert \\Delta x\\rVert ) \\\\\n& \\Rightarrow (1-\\lVert A^{-1} \\rVert \\lVert \\Delta A \\rVert) \\lVert \\Delta x \\rVert \\le \\lVert A^{-1} \\rVert (\\lVert \\Delta b \\rVert + \\lVert \\Delta A \\rVert \\lVert x \\rVert) \\\\\n& \\Rightarrow \\lVert \\Delta x \\rVert \\le \\frac{\\lVert A^{-1} \\rVert}{1-\\lVert A^{-1} \\rVert \\lVert \\Delta A \\rVert} \\left( \\lVert \\Delta b \\rVert + \\lVert \\Delta A \\rVert \\lVert x \\rVert \\right) \\qquad (\\because \\, \\lVert A^{-1} \\rVert \\lVert \\Delta A \\rVert \\le 1) \\\\\n& \\Rightarrow \\frac {\\lVert \\Delta x \\rVert}{\\lVert x \\rVert} \\le \\frac{\\lVert A^{-1} \\rVert \\lVert A \\rVert}{1-\\lVert A^{-1} \\rVert \\lVert \\Delta A \\rVert} \\left( \\frac{\\lVert \\Delta b \\rVert}{\\lVert b \\rVert} + \\frac{\\lVert \\Delta A \\rVert}{\\lVert A \\rVert} \\right) \\qquad (\\because \\, \\lVert b \\rVert \\le \\lVert A \\rVert \\lVert x \\rVert)\n\\end{aligned}</script><p> $\\Delta A = 0$ </p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\lVert \\Delta x \\rVert}{\\lVert x \\rVert} \\le \\kappa(A) \\frac{\\lVert \\Delta b \\rVert}{\\lVert b \\rVert}</script><p> $\\Delta b = 0$ </p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\lVert \\Delta x \\rVert}{\\lVert x \\rVert} \\le \\kappa(A) \\frac{\\lVert \\Delta A \\rVert}{\\lVert A \\rVert}(1+o(\\lVert A^{-1} \\rVert))</script><p> 2-</p>\n<script type=\"math/tex; mode=display\">\n\\kappa_{2}(A) = \\lVert A^{-1} \\rVert _{2} \\lVert A \\rVert _{2} = \\sqrt{\\frac{\\lambda _{max}}{\\lambda _{min}}}</script><p>$\\lambda_{max}$  $\\lambda_{min}$  $A^{\\mathsf{H}}A$ <br></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"-A-kappa-A\"><a href=\"#-A-kappa-A\" class=\"headerlink\" title=\" A $\\kappa({A})$\"></a> A $\\kappa({A})$</h1><p></p>\n<p> $AX = b$  $x$ </p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n& \\quad (A+\\Delta A)(x + \\Delta x) = b+\\Delta b \\\\\n& \\Rightarrow \\Delta x = A^{-1}[\\Delta b - (\\Delta A) x - (\\Delta A)(\\Delta x)] \\\\\n& \\Rightarrow \\lVert \\Delta x \\rVert \\le \\lVert A^{-1} \\rVert (\\lVert \\Delta b \\rVert + \\lVert \\Delta A \\rVert \\lVert x \\rVert + \\lVert \\Delta A\\rVert \\lVert \\Delta x\\rVert ) \\\\\n& \\Rightarrow (1-\\lVert A^{-1} \\rVert \\lVert \\Delta A \\rVert) \\lVert \\Delta x \\rVert \\le \\lVert A^{-1} \\rVert (\\lVert \\Delta b \\rVert + \\lVert \\Delta A \\rVert \\lVert x \\rVert) \\\\\n& \\Rightarrow \\lVert \\Delta x \\rVert \\le \\frac{\\lVert A^{-1} \\rVert}{1-\\lVert A^{-1} \\rVert \\lVert \\Delta A \\rVert} \\left( \\lVert \\Delta b \\rVert + \\lVert \\Delta A \\rVert \\lVert x \\rVert \\right) \\qquad (\\because \\, \\lVert A^{-1} \\rVert \\lVert \\Delta A \\rVert \\le 1) \\\\\n& \\Rightarrow \\frac {\\lVert \\Delta x \\rVert}{\\lVert x \\rVert} \\le \\frac{\\lVert A^{-1} \\rVert \\lVert A \\rVert}{1-\\lVert A^{-1} \\rVert \\lVert \\Delta A \\rVert} \\left( \\frac{\\lVert \\Delta b \\rVert}{\\lVert b \\rVert} + \\frac{\\lVert \\Delta A \\rVert}{\\lVert A \\rVert} \\right) \\qquad (\\because \\, \\lVert b \\rVert \\le \\lVert A \\rVert \\lVert x \\rVert)\n\\end{aligned}</script><p> $\\Delta A = 0$ </p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\lVert \\Delta x \\rVert}{\\lVert x \\rVert} \\le \\kappa(A) \\frac{\\lVert \\Delta b \\rVert}{\\lVert b \\rVert}</script><p> $\\Delta b = 0$ </p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\lVert \\Delta x \\rVert}{\\lVert x \\rVert} \\le \\kappa(A) \\frac{\\lVert \\Delta A \\rVert}{\\lVert A \\rVert}(1+o(\\lVert A^{-1} \\rVert))</script><p> 2-</p>\n<script type=\"math/tex; mode=display\">\n\\kappa_{2}(A) = \\lVert A^{-1} \\rVert _{2} \\lVert A \\rVert _{2} = \\sqrt{\\frac{\\lambda _{max}}{\\lambda _{min}}}</script><p>$\\lambda_{max}$  $\\lambda_{min}$  $A^{\\mathsf{H}}A$ <br></p>\n"},{"title":"","date":"2017-08-13T09:42:38.000Z","description":"","_content":"\n**functional analysis**function space\n\n\n\n{% asset_img linear_space.png Linear space %}\n\n# \n\nlinear spacebasis\n $\\lbrace 1, x, x^{2} \\rbrace$  $f(x)$  $f(x) = \\alpha_{1} \\cdot 1 + \\alpha_{2} \\cdot x + \\alpha_{3} \\cdot x^{2}$  $f(x)$  $\\lbrace \\alpha_{1}, \\alpha_{2}, \\alpha_{3} \\rbrace$  $\\lbrace x, y, z \\rbrace$ \n $\\lbrace 1, x+1, (x+1)^{2} \\rbrace$  $n$  $n$ \northogonal basis 0\n\n# \n\nmetric linear space**metric**\n\n\n1.  $d(x, y) \\ge 0; d(x, y) = 0 \\Leftrightarrow x = y$ \n2.  $d(x, y) = d(y, x)$ \n3. $d(x, z) + d(z, y) \\ge d(x, y)$ \n\n# \n\nnormed linear space**norm**\n\n\n1. $\\lVert x \\rVert \\ge 0$ \n2. $ \\lVert \\alpha x \\rVert = |\\alpha| \\lVert x \\rVert $ \n3. $\\lVert x \\rVert + \\lVert y \\rVert \\ge \\lVert x+y \\rVert$ \n\n $d(x, y) = \\lVert x - y \\rVert$ \n\n# \n\nBanach space\ncompletenessCauchy sequenceconvergence\n\n# \n\ninner product linear space**inner product**\n**scalar product****dot product**\n\n\n\n1. $\\langle x, y \\rangle = \\langle y, x \\rangle$ \n2.  $\\langle x, y \\rangle + \\langle x, z \\rangle = \\langle x, y + z \\rangle$  $\\langle \\alpha x, y \\rangle = \\alpha \\langle x, y \\rangle$ \n3. $\\langle x, x \\rangle \\ge 0$ \n\n $\\lVert x \\rVert^{2} = \\langle x, x \\rangle$ \n\n# \n\nEuclidean space\n\n# \n\nHilbert space\n\n# \n\n1. Taylor series $\\lbrace x^{i} \\rbrace_{0}^{\\infty}$ \n2. Fourier series $\\lbrace 1, \\cos x, \\sin x, \\cos 2x, \\sin 2x, \\cdots \\rbrace$ \n\n# \n\n\n\n $n \\times n$ \n$$\nAx=\\lambda x\n$$\n $n$ \n\n\n\n $f(x)$  $K(x, y)$ \n\n1. $\\forall f \\rightarrow \\iint f(x)K(x, y)f(y) \\,\\mathrm{d}x\\,\\mathrm{d}y \\ge 0$ \n2. $K(x, y) = K(y, x)$ \n\n**kernel function**\n\n $\\lambda$  $\\psi(x)$ \n$$\n\\int K(x, y)\\psi(x) \\mathrm{d}x = \\lambda \\psi (y)\n$$\n $\\lambda_{1}$  $\\lambda_{2}$  $\\psi_{1}(x)$  $\\psi_{2}(x)$ \n$$\n\\begin{aligned}\n\\int \\lambda_1 \\psi_{1}(x) \\psi_{2}(x) \\mathrm{d}x &= \\iint K(y, x) \\psi_{1}(y) \\mathrm{d}y\\,\\psi_{2}(x)\\mathrm{d}x \\\\\n&= \\iint K(y, x) \\psi_{2}(x) \\mathrm{d}x\\,\\psi_{1}(y)\\mathrm{d}y \\\\\n&= \\int \\lambda_2 \\psi_{2}(y) \\psi_{1}(y) \\mathrm{d}y \\\\\n&= \\int \\lambda_2 \\psi_{2}(x) \\psi_{1}(x) \\mathrm{d}x\n\\end{aligned}\n$$\n\n$$\n\\langle \\psi_{1}, \\psi_{2} \\rangle = \\int \\psi_{1}(x) \\psi_{2}(x) \\mathrm{d}x = 0\n$$\n $K$ $\\lbrace \\lambda_{i} \\rbrace_{i=1}^{\\infty}$  $\\lbrace \\psi_{i} \\rbrace_{i=1}^{\\infty}$ \n\n# \n\n $\\lbrace \\sqrt{\\lambda_{i}}\\psi_{i} \\rbrace_{i=1}^{\\infty}$  $\\mathcal{H}$ \n$$\nf = \\sum_{i=1}^{\\infty} f_{i}\\sqrt{\\lambda_{i}}\\psi_{i}\n$$\n $f$  $\\mathcal{H}$  $f = (f_{1}, f_{2}, \\cdots)_{\\mathcal{H}}^{\\mathsf{T}}$ \n $g = (g_{1}, g_{2}, \\cdots)_{\\mathcal{H}}^{\\mathsf{T}}$ \n$$\n\\langle f, g \\rangle_{\\mathcal{H}} = \\sum_{i=1}^{\\infty}f_{i}g_{i}\n$$\n\n$$\nK(x, \\cdot) = \\sum_{i=0}^{\\infty} \\lambda_{i}\\psi_{i}(x)\\psi_{i}(\\cdot)\n$$\n $\\psi_{i}$ $K(x, \\cdot) = \\sum_{i=0}^{\\infty} \\lambda_{i}\\psi_{i}(x)\\psi_{i}$ \n $K$ \n$$\nK(x, \\cdot) = (\\sqrt{\\lambda_{1}}\\psi_{1}(x), \\sqrt{\\lambda_{2}}\\psi_{2}(x),\\cdots)_{\\mathcal{H}}^{\\mathsf{T}}\n$$\n\n$$\n\\langle K(x, \\cdot), K(y, \\cdot) \\rangle_{\\mathcal{H}} = \\sum_{i=0}^{\\infty} \\lambda_{i} \\psi_{i}(x) \\psi_{i}(y) = K(x, y)\n$$\nreproducing $\\mathcal{H}$ reproducing kernel Hilbert spaceRKHS\n\n\n\n\n$$\n\\Phi(x) = K(x, \\cdot) = (\\sqrt{\\lambda_{1}}\\psi_{1}(x), \\sqrt{\\lambda_{2}}\\psi_{2}(x),\\cdots)_{\\mathcal{H}}^{\\mathsf{T}}\n$$\n\n$$\n\\langle \\Phi(x), \\Phi(y) \\rangle_{\\mathcal{H}} = \\langle K(x, \\cdot), K(y, \\cdot) \\rangle_{\\mathcal{H}} = K(x, y)\n$$\n $K$  $\\Phi$  $\\mathcal{H}$ \n$$\n\\langle \\Phi(x), \\Phi(y) \\rangle_{\\mathcal{H}} = K(x, y)\n$$\nkernel trick\n\n** RKHS **\n\n#  SVM \n\n\n\n $x$  $\\Phi(x)$ \n\n $\\langle x_{i}, x_{j} \\rangle$  $x$  $\\Phi(x)$  $\\langle \\Phi(x_{i}), \\Phi(x_{j}) \\rangle$ \n $K$  $K$  $\\Phi$  $\\langle \\Phi(x_{i}), \\Phi(x_{j}) \\rangle$  $K(x_{i}, x_{j})$  $K$  $x$ \n\n $\\lVert x - y \\rVert^{2}$ ","source":"_posts/about-kernel-02.md","raw":"---\ntitle: \ndate: 2017-08-13 17:42:38\ncategories: ML\ntags:\n     - Kernel\n     - RKHS\n     - Functional analysis\ndescription: \n---\n\n**functional analysis**function space\n\n\n\n{% asset_img linear_space.png Linear space %}\n\n# \n\nlinear spacebasis\n $\\lbrace 1, x, x^{2} \\rbrace$  $f(x)$  $f(x) = \\alpha_{1} \\cdot 1 + \\alpha_{2} \\cdot x + \\alpha_{3} \\cdot x^{2}$  $f(x)$  $\\lbrace \\alpha_{1}, \\alpha_{2}, \\alpha_{3} \\rbrace$  $\\lbrace x, y, z \\rbrace$ \n $\\lbrace 1, x+1, (x+1)^{2} \\rbrace$  $n$  $n$ \northogonal basis 0\n\n# \n\nmetric linear space**metric**\n\n\n1.  $d(x, y) \\ge 0; d(x, y) = 0 \\Leftrightarrow x = y$ \n2.  $d(x, y) = d(y, x)$ \n3. $d(x, z) + d(z, y) \\ge d(x, y)$ \n\n# \n\nnormed linear space**norm**\n\n\n1. $\\lVert x \\rVert \\ge 0$ \n2. $ \\lVert \\alpha x \\rVert = |\\alpha| \\lVert x \\rVert $ \n3. $\\lVert x \\rVert + \\lVert y \\rVert \\ge \\lVert x+y \\rVert$ \n\n $d(x, y) = \\lVert x - y \\rVert$ \n\n# \n\nBanach space\ncompletenessCauchy sequenceconvergence\n\n# \n\ninner product linear space**inner product**\n**scalar product****dot product**\n\n\n\n1. $\\langle x, y \\rangle = \\langle y, x \\rangle$ \n2.  $\\langle x, y \\rangle + \\langle x, z \\rangle = \\langle x, y + z \\rangle$  $\\langle \\alpha x, y \\rangle = \\alpha \\langle x, y \\rangle$ \n3. $\\langle x, x \\rangle \\ge 0$ \n\n $\\lVert x \\rVert^{2} = \\langle x, x \\rangle$ \n\n# \n\nEuclidean space\n\n# \n\nHilbert space\n\n# \n\n1. Taylor series $\\lbrace x^{i} \\rbrace_{0}^{\\infty}$ \n2. Fourier series $\\lbrace 1, \\cos x, \\sin x, \\cos 2x, \\sin 2x, \\cdots \\rbrace$ \n\n# \n\n\n\n $n \\times n$ \n$$\nAx=\\lambda x\n$$\n $n$ \n\n\n\n $f(x)$  $K(x, y)$ \n\n1. $\\forall f \\rightarrow \\iint f(x)K(x, y)f(y) \\,\\mathrm{d}x\\,\\mathrm{d}y \\ge 0$ \n2. $K(x, y) = K(y, x)$ \n\n**kernel function**\n\n $\\lambda$  $\\psi(x)$ \n$$\n\\int K(x, y)\\psi(x) \\mathrm{d}x = \\lambda \\psi (y)\n$$\n $\\lambda_{1}$  $\\lambda_{2}$  $\\psi_{1}(x)$  $\\psi_{2}(x)$ \n$$\n\\begin{aligned}\n\\int \\lambda_1 \\psi_{1}(x) \\psi_{2}(x) \\mathrm{d}x &= \\iint K(y, x) \\psi_{1}(y) \\mathrm{d}y\\,\\psi_{2}(x)\\mathrm{d}x \\\\\n&= \\iint K(y, x) \\psi_{2}(x) \\mathrm{d}x\\,\\psi_{1}(y)\\mathrm{d}y \\\\\n&= \\int \\lambda_2 \\psi_{2}(y) \\psi_{1}(y) \\mathrm{d}y \\\\\n&= \\int \\lambda_2 \\psi_{2}(x) \\psi_{1}(x) \\mathrm{d}x\n\\end{aligned}\n$$\n\n$$\n\\langle \\psi_{1}, \\psi_{2} \\rangle = \\int \\psi_{1}(x) \\psi_{2}(x) \\mathrm{d}x = 0\n$$\n $K$ $\\lbrace \\lambda_{i} \\rbrace_{i=1}^{\\infty}$  $\\lbrace \\psi_{i} \\rbrace_{i=1}^{\\infty}$ \n\n# \n\n $\\lbrace \\sqrt{\\lambda_{i}}\\psi_{i} \\rbrace_{i=1}^{\\infty}$  $\\mathcal{H}$ \n$$\nf = \\sum_{i=1}^{\\infty} f_{i}\\sqrt{\\lambda_{i}}\\psi_{i}\n$$\n $f$  $\\mathcal{H}$  $f = (f_{1}, f_{2}, \\cdots)_{\\mathcal{H}}^{\\mathsf{T}}$ \n $g = (g_{1}, g_{2}, \\cdots)_{\\mathcal{H}}^{\\mathsf{T}}$ \n$$\n\\langle f, g \\rangle_{\\mathcal{H}} = \\sum_{i=1}^{\\infty}f_{i}g_{i}\n$$\n\n$$\nK(x, \\cdot) = \\sum_{i=0}^{\\infty} \\lambda_{i}\\psi_{i}(x)\\psi_{i}(\\cdot)\n$$\n $\\psi_{i}$ $K(x, \\cdot) = \\sum_{i=0}^{\\infty} \\lambda_{i}\\psi_{i}(x)\\psi_{i}$ \n $K$ \n$$\nK(x, \\cdot) = (\\sqrt{\\lambda_{1}}\\psi_{1}(x), \\sqrt{\\lambda_{2}}\\psi_{2}(x),\\cdots)_{\\mathcal{H}}^{\\mathsf{T}}\n$$\n\n$$\n\\langle K(x, \\cdot), K(y, \\cdot) \\rangle_{\\mathcal{H}} = \\sum_{i=0}^{\\infty} \\lambda_{i} \\psi_{i}(x) \\psi_{i}(y) = K(x, y)\n$$\nreproducing $\\mathcal{H}$ reproducing kernel Hilbert spaceRKHS\n\n\n\n\n$$\n\\Phi(x) = K(x, \\cdot) = (\\sqrt{\\lambda_{1}}\\psi_{1}(x), \\sqrt{\\lambda_{2}}\\psi_{2}(x),\\cdots)_{\\mathcal{H}}^{\\mathsf{T}}\n$$\n\n$$\n\\langle \\Phi(x), \\Phi(y) \\rangle_{\\mathcal{H}} = \\langle K(x, \\cdot), K(y, \\cdot) \\rangle_{\\mathcal{H}} = K(x, y)\n$$\n $K$  $\\Phi$  $\\mathcal{H}$ \n$$\n\\langle \\Phi(x), \\Phi(y) \\rangle_{\\mathcal{H}} = K(x, y)\n$$\nkernel trick\n\n** RKHS **\n\n#  SVM \n\n\n\n $x$  $\\Phi(x)$ \n\n $\\langle x_{i}, x_{j} \\rangle$  $x$  $\\Phi(x)$  $\\langle \\Phi(x_{i}), \\Phi(x_{j}) \\rangle$ \n $K$  $K$  $\\Phi$  $\\langle \\Phi(x_{i}), \\Phi(x_{j}) \\rangle$  $K(x_{i}, x_{j})$  $K$  $x$ \n\n $\\lVert x - y \\rVert^{2}$ ","slug":"about-kernel-02","published":1,"updated":"2021-07-21T12:39:59.772Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckwrktivg000hqxt6ns6fqudw","content":"<p><strong>functional analysis</strong>function space</p>\n<p></p>\n<img src=\"/2017/08/13/about-kernel-02/linear_space.png\" title=\"Linear space\">\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>linear spacebasis<br> $\\lbrace 1, x, x^{2} \\rbrace$  $f(x)$  $f(x) = \\alpha_{1} \\cdot 1 + \\alpha_{2} \\cdot x + \\alpha_{3} \\cdot x^{2}$  $f(x)$  $\\lbrace \\alpha_{1}, \\alpha_{2}, \\alpha_{3} \\rbrace$  $\\lbrace x, y, z \\rbrace$ <br> $\\lbrace 1, x+1, (x+1)^{2} \\rbrace$  $n$  $n$ <br>orthogonal basis 0</p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>metric linear space<strong>metric</strong><br></p>\n<ol>\n<li> $d(x, y) \\ge 0; d(x, y) = 0 \\Leftrightarrow x = y$ </li>\n<li> $d(x, y) = d(y, x)$ </li>\n<li>$d(x, z) + d(z, y) \\ge d(x, y)$ </li>\n</ol>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>normed linear space<strong>norm</strong><br></p>\n<ol>\n<li>$\\lVert x \\rVert \\ge 0$ </li>\n<li>$ \\lVert \\alpha x \\rVert = |\\alpha| \\lVert x \\rVert $ </li>\n<li>$\\lVert x \\rVert + \\lVert y \\rVert \\ge \\lVert x+y \\rVert$ </li>\n</ol>\n<p> $d(x, y) = \\lVert x - y \\rVert$ </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>Banach space<br>completenessCauchy sequenceconvergence</p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>inner product linear space<strong>inner product</strong><br><strong>scalar product</strong><strong>dot product</strong><br><br></p>\n<ol>\n<li>$\\langle x, y \\rangle = \\langle y, x \\rangle$ </li>\n<li> $\\langle x, y \\rangle + \\langle x, z \\rangle = \\langle x, y + z \\rangle$  $\\langle \\alpha x, y \\rangle = \\alpha \\langle x, y \\rangle$ </li>\n<li>$\\langle x, x \\rangle \\ge 0$ </li>\n</ol>\n<p> $\\lVert x \\rVert^{2} = \\langle x, x \\rangle$ </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>Euclidean space</p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>Hilbert space</p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><ol>\n<li>Taylor series $\\lbrace x^{i} \\rbrace_{0}^{\\infty}$ </li>\n<li>Fourier series $\\lbrace 1, \\cos x, \\sin x, \\cos 2x, \\sin 2x, \\cdots \\rbrace$ </li>\n</ol>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n<p> $n \\times n$ </p>\n<script type=\"math/tex; mode=display\">\nAx=\\lambda x</script><p> $n$ </p>\n<p></p>\n<p> $f(x)$  $K(x, y)$ </p>\n<ol>\n<li>$\\forall f \\rightarrow \\iint f(x)K(x, y)f(y) \\,\\mathrm{d}x\\,\\mathrm{d}y \\ge 0$ </li>\n<li>$K(x, y) = K(y, x)$ </li>\n</ol>\n<p><strong>kernel function</strong></p>\n<p> $\\lambda$  $\\psi(x)$ </p>\n<script type=\"math/tex; mode=display\">\n\\int K(x, y)\\psi(x) \\mathrm{d}x = \\lambda \\psi (y)</script><p> $\\lambda_{1}$  $\\lambda_{2}$  $\\psi_{1}(x)$  $\\psi_{2}(x)$ </p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\int \\lambda_1 \\psi_{1}(x) \\psi_{2}(x) \\mathrm{d}x &= \\iint K(y, x) \\psi_{1}(y) \\mathrm{d}y\\,\\psi_{2}(x)\\mathrm{d}x \\\\\n&= \\iint K(y, x) \\psi_{2}(x) \\mathrm{d}x\\,\\psi_{1}(y)\\mathrm{d}y \\\\\n&= \\int \\lambda_2 \\psi_{2}(y) \\psi_{1}(y) \\mathrm{d}y \\\\\n&= \\int \\lambda_2 \\psi_{2}(x) \\psi_{1}(x) \\mathrm{d}x\n\\end{aligned}</script><p></p>\n<script type=\"math/tex; mode=display\">\n\\langle \\psi_{1}, \\psi_{2} \\rangle = \\int \\psi_{1}(x) \\psi_{2}(x) \\mathrm{d}x = 0</script><p> $K$ $\\lbrace \\lambda_{i} \\rbrace_{i=1}^{\\infty}$  $\\lbrace \\psi_{i} \\rbrace_{i=1}^{\\infty}$ </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> $\\lbrace \\sqrt{\\lambda_{i}}\\psi_{i} \\rbrace_{i=1}^{\\infty}$  $\\mathcal{H}$ </p>\n<script type=\"math/tex; mode=display\">\nf = \\sum_{i=1}^{\\infty} f_{i}\\sqrt{\\lambda_{i}}\\psi_{i}</script><p> $f$  $\\mathcal{H}$  $f = (f_{1}, f_{2}, \\cdots)_{\\mathcal{H}}^{\\mathsf{T}}$ <br> $g = (g_{1}, g_{2}, \\cdots)_{\\mathcal{H}}^{\\mathsf{T}}$ </p>\n<script type=\"math/tex; mode=display\">\n\\langle f, g \\rangle_{\\mathcal{H}} = \\sum_{i=1}^{\\infty}f_{i}g_{i}</script><p></p>\n<script type=\"math/tex; mode=display\">\nK(x, \\cdot) = \\sum_{i=0}^{\\infty} \\lambda_{i}\\psi_{i}(x)\\psi_{i}(\\cdot)</script><p> $\\psi_{i}$ $K(x, \\cdot) = \\sum_{i=0}^{\\infty} \\lambda_{i}\\psi_{i}(x)\\psi_{i}$ <br> $K$ </p>\n<script type=\"math/tex; mode=display\">\nK(x, \\cdot) = (\\sqrt{\\lambda_{1}}\\psi_{1}(x), \\sqrt{\\lambda_{2}}\\psi_{2}(x),\\cdots)_{\\mathcal{H}}^{\\mathsf{T}}</script><p></p>\n<script type=\"math/tex; mode=display\">\n\\langle K(x, \\cdot), K(y, \\cdot) \\rangle_{\\mathcal{H}} = \\sum_{i=0}^{\\infty} \\lambda_{i} \\psi_{i}(x) \\psi_{i}(y) = K(x, y)</script><p>reproducing $\\mathcal{H}$ reproducing kernel Hilbert spaceRKHS</p>\n<p></p>\n<p></p>\n<script type=\"math/tex; mode=display\">\n\\Phi(x) = K(x, \\cdot) = (\\sqrt{\\lambda_{1}}\\psi_{1}(x), \\sqrt{\\lambda_{2}}\\psi_{2}(x),\\cdots)_{\\mathcal{H}}^{\\mathsf{T}}</script><p></p>\n<script type=\"math/tex; mode=display\">\n\\langle \\Phi(x), \\Phi(y) \\rangle_{\\mathcal{H}} = \\langle K(x, \\cdot), K(y, \\cdot) \\rangle_{\\mathcal{H}} = K(x, y)</script><p> $K$  $\\Phi$  $\\mathcal{H}$ </p>\n<script type=\"math/tex; mode=display\">\n\\langle \\Phi(x), \\Phi(y) \\rangle_{\\mathcal{H}} = K(x, y)</script><p>kernel trick</p>\n<p><strong> RKHS </strong></p>\n<h1 id=\"-SVM-\"><a href=\"#-SVM-\" class=\"headerlink\" title=\" SVM \"></a> SVM </h1><p></p>\n<p> $x$  $\\Phi(x)$ </p>\n<p> $\\langle x_{i}, x_{j} \\rangle$  $x$  $\\Phi(x)$  $\\langle \\Phi(x_{i}), \\Phi(x_{j}) \\rangle$ <br> $K$  $K$  $\\Phi$  $\\langle \\Phi(x_{i}), \\Phi(x_{j}) \\rangle$  $K(x_{i}, x_{j})$  $K$  $x$ </p>\n<p> $\\lVert x - y \\rVert^{2}$ </p>\n","site":{"data":{}},"excerpt":"","more":"<p><strong>functional analysis</strong>function space</p>\n<p></p>\n<img src=\"/2017/08/13/about-kernel-02/linear_space.png\" title=\"Linear space\">\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>linear spacebasis<br> $\\lbrace 1, x, x^{2} \\rbrace$  $f(x)$  $f(x) = \\alpha_{1} \\cdot 1 + \\alpha_{2} \\cdot x + \\alpha_{3} \\cdot x^{2}$  $f(x)$  $\\lbrace \\alpha_{1}, \\alpha_{2}, \\alpha_{3} \\rbrace$  $\\lbrace x, y, z \\rbrace$ <br> $\\lbrace 1, x+1, (x+1)^{2} \\rbrace$  $n$  $n$ <br>orthogonal basis 0</p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>metric linear space<strong>metric</strong><br></p>\n<ol>\n<li> $d(x, y) \\ge 0; d(x, y) = 0 \\Leftrightarrow x = y$ </li>\n<li> $d(x, y) = d(y, x)$ </li>\n<li>$d(x, z) + d(z, y) \\ge d(x, y)$ </li>\n</ol>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>normed linear space<strong>norm</strong><br></p>\n<ol>\n<li>$\\lVert x \\rVert \\ge 0$ </li>\n<li>$ \\lVert \\alpha x \\rVert = |\\alpha| \\lVert x \\rVert $ </li>\n<li>$\\lVert x \\rVert + \\lVert y \\rVert \\ge \\lVert x+y \\rVert$ </li>\n</ol>\n<p> $d(x, y) = \\lVert x - y \\rVert$ </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>Banach space<br>completenessCauchy sequenceconvergence</p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>inner product linear space<strong>inner product</strong><br><strong>scalar product</strong><strong>dot product</strong><br><br></p>\n<ol>\n<li>$\\langle x, y \\rangle = \\langle y, x \\rangle$ </li>\n<li> $\\langle x, y \\rangle + \\langle x, z \\rangle = \\langle x, y + z \\rangle$  $\\langle \\alpha x, y \\rangle = \\alpha \\langle x, y \\rangle$ </li>\n<li>$\\langle x, x \\rangle \\ge 0$ </li>\n</ol>\n<p> $\\lVert x \\rVert^{2} = \\langle x, x \\rangle$ </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>Euclidean space</p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>Hilbert space</p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><ol>\n<li>Taylor series $\\lbrace x^{i} \\rbrace_{0}^{\\infty}$ </li>\n<li>Fourier series $\\lbrace 1, \\cos x, \\sin x, \\cos 2x, \\sin 2x, \\cdots \\rbrace$ </li>\n</ol>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n<p> $n \\times n$ </p>\n<script type=\"math/tex; mode=display\">\nAx=\\lambda x</script><p> $n$ </p>\n<p></p>\n<p> $f(x)$  $K(x, y)$ </p>\n<ol>\n<li>$\\forall f \\rightarrow \\iint f(x)K(x, y)f(y) \\,\\mathrm{d}x\\,\\mathrm{d}y \\ge 0$ </li>\n<li>$K(x, y) = K(y, x)$ </li>\n</ol>\n<p><strong>kernel function</strong></p>\n<p> $\\lambda$  $\\psi(x)$ </p>\n<script type=\"math/tex; mode=display\">\n\\int K(x, y)\\psi(x) \\mathrm{d}x = \\lambda \\psi (y)</script><p> $\\lambda_{1}$  $\\lambda_{2}$  $\\psi_{1}(x)$  $\\psi_{2}(x)$ </p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\int \\lambda_1 \\psi_{1}(x) \\psi_{2}(x) \\mathrm{d}x &= \\iint K(y, x) \\psi_{1}(y) \\mathrm{d}y\\,\\psi_{2}(x)\\mathrm{d}x \\\\\n&= \\iint K(y, x) \\psi_{2}(x) \\mathrm{d}x\\,\\psi_{1}(y)\\mathrm{d}y \\\\\n&= \\int \\lambda_2 \\psi_{2}(y) \\psi_{1}(y) \\mathrm{d}y \\\\\n&= \\int \\lambda_2 \\psi_{2}(x) \\psi_{1}(x) \\mathrm{d}x\n\\end{aligned}</script><p></p>\n<script type=\"math/tex; mode=display\">\n\\langle \\psi_{1}, \\psi_{2} \\rangle = \\int \\psi_{1}(x) \\psi_{2}(x) \\mathrm{d}x = 0</script><p> $K$ $\\lbrace \\lambda_{i} \\rbrace_{i=1}^{\\infty}$  $\\lbrace \\psi_{i} \\rbrace_{i=1}^{\\infty}$ </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> $\\lbrace \\sqrt{\\lambda_{i}}\\psi_{i} \\rbrace_{i=1}^{\\infty}$  $\\mathcal{H}$ </p>\n<script type=\"math/tex; mode=display\">\nf = \\sum_{i=1}^{\\infty} f_{i}\\sqrt{\\lambda_{i}}\\psi_{i}</script><p> $f$  $\\mathcal{H}$  $f = (f_{1}, f_{2}, \\cdots)_{\\mathcal{H}}^{\\mathsf{T}}$ <br> $g = (g_{1}, g_{2}, \\cdots)_{\\mathcal{H}}^{\\mathsf{T}}$ </p>\n<script type=\"math/tex; mode=display\">\n\\langle f, g \\rangle_{\\mathcal{H}} = \\sum_{i=1}^{\\infty}f_{i}g_{i}</script><p></p>\n<script type=\"math/tex; mode=display\">\nK(x, \\cdot) = \\sum_{i=0}^{\\infty} \\lambda_{i}\\psi_{i}(x)\\psi_{i}(\\cdot)</script><p> $\\psi_{i}$ $K(x, \\cdot) = \\sum_{i=0}^{\\infty} \\lambda_{i}\\psi_{i}(x)\\psi_{i}$ <br> $K$ </p>\n<script type=\"math/tex; mode=display\">\nK(x, \\cdot) = (\\sqrt{\\lambda_{1}}\\psi_{1}(x), \\sqrt{\\lambda_{2}}\\psi_{2}(x),\\cdots)_{\\mathcal{H}}^{\\mathsf{T}}</script><p></p>\n<script type=\"math/tex; mode=display\">\n\\langle K(x, \\cdot), K(y, \\cdot) \\rangle_{\\mathcal{H}} = \\sum_{i=0}^{\\infty} \\lambda_{i} \\psi_{i}(x) \\psi_{i}(y) = K(x, y)</script><p>reproducing $\\mathcal{H}$ reproducing kernel Hilbert spaceRKHS</p>\n<p></p>\n<p></p>\n<script type=\"math/tex; mode=display\">\n\\Phi(x) = K(x, \\cdot) = (\\sqrt{\\lambda_{1}}\\psi_{1}(x), \\sqrt{\\lambda_{2}}\\psi_{2}(x),\\cdots)_{\\mathcal{H}}^{\\mathsf{T}}</script><p></p>\n<script type=\"math/tex; mode=display\">\n\\langle \\Phi(x), \\Phi(y) \\rangle_{\\mathcal{H}} = \\langle K(x, \\cdot), K(y, \\cdot) \\rangle_{\\mathcal{H}} = K(x, y)</script><p> $K$  $\\Phi$  $\\mathcal{H}$ </p>\n<script type=\"math/tex; mode=display\">\n\\langle \\Phi(x), \\Phi(y) \\rangle_{\\mathcal{H}} = K(x, y)</script><p>kernel trick</p>\n<p><strong> RKHS </strong></p>\n<h1 id=\"-SVM-\"><a href=\"#-SVM-\" class=\"headerlink\" title=\" SVM \"></a> SVM </h1><p></p>\n<p> $x$  $\\Phi(x)$ </p>\n<p> $\\langle x_{i}, x_{j} \\rangle$  $x$  $\\Phi(x)$  $\\langle \\Phi(x_{i}), \\Phi(x_{j}) \\rangle$ <br> $K$  $K$  $\\Phi$  $\\langle \\Phi(x_{i}), \\Phi(x_{j}) \\rangle$  $K(x_{i}, x_{j})$  $K$  $x$ </p>\n<p> $\\lVert x - y \\rVert^{2}$ </p>\n"},{"title":"","date":"2017-12-27T12:57:14.000Z","description":"","_content":"\n# \n\nYouTube StackOverflow  item  50000  95  5  95  item 95 50000  90  40000  92 \n\n# \n\n 100  85  100  85%  15%  item  85%  $\\alpha$  $\\alpha = 95\\%$  item \n\n\n\n- $S_{\\mathrm{max}}$ \n-  item $p$\n- $n$\n- $K = z_{1-\\frac{\\alpha}{2}}$\n\n\n$$\ns = p_{\\mathrm{min}} \\cdot S_{\\mathrm{max}}, \\, p_{\\mathrm{min}} = \\frac{p + \\frac{K^{2}}{2n} - K \\sqrt{\\frac{p(1 - p)}{n} + \\frac{K^{2}}{4n^{2}}}}{1 + \\frac{K^{2}}{n}}\n$$\n [](http://sakigami-yang.me/2017/10/12/statistical-formulars-for-programmers/) \n\n# \n\n item  $C$ \n\n\n\n- $C$\n- $M$\n-  item $n$\n-  item $s$\n\n\n$$\n\\hat{s} = \\frac{CM + ns}{C + n}\n$$\n\n# Python \n\n```python\ndef Wilson(p, n):\n    \"\"\"\n    \n    \"\"\"\n    p = float(p)\n    K = 1.96  # 95% confidence level\n    _K2_div_n = (K ** 2) / n\n    pmin = (p + _K2_div_n / 2.0 -\n            K * ((p * (1 - p) / n + _K2_div_n / n / 4.0) ** 0.5)) / (1 + _K2_div_n)\n    return pmin\n\n\ndef WilsonAvgP(n):\n    \"\"\"\n     Bayesian  M \n     0.01~1  100 \n    \n    \"\"\"\n    totalP = 0.0\n    totalN = 0\n    p = 0.01\n\n    while True:\n        totalP += Wilson(p, n, 1);\n        totalN += 1\n        p += 0.01\n        if p >= 1:\n            break\n\n    return totalP / totalN\n\n\ndef Bayesian(C, M, n, s):\n    \"\"\"\n    \n     C  M \n    \n    \"\"\"\n    return (C * M + n * s) / (n + C)\n```\n\n\n\n\n\n","source":"_posts/rating-model-considering-user-count.md","raw":"---\ntitle: \ndate: 2017-12-27 20:57:14\ncategories: ML\ntags:\n     - Rating\ndescription: \n---\n\n# \n\nYouTube StackOverflow  item  50000  95  5  95  item 95 50000  90  40000  92 \n\n# \n\n 100  85  100  85%  15%  item  85%  $\\alpha$  $\\alpha = 95\\%$  item \n\n\n\n- $S_{\\mathrm{max}}$ \n-  item $p$\n- $n$\n- $K = z_{1-\\frac{\\alpha}{2}}$\n\n\n$$\ns = p_{\\mathrm{min}} \\cdot S_{\\mathrm{max}}, \\, p_{\\mathrm{min}} = \\frac{p + \\frac{K^{2}}{2n} - K \\sqrt{\\frac{p(1 - p)}{n} + \\frac{K^{2}}{4n^{2}}}}{1 + \\frac{K^{2}}{n}}\n$$\n [](http://sakigami-yang.me/2017/10/12/statistical-formulars-for-programmers/) \n\n# \n\n item  $C$ \n\n\n\n- $C$\n- $M$\n-  item $n$\n-  item $s$\n\n\n$$\n\\hat{s} = \\frac{CM + ns}{C + n}\n$$\n\n# Python \n\n```python\ndef Wilson(p, n):\n    \"\"\"\n    \n    \"\"\"\n    p = float(p)\n    K = 1.96  # 95% confidence level\n    _K2_div_n = (K ** 2) / n\n    pmin = (p + _K2_div_n / 2.0 -\n            K * ((p * (1 - p) / n + _K2_div_n / n / 4.0) ** 0.5)) / (1 + _K2_div_n)\n    return pmin\n\n\ndef WilsonAvgP(n):\n    \"\"\"\n     Bayesian  M \n     0.01~1  100 \n    \n    \"\"\"\n    totalP = 0.0\n    totalN = 0\n    p = 0.01\n\n    while True:\n        totalP += Wilson(p, n, 1);\n        totalN += 1\n        p += 0.01\n        if p >= 1:\n            break\n\n    return totalP / totalN\n\n\ndef Bayesian(C, M, n, s):\n    \"\"\"\n    \n     C  M \n    \n    \"\"\"\n    return (C * M + n * s) / (n + C)\n```\n\n\n\n\n\n","slug":"rating-model-considering-user-count","published":1,"updated":"2021-07-19T13:03:31.698Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckwrktivh000iqxt6qn87def1","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>YouTube StackOverflow  item  50000  95  5  95  item 95 50000  90  40000  92 </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> 100  85  100  85%  15%  item  85%  $\\alpha$  $\\alpha = 95\\%$  item </p>\n<p></p>\n<ul>\n<li>$S_{\\mathrm{max}}$ </li>\n<li> item $p$</li>\n<li>$n$</li>\n<li>$K = z_{1-\\frac{\\alpha}{2}}$</li>\n</ul>\n<p></p>\n<script type=\"math/tex; mode=display\">\ns = p_{\\mathrm{min}} \\cdot S_{\\mathrm{max}}, \\, p_{\\mathrm{min}} = \\frac{p + \\frac{K^{2}}{2n} - K \\sqrt{\\frac{p(1 - p)}{n} + \\frac{K^{2}}{4n^{2}}}}{1 + \\frac{K^{2}}{n}}</script><p> <a href=\"http://sakigami-yang.me/2017/10/12/statistical-formulars-for-programmers/\"></a> </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> item  $C$ </p>\n<p></p>\n<ul>\n<li>$C$</li>\n<li>$M$</li>\n<li> item $n$</li>\n<li> item $s$</li>\n</ul>\n<p></p>\n<script type=\"math/tex; mode=display\">\n\\hat{s} = \\frac{CM + ns}{C + n}</script><h1 id=\"Python-\"><a href=\"#Python-\" class=\"headerlink\" title=\"Python \"></a>Python </h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Wilson</span><span class=\"params\">(p, n)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    p = float(p)</span><br><span class=\"line\">    K = <span class=\"number\">1.96</span>  <span class=\"comment\"># 95% confidence level</span></span><br><span class=\"line\">    _K2_div_n = (K ** <span class=\"number\">2</span>) / n</span><br><span class=\"line\">    pmin = (p + _K2_div_n / <span class=\"number\">2.0</span> -</span><br><span class=\"line\">            K * ((p * (<span class=\"number\">1</span> - p) / n + _K2_div_n / n / <span class=\"number\">4.0</span>) ** <span class=\"number\">0.5</span>)) / (<span class=\"number\">1</span> + _K2_div_n)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> pmin</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">WilsonAvgP</span><span class=\"params\">(n)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">     Bayesian  M </span></span><br><span class=\"line\"><span class=\"string\">     0.01~1  100 </span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    totalP = <span class=\"number\">0.0</span></span><br><span class=\"line\">    totalN = <span class=\"number\">0</span></span><br><span class=\"line\">    p = <span class=\"number\">0.01</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">while</span> <span class=\"keyword\">True</span>:</span><br><span class=\"line\">        totalP += Wilson(p, n, <span class=\"number\">1</span>);</span><br><span class=\"line\">        totalN += <span class=\"number\">1</span></span><br><span class=\"line\">        p += <span class=\"number\">0.01</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> p &gt;= <span class=\"number\">1</span>:</span><br><span class=\"line\">            <span class=\"keyword\">break</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> totalP / totalN</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Bayesian</span><span class=\"params\">(C, M, n, s)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">     C  M </span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> (C * M + n * s) / (n + C)</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>YouTube StackOverflow  item  50000  95  5  95  item 95 50000  90  40000  92 </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> 100  85  100  85%  15%  item  85%  $\\alpha$  $\\alpha = 95\\%$  item </p>\n<p></p>\n<ul>\n<li>$S_{\\mathrm{max}}$ </li>\n<li> item $p$</li>\n<li>$n$</li>\n<li>$K = z_{1-\\frac{\\alpha}{2}}$</li>\n</ul>\n<p></p>\n<script type=\"math/tex; mode=display\">\ns = p_{\\mathrm{min}} \\cdot S_{\\mathrm{max}}, \\, p_{\\mathrm{min}} = \\frac{p + \\frac{K^{2}}{2n} - K \\sqrt{\\frac{p(1 - p)}{n} + \\frac{K^{2}}{4n^{2}}}}{1 + \\frac{K^{2}}{n}}</script><p> <a href=\"http://sakigami-yang.me/2017/10/12/statistical-formulars-for-programmers/\"></a> </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> item  $C$ </p>\n<p></p>\n<ul>\n<li>$C$</li>\n<li>$M$</li>\n<li> item $n$</li>\n<li> item $s$</li>\n</ul>\n<p></p>\n<script type=\"math/tex; mode=display\">\n\\hat{s} = \\frac{CM + ns}{C + n}</script><h1 id=\"Python-\"><a href=\"#Python-\" class=\"headerlink\" title=\"Python \"></a>Python </h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Wilson</span><span class=\"params\">(p, n)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    p = float(p)</span><br><span class=\"line\">    K = <span class=\"number\">1.96</span>  <span class=\"comment\"># 95% confidence level</span></span><br><span class=\"line\">    _K2_div_n = (K ** <span class=\"number\">2</span>) / n</span><br><span class=\"line\">    pmin = (p + _K2_div_n / <span class=\"number\">2.0</span> -</span><br><span class=\"line\">            K * ((p * (<span class=\"number\">1</span> - p) / n + _K2_div_n / n / <span class=\"number\">4.0</span>) ** <span class=\"number\">0.5</span>)) / (<span class=\"number\">1</span> + _K2_div_n)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> pmin</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">WilsonAvgP</span><span class=\"params\">(n)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">     Bayesian  M </span></span><br><span class=\"line\"><span class=\"string\">     0.01~1  100 </span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    totalP = <span class=\"number\">0.0</span></span><br><span class=\"line\">    totalN = <span class=\"number\">0</span></span><br><span class=\"line\">    p = <span class=\"number\">0.01</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">while</span> <span class=\"keyword\">True</span>:</span><br><span class=\"line\">        totalP += Wilson(p, n, <span class=\"number\">1</span>);</span><br><span class=\"line\">        totalN += <span class=\"number\">1</span></span><br><span class=\"line\">        p += <span class=\"number\">0.01</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> p &gt;= <span class=\"number\">1</span>:</span><br><span class=\"line\">            <span class=\"keyword\">break</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> totalP / totalN</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Bayesian</span><span class=\"params\">(C, M, n, s)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">     C  M </span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> (C * M + n * s) / (n + C)</span><br></pre></td></tr></table></figure>\n"},{"title":" Softmax ","date":"2017-08-10T13:12:46.000Z","description":"   Softmax  Logistic  Softmax ","_content":"\n SoftmaxSoftmax\n\n\n#   \n\nLogistic  Softmax \nGeneralized Linear Model\n\n\n- \n-  Logistic \n-  Softmax \n\n $y$ exponential family distribution $h(x)$ \n\n\n\n\n\n1.  $x$  $\\theta$ $y$  $P(y \\mid x; \\, \\theta)$ \n2.  $T(y)$  $E[T(y) \\mid x]$ \n3. $\\eta$  $x$  $\\eta=\\theta^{\\mathsf{T}}x$ \n\n\n$$\nP(y; \\, \\eta) = b(y) \\ast \\exp(\\eta^{\\mathsf{T}}T(y)-a(\\eta))\n$$\n\n#  Logistic \n\nBernoulli distribution 0-1 binomial distribution\n $n$  $n$ \n$P(y; \\, \\phi) = \\phi^{y}(1-\\phi)^{1-y}$ \n$P(y; \\, \\phi) = \\exp(y\\ln \\frac{\\phi}{1-\\phi}+\\ln(1-\\phi))$ \n$\\eta=\\ln \\frac{\\phi}{1-\\phi} \\Rightarrow \\phi=\\frac{1}{1+e^{-\\eta}}$ \n\n$$\n\\phi = \\frac{1}{1+e^{-\\theta^{\\mathsf{T}}x}}\n$$\n Logistic \n 0  1\n\n#  Softmax \n\nmultinomial distribution $n$  $k$  Logistic \n$$\nP(y^{(i)}=j \\mid x^{(i)}; \\, \\theta)=\\frac{e^{\\theta_{j}^{\\mathsf{T}}x^{(i)}}}{\\sum_k e^{\\theta_{k}^{\\mathsf{T}}x^{(i)}}}\n$$\n Softmax  $k$  $k-1$  $k$ \n\n $k=2$ Softmax  Logistic ","source":"_posts/thinking-from-softmax-01.md","raw":"---\ntitle:  Softmax \ndate: 2017-08-10 21:12:46\ncategories: ML\ntags: \n     - Generalized Linear Model\n     - Exponential family distribution\n     - Logistic\n     - Softmax\ndescription:    Softmax  Logistic  Softmax \n---\n\n SoftmaxSoftmax\n\n\n#   \n\nLogistic  Softmax \nGeneralized Linear Model\n\n\n- \n-  Logistic \n-  Softmax \n\n $y$ exponential family distribution $h(x)$ \n\n\n\n\n\n1.  $x$  $\\theta$ $y$  $P(y \\mid x; \\, \\theta)$ \n2.  $T(y)$  $E[T(y) \\mid x]$ \n3. $\\eta$  $x$  $\\eta=\\theta^{\\mathsf{T}}x$ \n\n\n$$\nP(y; \\, \\eta) = b(y) \\ast \\exp(\\eta^{\\mathsf{T}}T(y)-a(\\eta))\n$$\n\n#  Logistic \n\nBernoulli distribution 0-1 binomial distribution\n $n$  $n$ \n$P(y; \\, \\phi) = \\phi^{y}(1-\\phi)^{1-y}$ \n$P(y; \\, \\phi) = \\exp(y\\ln \\frac{\\phi}{1-\\phi}+\\ln(1-\\phi))$ \n$\\eta=\\ln \\frac{\\phi}{1-\\phi} \\Rightarrow \\phi=\\frac{1}{1+e^{-\\eta}}$ \n\n$$\n\\phi = \\frac{1}{1+e^{-\\theta^{\\mathsf{T}}x}}\n$$\n Logistic \n 0  1\n\n#  Softmax \n\nmultinomial distribution $n$  $k$  Logistic \n$$\nP(y^{(i)}=j \\mid x^{(i)}; \\, \\theta)=\\frac{e^{\\theta_{j}^{\\mathsf{T}}x^{(i)}}}{\\sum_k e^{\\theta_{k}^{\\mathsf{T}}x^{(i)}}}\n$$\n Softmax  $k$  $k-1$  $k$ \n\n $k=2$ Softmax  Logistic ","slug":"thinking-from-softmax-01","published":1,"updated":"2021-07-19T13:03:31.698Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckwrktivh000jqxt6azgq5ni1","content":"<p> SoftmaxSoftmax<br></p>\n<h1 id=\"--\"><a href=\"#--\" class=\"headerlink\" title=\"  \"></a>  </h1><p>Logistic  Softmax <br>Generalized Linear Model<br></p>\n<ul>\n<li></li>\n<li> Logistic </li>\n<li> Softmax </li>\n</ul>\n<p> $y$ exponential family distribution $h(x)$ </p>\n<p></p>\n<ol>\n<li> $x$  $\\theta$ $y$  $P(y \\mid x; \\, \\theta)$ </li>\n<li> $T(y)$  $E[T(y) \\mid x]$ </li>\n<li>$\\eta$  $x$  $\\eta=\\theta^{\\mathsf{T}}x$ </li>\n</ol>\n<p></p>\n<script type=\"math/tex; mode=display\">\nP(y; \\, \\eta) = b(y) \\ast \\exp(\\eta^{\\mathsf{T}}T(y)-a(\\eta))</script><h1 id=\"-Logistic-\"><a href=\"#-Logistic-\" class=\"headerlink\" title=\" Logistic \"></a> Logistic </h1><p>Bernoulli distribution 0-1 binomial distribution<br> $n$  $n$ <br>$P(y; \\, \\phi) = \\phi^{y}(1-\\phi)^{1-y}$ <br>$P(y; \\, \\phi) = \\exp(y\\ln \\frac{\\phi}{1-\\phi}+\\ln(1-\\phi))$ <br>$\\eta=\\ln \\frac{\\phi}{1-\\phi} \\Rightarrow \\phi=\\frac{1}{1+e^{-\\eta}}$ <br></p>\n<script type=\"math/tex; mode=display\">\n\\phi = \\frac{1}{1+e^{-\\theta^{\\mathsf{T}}x}}</script><p> Logistic <br> 0  1</p>\n<h1 id=\"-Softmax-\"><a href=\"#-Softmax-\" class=\"headerlink\" title=\" Softmax \"></a> Softmax </h1><p>multinomial distribution $n$  $k$  Logistic </p>\n<script type=\"math/tex; mode=display\">\nP(y^{(i)}=j \\mid x^{(i)}; \\, \\theta)=\\frac{e^{\\theta_{j}^{\\mathsf{T}}x^{(i)}}}{\\sum_k e^{\\theta_{k}^{\\mathsf{T}}x^{(i)}}}</script><p> Softmax  $k$  $k-1$  $k$ </p>\n<p> $k=2$ Softmax  Logistic </p>\n","site":{"data":{}},"excerpt":"","more":"<p> SoftmaxSoftmax<br></p>\n<h1 id=\"--\"><a href=\"#--\" class=\"headerlink\" title=\"  \"></a>  </h1><p>Logistic  Softmax <br>Generalized Linear Model<br></p>\n<ul>\n<li></li>\n<li> Logistic </li>\n<li> Softmax </li>\n</ul>\n<p> $y$ exponential family distribution $h(x)$ </p>\n<p></p>\n<ol>\n<li> $x$  $\\theta$ $y$  $P(y \\mid x; \\, \\theta)$ </li>\n<li> $T(y)$  $E[T(y) \\mid x]$ </li>\n<li>$\\eta$  $x$  $\\eta=\\theta^{\\mathsf{T}}x$ </li>\n</ol>\n<p></p>\n<script type=\"math/tex; mode=display\">\nP(y; \\, \\eta) = b(y) \\ast \\exp(\\eta^{\\mathsf{T}}T(y)-a(\\eta))</script><h1 id=\"-Logistic-\"><a href=\"#-Logistic-\" class=\"headerlink\" title=\" Logistic \"></a> Logistic </h1><p>Bernoulli distribution 0-1 binomial distribution<br> $n$  $n$ <br>$P(y; \\, \\phi) = \\phi^{y}(1-\\phi)^{1-y}$ <br>$P(y; \\, \\phi) = \\exp(y\\ln \\frac{\\phi}{1-\\phi}+\\ln(1-\\phi))$ <br>$\\eta=\\ln \\frac{\\phi}{1-\\phi} \\Rightarrow \\phi=\\frac{1}{1+e^{-\\eta}}$ <br></p>\n<script type=\"math/tex; mode=display\">\n\\phi = \\frac{1}{1+e^{-\\theta^{\\mathsf{T}}x}}</script><p> Logistic <br> 0  1</p>\n<h1 id=\"-Softmax-\"><a href=\"#-Softmax-\" class=\"headerlink\" title=\" Softmax \"></a> Softmax </h1><p>multinomial distribution $n$  $k$  Logistic </p>\n<script type=\"math/tex; mode=display\">\nP(y^{(i)}=j \\mid x^{(i)}; \\, \\theta)=\\frac{e^{\\theta_{j}^{\\mathsf{T}}x^{(i)}}}{\\sum_k e^{\\theta_{k}^{\\mathsf{T}}x^{(i)}}}</script><p> Softmax  $k$  $k-1$  $k$ </p>\n<p> $k=2$ Softmax  Logistic </p>\n"},{"title":"","date":"2017-10-12T01:56:30.000Z","description":"","_content":"\n# \n\n## \n\n\n$$\ns = \\sqrt{\\frac{1}{N-1}\\sum_{i=1}^{N}(x_{i}-\\bar{x})^{2}}\n$$\n\n\n- $N$ \n- $x_{i}$  $i$ \n- $\\bar{x}$ \n\n## \n\n\n$$\nSE = \\frac{s}{\\sqrt{N}}\n$$\n\n## \n\n\n$$\nCI = \\bar{x} \\pm t_{\\alpha/2} SE\n$$\n\n\n- $\\alpha$  5%  1 \n- $t_{\\alpha/2}$  $N-1$  t \n\n##  t \n\n t \n\n\n$$\nt = \\frac{\\bar{x_{1}} - \\bar{x_{2}}}{\\sqrt{s_{1}^{2}/n_{1}+s_{2}^{2}/n_{2}}}\n$$\n $\\lvert t \\rvert$ \n$$\ndf = \\frac{(s_{1}^{2}/n_{1}+s_{2}^{2}/n_{2})^2}{(s_{1}^{2}/n_{1})^{2}/(n_{1}-1)+(s_{2}^{2}/n_{2})^{2}/(n_{2}-1)}\n$$\n t  $1-\\alpha /2$ \n\n# \n\n## \n\n\n$$\nCI = \\left( p + \\frac{z_{\\alpha / 2}^{2}}{2N} \\pm z_{\\alpha / 2}\\sqrt{[p(1-p)+z_{\\alpha / 2}/4N] / N} \\right) / (1+z_{\\alpha / 2}^{2}/N)\n$$\n\n\n- $p$ \n- $z_{\\alpha / 2}$  $1-\\alpha / 2$ \n\n## \n\n$$\nCI = \\left( p_{j} + \\frac{z_{\\alpha / 2}^{2}}{2N} \\pm z_{\\alpha / 2}\\sqrt{[p_{j}(1-p_{j})+z_{\\alpha / 2}/4N] / N} \\right) / (1+z_{\\alpha / 2}^{2}/N)\n$$\n\n## \n\n\n\n\n$$\nX^{2} = \\sum_{i=1}^{n} \\sum_{j=1}^{m} \\frac{(O_{i,j}-E_{i,j})^{2}}{E_{i,j}}\n$$\n\n\n- $n$ \n- $m$ \n- $O_{i,j}$  $i$  $j$ \n- $E_{i,j}$  $i$  $j$ \n\n\n$$\nE_{i,j} = \\frac{\\sum_{k=1}^{n} O_{k,j} \\sum_{l=1}^{m} O_{i,l}}{N}\n$$\n\n\n- $N$ \n\n$X^{2}$  $(m-1) \\times (n-1)$  $\\chi^{2}$  $1-\\alpha$ \n\n# \n\n## \n\n$$\n\\sigma = \\sqrt{\\lambda}\n$$\n\n## \n\n$$\nCI = \\left( \\frac{\\gamma^{-1}(\\alpha / 2,c)}{t},\\frac{\\gamma^{-1}(1 - \\alpha / 2,c+1)}{t} \\right)\n$$\n\n\n\n- $c$  $t$ \n- $\\gamma^{-1}(p, c)$ \n\n## \n\n 5  -2  28.57%\n\n5  7 \n\np \n$$\np = 2 \\times \\frac{c!}{t^{c}} \\times \\min \\left\\{ \\sum_{i=0}^{c_{1}}\\frac{t_{1}^{i}t_{2}^{c-i}}{i!(c-i!)}, \\sum_{i=c_{1}}^{c}\\frac{t_{1}^{i}t_{2}^{c-i}}{i!(c-i!)} \\right\\}\n$$\n\n\n-  1  $t_{1}$  $c_{1}$ \n-  2  $t_{2}$  $c_{2}$ \n- $c = c_{1} + c_{2}$ $t = t_{1}+t_{2}$ \n\n","source":"_posts/statistical-formulars-for-programmers.md","raw":"---\ntitle: \ndate: 2017-10-12 09:56:30\ncategories: Mathematics\ntags: \n     - Statistics\ndescription: \n---\n\n# \n\n## \n\n\n$$\ns = \\sqrt{\\frac{1}{N-1}\\sum_{i=1}^{N}(x_{i}-\\bar{x})^{2}}\n$$\n\n\n- $N$ \n- $x_{i}$  $i$ \n- $\\bar{x}$ \n\n## \n\n\n$$\nSE = \\frac{s}{\\sqrt{N}}\n$$\n\n## \n\n\n$$\nCI = \\bar{x} \\pm t_{\\alpha/2} SE\n$$\n\n\n- $\\alpha$  5%  1 \n- $t_{\\alpha/2}$  $N-1$  t \n\n##  t \n\n t \n\n\n$$\nt = \\frac{\\bar{x_{1}} - \\bar{x_{2}}}{\\sqrt{s_{1}^{2}/n_{1}+s_{2}^{2}/n_{2}}}\n$$\n $\\lvert t \\rvert$ \n$$\ndf = \\frac{(s_{1}^{2}/n_{1}+s_{2}^{2}/n_{2})^2}{(s_{1}^{2}/n_{1})^{2}/(n_{1}-1)+(s_{2}^{2}/n_{2})^{2}/(n_{2}-1)}\n$$\n t  $1-\\alpha /2$ \n\n# \n\n## \n\n\n$$\nCI = \\left( p + \\frac{z_{\\alpha / 2}^{2}}{2N} \\pm z_{\\alpha / 2}\\sqrt{[p(1-p)+z_{\\alpha / 2}/4N] / N} \\right) / (1+z_{\\alpha / 2}^{2}/N)\n$$\n\n\n- $p$ \n- $z_{\\alpha / 2}$  $1-\\alpha / 2$ \n\n## \n\n$$\nCI = \\left( p_{j} + \\frac{z_{\\alpha / 2}^{2}}{2N} \\pm z_{\\alpha / 2}\\sqrt{[p_{j}(1-p_{j})+z_{\\alpha / 2}/4N] / N} \\right) / (1+z_{\\alpha / 2}^{2}/N)\n$$\n\n## \n\n\n\n\n$$\nX^{2} = \\sum_{i=1}^{n} \\sum_{j=1}^{m} \\frac{(O_{i,j}-E_{i,j})^{2}}{E_{i,j}}\n$$\n\n\n- $n$ \n- $m$ \n- $O_{i,j}$  $i$  $j$ \n- $E_{i,j}$  $i$  $j$ \n\n\n$$\nE_{i,j} = \\frac{\\sum_{k=1}^{n} O_{k,j} \\sum_{l=1}^{m} O_{i,l}}{N}\n$$\n\n\n- $N$ \n\n$X^{2}$  $(m-1) \\times (n-1)$  $\\chi^{2}$  $1-\\alpha$ \n\n# \n\n## \n\n$$\n\\sigma = \\sqrt{\\lambda}\n$$\n\n## \n\n$$\nCI = \\left( \\frac{\\gamma^{-1}(\\alpha / 2,c)}{t},\\frac{\\gamma^{-1}(1 - \\alpha / 2,c+1)}{t} \\right)\n$$\n\n\n\n- $c$  $t$ \n- $\\gamma^{-1}(p, c)$ \n\n## \n\n 5  -2  28.57%\n\n5  7 \n\np \n$$\np = 2 \\times \\frac{c!}{t^{c}} \\times \\min \\left\\{ \\sum_{i=0}^{c_{1}}\\frac{t_{1}^{i}t_{2}^{c-i}}{i!(c-i!)}, \\sum_{i=c_{1}}^{c}\\frac{t_{1}^{i}t_{2}^{c-i}}{i!(c-i!)} \\right\\}\n$$\n\n\n-  1  $t_{1}$  $c_{1}$ \n-  2  $t_{2}$  $c_{2}$ \n- $c = c_{1} + c_{2}$ $t = t_{1}+t_{2}$ \n\n","slug":"statistical-formulars-for-programmers","published":1,"updated":"2021-07-19T13:03:31.698Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckwrktivi000kqxt6u0339qal","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p></p>\n<script type=\"math/tex; mode=display\">\ns = \\sqrt{\\frac{1}{N-1}\\sum_{i=1}^{N}(x_{i}-\\bar{x})^{2}}</script><p></p>\n<ul>\n<li>$N$ </li>\n<li>$x_{i}$  $i$ </li>\n<li>$\\bar{x}$ </li>\n</ul>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p></p>\n<script type=\"math/tex; mode=display\">\nSE = \\frac{s}{\\sqrt{N}}</script><h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p></p>\n<script type=\"math/tex; mode=display\">\nCI = \\bar{x} \\pm t_{\\alpha/2} SE</script><p></p>\n<ul>\n<li>$\\alpha$  5%  1 </li>\n<li>$t_{\\alpha/2}$  $N-1$  t </li>\n</ul>\n<h2 id=\"-t-\"><a href=\"#-t-\" class=\"headerlink\" title=\" t \"></a> t </h2><p> t </p>\n<p></p>\n<script type=\"math/tex; mode=display\">\nt = \\frac{\\bar{x_{1}} - \\bar{x_{2}}}{\\sqrt{s_{1}^{2}/n_{1}+s_{2}^{2}/n_{2}}}</script><p> $\\lvert t \\rvert$ </p>\n<script type=\"math/tex; mode=display\">\ndf = \\frac{(s_{1}^{2}/n_{1}+s_{2}^{2}/n_{2})^2}{(s_{1}^{2}/n_{1})^{2}/(n_{1}-1)+(s_{2}^{2}/n_{2})^{2}/(n_{2}-1)}</script><p> t  $1-\\alpha /2$ </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p></p>\n<script type=\"math/tex; mode=display\">\nCI = \\left( p + \\frac{z_{\\alpha / 2}^{2}}{2N} \\pm z_{\\alpha / 2}\\sqrt{[p(1-p)+z_{\\alpha / 2}/4N] / N} \\right) / (1+z_{\\alpha / 2}^{2}/N)</script><p></p>\n<ul>\n<li>$p$ </li>\n<li>$z_{\\alpha / 2}$  $1-\\alpha / 2$ </li>\n</ul>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><script type=\"math/tex; mode=display\">\nCI = \\left( p_{j} + \\frac{z_{\\alpha / 2}^{2}}{2N} \\pm z_{\\alpha / 2}\\sqrt{[p_{j}(1-p_{j})+z_{\\alpha / 2}/4N] / N} \\right) / (1+z_{\\alpha / 2}^{2}/N)</script><h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p></p>\n<p></p>\n<script type=\"math/tex; mode=display\">\nX^{2} = \\sum_{i=1}^{n} \\sum_{j=1}^{m} \\frac{(O_{i,j}-E_{i,j})^{2}}{E_{i,j}}</script><p></p>\n<ul>\n<li>$n$ </li>\n<li>$m$ </li>\n<li>$O_{i,j}$  $i$  $j$ </li>\n<li>$E_{i,j}$  $i$  $j$ </li>\n</ul>\n<p></p>\n<script type=\"math/tex; mode=display\">\nE_{i,j} = \\frac{\\sum_{k=1}^{n} O_{k,j} \\sum_{l=1}^{m} O_{i,l}}{N}</script><p></p>\n<ul>\n<li>$N$ </li>\n</ul>\n<p>$X^{2}$  $(m-1) \\times (n-1)$  $\\chi^{2}$  $1-\\alpha$ </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><script type=\"math/tex; mode=display\">\n\\sigma = \\sqrt{\\lambda}</script><h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><script type=\"math/tex; mode=display\">\nCI = \\left( \\frac{\\gamma^{-1}(\\alpha / 2,c)}{t},\\frac{\\gamma^{-1}(1 - \\alpha / 2,c+1)}{t} \\right)</script><p></p>\n<ul>\n<li>$c$  $t$ </li>\n<li>$\\gamma^{-1}(p, c)$ </li>\n</ul>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> 5  -2  28.57%</p>\n<p>5  7 </p>\n<p>p </p>\n<script type=\"math/tex; mode=display\">\np = 2 \\times \\frac{c!}{t^{c}} \\times \\min \\left\\{ \\sum_{i=0}^{c_{1}}\\frac{t_{1}^{i}t_{2}^{c-i}}{i!(c-i!)}, \\sum_{i=c_{1}}^{c}\\frac{t_{1}^{i}t_{2}^{c-i}}{i!(c-i!)} \\right\\}</script><p></p>\n<ul>\n<li> 1  $t_{1}$  $c_{1}$ </li>\n<li> 2  $t_{2}$  $c_{2}$ </li>\n<li>$c = c_{1} + c_{2}$ $t = t_{1}+t_{2}$ </li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p></p>\n<script type=\"math/tex; mode=display\">\ns = \\sqrt{\\frac{1}{N-1}\\sum_{i=1}^{N}(x_{i}-\\bar{x})^{2}}</script><p></p>\n<ul>\n<li>$N$ </li>\n<li>$x_{i}$  $i$ </li>\n<li>$\\bar{x}$ </li>\n</ul>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p></p>\n<script type=\"math/tex; mode=display\">\nSE = \\frac{s}{\\sqrt{N}}</script><h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p></p>\n<script type=\"math/tex; mode=display\">\nCI = \\bar{x} \\pm t_{\\alpha/2} SE</script><p></p>\n<ul>\n<li>$\\alpha$  5%  1 </li>\n<li>$t_{\\alpha/2}$  $N-1$  t </li>\n</ul>\n<h2 id=\"-t-\"><a href=\"#-t-\" class=\"headerlink\" title=\" t \"></a> t </h2><p> t </p>\n<p></p>\n<script type=\"math/tex; mode=display\">\nt = \\frac{\\bar{x_{1}} - \\bar{x_{2}}}{\\sqrt{s_{1}^{2}/n_{1}+s_{2}^{2}/n_{2}}}</script><p> $\\lvert t \\rvert$ </p>\n<script type=\"math/tex; mode=display\">\ndf = \\frac{(s_{1}^{2}/n_{1}+s_{2}^{2}/n_{2})^2}{(s_{1}^{2}/n_{1})^{2}/(n_{1}-1)+(s_{2}^{2}/n_{2})^{2}/(n_{2}-1)}</script><p> t  $1-\\alpha /2$ </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p></p>\n<script type=\"math/tex; mode=display\">\nCI = \\left( p + \\frac{z_{\\alpha / 2}^{2}}{2N} \\pm z_{\\alpha / 2}\\sqrt{[p(1-p)+z_{\\alpha / 2}/4N] / N} \\right) / (1+z_{\\alpha / 2}^{2}/N)</script><p></p>\n<ul>\n<li>$p$ </li>\n<li>$z_{\\alpha / 2}$  $1-\\alpha / 2$ </li>\n</ul>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><script type=\"math/tex; mode=display\">\nCI = \\left( p_{j} + \\frac{z_{\\alpha / 2}^{2}}{2N} \\pm z_{\\alpha / 2}\\sqrt{[p_{j}(1-p_{j})+z_{\\alpha / 2}/4N] / N} \\right) / (1+z_{\\alpha / 2}^{2}/N)</script><h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p></p>\n<p></p>\n<script type=\"math/tex; mode=display\">\nX^{2} = \\sum_{i=1}^{n} \\sum_{j=1}^{m} \\frac{(O_{i,j}-E_{i,j})^{2}}{E_{i,j}}</script><p></p>\n<ul>\n<li>$n$ </li>\n<li>$m$ </li>\n<li>$O_{i,j}$  $i$  $j$ </li>\n<li>$E_{i,j}$  $i$  $j$ </li>\n</ul>\n<p></p>\n<script type=\"math/tex; mode=display\">\nE_{i,j} = \\frac{\\sum_{k=1}^{n} O_{k,j} \\sum_{l=1}^{m} O_{i,l}}{N}</script><p></p>\n<ul>\n<li>$N$ </li>\n</ul>\n<p>$X^{2}$  $(m-1) \\times (n-1)$  $\\chi^{2}$  $1-\\alpha$ </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><script type=\"math/tex; mode=display\">\n\\sigma = \\sqrt{\\lambda}</script><h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><script type=\"math/tex; mode=display\">\nCI = \\left( \\frac{\\gamma^{-1}(\\alpha / 2,c)}{t},\\frac{\\gamma^{-1}(1 - \\alpha / 2,c+1)}{t} \\right)</script><p></p>\n<ul>\n<li>$c$  $t$ </li>\n<li>$\\gamma^{-1}(p, c)$ </li>\n</ul>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> 5  -2  28.57%</p>\n<p>5  7 </p>\n<p>p </p>\n<script type=\"math/tex; mode=display\">\np = 2 \\times \\frac{c!}{t^{c}} \\times \\min \\left\\{ \\sum_{i=0}^{c_{1}}\\frac{t_{1}^{i}t_{2}^{c-i}}{i!(c-i!)}, \\sum_{i=c_{1}}^{c}\\frac{t_{1}^{i}t_{2}^{c-i}}{i!(c-i!)} \\right\\}</script><p></p>\n<ul>\n<li> 1  $t_{1}$  $c_{1}$ </li>\n<li> 2  $t_{2}$  $c_{2}$ </li>\n<li>$c = c_{1} + c_{2}$ $t = t_{1}+t_{2}$ </li>\n</ul>\n"},{"title":" ","date":"2017-09-09T01:42:10.000Z","description":"","_content":"\n# matrix completion\n\n Netflix  100  10%  2009  9  BellKors Pragmatic \n\n Netflix \n rate \n\n\nlow rank\n1cliques2genres\ncollaborative filtering\n\n\n\n$$\n\\begin{aligned}\n&\\mathrm{minimize} \\quad  \\mathrm{rank}(Z) \\\\\n&\\text{subject to} \\quad  \\sum_{(i,j):\\text{Observed}} (Z_{ij} - X_{ij})^{2} \\le \\delta\n\\end{aligned} \\\\\n\\text{Impute missing } X_{ij} \\text{ with } Z_{ij}\n$$\n NP-hard \n\n\n\n L1  L0 \n\n\n# \n\n## \n\n $X$ \n$$\n\\lVert X \\rVert _{\\ast} = \\mathrm{tr}\\left( \\sqrt{X^{\\mathsf{T}}X} \\right)\n$$\n $X$  $X=U \\Sigma V^{\\mathsf{T}}$ \n$$\n\\begin{aligned}\n\\mathrm{tr}\\left( \\sqrt{X^{\\mathsf{T}}X} \\right) & =  \\mathrm{tr} \\left( \\sqrt{(U \\Sigma V^{\\mathsf{T}})^{\\mathsf{T}}U \\Sigma V^{\\mathsf{T}}} \\right) \\\\\n& = \\mathrm{tr} \\left( \\sqrt{V \\Sigma^{\\mathsf{T}} U^{\\mathsf{T}} U \\Sigma V^{\\mathsf{T}}} \\right) \\\\\n& = \\mathrm{tr} \\left( \\sqrt{V \\Sigma^{2} V^{\\mathsf{T}}} \\right)  \\quad \\left( \\Sigma^{\\mathsf{T}} = \\Sigma \\right) \\\\\n& = \\mathrm{tr} \\left( \\sqrt{V^{\\mathsf{T}} V \\Sigma^{2}} \\right) \\\\\n& = \\mathrm{tr} (\\Sigma)\n\\end{aligned}\n$$\n\n## \n\n $ f_x (A) = \\lVert Ax \\rVert _{p} \\quad (p \\ge 1) $  $f_x$  $\\lVert A \\rVert _{p} = \\sup\\limits_{\\Vert x \\rVert _{p}=1} f_x (A)$ $\\lVert A \\rVert_{2}$ \n $\\lVert A \\rVert_{\\ast}$  $\\lVert A \\rVert_{2}$  $\\lVert A \\rVert_{\\ast}$ $\\lVert A \\rVert_{\\ast}=\\sup \\limits_{\\lVert X \\rVert_{2}=1} \\mathrm{tr} \\left( A^{\\mathsf{T}}X \\right)$\n\n## \n\n S.V.D \n$$\n\\frac{\\partial \\lVert X \\rVert_{\\ast}}{\\partial X} = \\frac{\\partial \\mathrm{tr} (\\Sigma)}{\\partial X} = \\frac{\\mathrm{tr} (\\partial \\Sigma)}{\\partial X}\n$$\n $\\partial \\Sigma$ \n $X=U \\Sigma V^{\\mathsf{T}}$ \n$$\n\\begin{aligned}\n& & \\partial X & = (\\partial U )\\Sigma V^{\\mathsf{T}} + U (\\partial \\Sigma) V^{\\mathsf{T}} + U \\Sigma (\\partial V^{\\mathsf{T}}) \\\\\n&\\Rightarrow \\, & \\partial \\Sigma & = U^{\\mathsf{T}} (\\partial X) V - U^{\\mathsf{T}} (\\partial U) \\Sigma - \\Sigma (\\partial V^{\\mathsf{T}}) V \\\\\n& & & = U^{\\mathsf{T}} (\\partial X) V \\qquad (- U^{\\mathsf{T}} (\\partial U) \\Sigma - \\Sigma (\\partial V^{\\mathsf{T}}) V = 0)\n\\end{aligned}\n$$\n\n$$\n\\frac{\\partial \\lVert X \\rVert_{\\ast}}{\\partial X} = \\frac{\\mathrm{tr} (\\partial \\Sigma)}{\\partial X} = \\frac{\\mathrm{tr} (U^{\\mathsf{T}} (\\partial X)V)}{\\partial X} = \\frac{\\mathrm{tr} (V U^{\\mathsf{T}} (\\partial X))}{\\partial X} = (V U^{\\mathsf{T}})^{\\mathsf{T}} = U V^{\\mathsf{T}}\n$$\n\n# \n\n\n\n$$\n\\begin{aligned}\n&\\mathrm{minimize} \\quad  \\lVert Z \\rVert_{\\ast} \\\\\n&\\text{subject to} \\quad  \\sum_{(i,j):\\text{Observed}} (Z_{ij} - X_{ij})^{2} \\le \\delta\n\\end{aligned} \\\\\n\\text{Impute missing } X_{ij} \\text{ with } Z_{ij}\n$$\n","source":"_posts/norm-regularization-02.md","raw":"---\ntitle:  \ndate: 2017-09-09 09:42:10\ncategories: ML\ntags:\n      - Norm regularization\n      - Convex optimization\n      - Matrix theory\ndescription: \n---\n\n# matrix completion\n\n Netflix  100  10%  2009  9  BellKors Pragmatic \n\n Netflix \n rate \n\n\nlow rank\n1cliques2genres\ncollaborative filtering\n\n\n\n$$\n\\begin{aligned}\n&\\mathrm{minimize} \\quad  \\mathrm{rank}(Z) \\\\\n&\\text{subject to} \\quad  \\sum_{(i,j):\\text{Observed}} (Z_{ij} - X_{ij})^{2} \\le \\delta\n\\end{aligned} \\\\\n\\text{Impute missing } X_{ij} \\text{ with } Z_{ij}\n$$\n NP-hard \n\n\n\n L1  L0 \n\n\n# \n\n## \n\n $X$ \n$$\n\\lVert X \\rVert _{\\ast} = \\mathrm{tr}\\left( \\sqrt{X^{\\mathsf{T}}X} \\right)\n$$\n $X$  $X=U \\Sigma V^{\\mathsf{T}}$ \n$$\n\\begin{aligned}\n\\mathrm{tr}\\left( \\sqrt{X^{\\mathsf{T}}X} \\right) & =  \\mathrm{tr} \\left( \\sqrt{(U \\Sigma V^{\\mathsf{T}})^{\\mathsf{T}}U \\Sigma V^{\\mathsf{T}}} \\right) \\\\\n& = \\mathrm{tr} \\left( \\sqrt{V \\Sigma^{\\mathsf{T}} U^{\\mathsf{T}} U \\Sigma V^{\\mathsf{T}}} \\right) \\\\\n& = \\mathrm{tr} \\left( \\sqrt{V \\Sigma^{2} V^{\\mathsf{T}}} \\right)  \\quad \\left( \\Sigma^{\\mathsf{T}} = \\Sigma \\right) \\\\\n& = \\mathrm{tr} \\left( \\sqrt{V^{\\mathsf{T}} V \\Sigma^{2}} \\right) \\\\\n& = \\mathrm{tr} (\\Sigma)\n\\end{aligned}\n$$\n\n## \n\n $ f_x (A) = \\lVert Ax \\rVert _{p} \\quad (p \\ge 1) $  $f_x$  $\\lVert A \\rVert _{p} = \\sup\\limits_{\\Vert x \\rVert _{p}=1} f_x (A)$ $\\lVert A \\rVert_{2}$ \n $\\lVert A \\rVert_{\\ast}$  $\\lVert A \\rVert_{2}$  $\\lVert A \\rVert_{\\ast}$ $\\lVert A \\rVert_{\\ast}=\\sup \\limits_{\\lVert X \\rVert_{2}=1} \\mathrm{tr} \\left( A^{\\mathsf{T}}X \\right)$\n\n## \n\n S.V.D \n$$\n\\frac{\\partial \\lVert X \\rVert_{\\ast}}{\\partial X} = \\frac{\\partial \\mathrm{tr} (\\Sigma)}{\\partial X} = \\frac{\\mathrm{tr} (\\partial \\Sigma)}{\\partial X}\n$$\n $\\partial \\Sigma$ \n $X=U \\Sigma V^{\\mathsf{T}}$ \n$$\n\\begin{aligned}\n& & \\partial X & = (\\partial U )\\Sigma V^{\\mathsf{T}} + U (\\partial \\Sigma) V^{\\mathsf{T}} + U \\Sigma (\\partial V^{\\mathsf{T}}) \\\\\n&\\Rightarrow \\, & \\partial \\Sigma & = U^{\\mathsf{T}} (\\partial X) V - U^{\\mathsf{T}} (\\partial U) \\Sigma - \\Sigma (\\partial V^{\\mathsf{T}}) V \\\\\n& & & = U^{\\mathsf{T}} (\\partial X) V \\qquad (- U^{\\mathsf{T}} (\\partial U) \\Sigma - \\Sigma (\\partial V^{\\mathsf{T}}) V = 0)\n\\end{aligned}\n$$\n\n$$\n\\frac{\\partial \\lVert X \\rVert_{\\ast}}{\\partial X} = \\frac{\\mathrm{tr} (\\partial \\Sigma)}{\\partial X} = \\frac{\\mathrm{tr} (U^{\\mathsf{T}} (\\partial X)V)}{\\partial X} = \\frac{\\mathrm{tr} (V U^{\\mathsf{T}} (\\partial X))}{\\partial X} = (V U^{\\mathsf{T}})^{\\mathsf{T}} = U V^{\\mathsf{T}}\n$$\n\n# \n\n\n\n$$\n\\begin{aligned}\n&\\mathrm{minimize} \\quad  \\lVert Z \\rVert_{\\ast} \\\\\n&\\text{subject to} \\quad  \\sum_{(i,j):\\text{Observed}} (Z_{ij} - X_{ij})^{2} \\le \\delta\n\\end{aligned} \\\\\n\\text{Impute missing } X_{ij} \\text{ with } Z_{ij}\n$$\n","slug":"norm-regularization-02","published":1,"updated":"2021-07-20T15:07:34.647Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckwrktivi000lqxt60e8fbxb4","content":"<h1 id=\"matrix-completion\"><a href=\"#matrix-completion\" class=\"headerlink\" title=\"matrix completion\"></a>matrix completion</h1><p> Netflix  100  10%  2009  9  BellKors Pragmatic </p>\n<p> Netflix <br> rate </p>\n<p><br>low rank<br>1cliques2genres<br>collaborative filtering<br></p>\n<p></p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n&\\mathrm{minimize} \\quad  \\mathrm{rank}(Z) \\\\\n&\\text{subject to} \\quad  \\sum_{(i,j):\\text{Observed}} (Z_{ij} - X_{ij})^{2} \\le \\delta\n\\end{aligned} \\\\\n\\text{Impute missing } X_{ij} \\text{ with } Z_{ij}</script><p> NP-hard </p>\n<p><br><br> L1  L0 <br></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> $X$ </p>\n<script type=\"math/tex; mode=display\">\n\\lVert X \\rVert _{\\ast} = \\mathrm{tr}\\left( \\sqrt{X^{\\mathsf{T}}X} \\right)</script><p> $X$  $X=U \\Sigma V^{\\mathsf{T}}$ </p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\mathrm{tr}\\left( \\sqrt{X^{\\mathsf{T}}X} \\right) & =  \\mathrm{tr} \\left( \\sqrt{(U \\Sigma V^{\\mathsf{T}})^{\\mathsf{T}}U \\Sigma V^{\\mathsf{T}}} \\right) \\\\\n& = \\mathrm{tr} \\left( \\sqrt{V \\Sigma^{\\mathsf{T}} U^{\\mathsf{T}} U \\Sigma V^{\\mathsf{T}}} \\right) \\\\\n& = \\mathrm{tr} \\left( \\sqrt{V \\Sigma^{2} V^{\\mathsf{T}}} \\right)  \\quad \\left( \\Sigma^{\\mathsf{T}} = \\Sigma \\right) \\\\\n& = \\mathrm{tr} \\left( \\sqrt{V^{\\mathsf{T}} V \\Sigma^{2}} \\right) \\\\\n& = \\mathrm{tr} (\\Sigma)\n\\end{aligned}</script><h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> $ f_x (A) = \\lVert Ax \\rVert _{p} \\quad (p \\ge 1) $  $f_x$  $\\lVert A \\rVert _{p} = \\sup\\limits_{\\Vert x \\rVert _{p}=1} f_x (A)$ $\\lVert A \\rVert_{2}$ <br> $\\lVert A \\rVert_{\\ast}$  $\\lVert A \\rVert_{2}$  $\\lVert A \\rVert_{\\ast}$ $\\lVert A \\rVert_{\\ast}=\\sup \\limits_{\\lVert X \\rVert_{2}=1} \\mathrm{tr} \\left( A^{\\mathsf{T}}X \\right)$</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> S.V.D </p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial \\lVert X \\rVert_{\\ast}}{\\partial X} = \\frac{\\partial \\mathrm{tr} (\\Sigma)}{\\partial X} = \\frac{\\mathrm{tr} (\\partial \\Sigma)}{\\partial X}</script><p> $\\partial \\Sigma$ <br> $X=U \\Sigma V^{\\mathsf{T}}$ </p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n& & \\partial X & = (\\partial U )\\Sigma V^{\\mathsf{T}} + U (\\partial \\Sigma) V^{\\mathsf{T}} + U \\Sigma (\\partial V^{\\mathsf{T}}) \\\\\n&\\Rightarrow \\, & \\partial \\Sigma & = U^{\\mathsf{T}} (\\partial X) V - U^{\\mathsf{T}} (\\partial U) \\Sigma - \\Sigma (\\partial V^{\\mathsf{T}}) V \\\\\n& & & = U^{\\mathsf{T}} (\\partial X) V \\qquad (- U^{\\mathsf{T}} (\\partial U) \\Sigma - \\Sigma (\\partial V^{\\mathsf{T}}) V = 0)\n\\end{aligned}</script><p></p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial \\lVert X \\rVert_{\\ast}}{\\partial X} = \\frac{\\mathrm{tr} (\\partial \\Sigma)}{\\partial X} = \\frac{\\mathrm{tr} (U^{\\mathsf{T}} (\\partial X)V)}{\\partial X} = \\frac{\\mathrm{tr} (V U^{\\mathsf{T}} (\\partial X))}{\\partial X} = (V U^{\\mathsf{T}})^{\\mathsf{T}} = U V^{\\mathsf{T}}</script><h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p><br></p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n&\\mathrm{minimize} \\quad  \\lVert Z \\rVert_{\\ast} \\\\\n&\\text{subject to} \\quad  \\sum_{(i,j):\\text{Observed}} (Z_{ij} - X_{ij})^{2} \\le \\delta\n\\end{aligned} \\\\\n\\text{Impute missing } X_{ij} \\text{ with } Z_{ij}</script>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"matrix-completion\"><a href=\"#matrix-completion\" class=\"headerlink\" title=\"matrix completion\"></a>matrix completion</h1><p> Netflix  100  10%  2009  9  BellKors Pragmatic </p>\n<p> Netflix <br> rate </p>\n<p><br>low rank<br>1cliques2genres<br>collaborative filtering<br></p>\n<p></p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n&\\mathrm{minimize} \\quad  \\mathrm{rank}(Z) \\\\\n&\\text{subject to} \\quad  \\sum_{(i,j):\\text{Observed}} (Z_{ij} - X_{ij})^{2} \\le \\delta\n\\end{aligned} \\\\\n\\text{Impute missing } X_{ij} \\text{ with } Z_{ij}</script><p> NP-hard </p>\n<p><br><br> L1  L0 <br></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> $X$ </p>\n<script type=\"math/tex; mode=display\">\n\\lVert X \\rVert _{\\ast} = \\mathrm{tr}\\left( \\sqrt{X^{\\mathsf{T}}X} \\right)</script><p> $X$  $X=U \\Sigma V^{\\mathsf{T}}$ </p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\mathrm{tr}\\left( \\sqrt{X^{\\mathsf{T}}X} \\right) & =  \\mathrm{tr} \\left( \\sqrt{(U \\Sigma V^{\\mathsf{T}})^{\\mathsf{T}}U \\Sigma V^{\\mathsf{T}}} \\right) \\\\\n& = \\mathrm{tr} \\left( \\sqrt{V \\Sigma^{\\mathsf{T}} U^{\\mathsf{T}} U \\Sigma V^{\\mathsf{T}}} \\right) \\\\\n& = \\mathrm{tr} \\left( \\sqrt{V \\Sigma^{2} V^{\\mathsf{T}}} \\right)  \\quad \\left( \\Sigma^{\\mathsf{T}} = \\Sigma \\right) \\\\\n& = \\mathrm{tr} \\left( \\sqrt{V^{\\mathsf{T}} V \\Sigma^{2}} \\right) \\\\\n& = \\mathrm{tr} (\\Sigma)\n\\end{aligned}</script><h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> $ f_x (A) = \\lVert Ax \\rVert _{p} \\quad (p \\ge 1) $  $f_x$  $\\lVert A \\rVert _{p} = \\sup\\limits_{\\Vert x \\rVert _{p}=1} f_x (A)$ $\\lVert A \\rVert_{2}$ <br> $\\lVert A \\rVert_{\\ast}$  $\\lVert A \\rVert_{2}$  $\\lVert A \\rVert_{\\ast}$ $\\lVert A \\rVert_{\\ast}=\\sup \\limits_{\\lVert X \\rVert_{2}=1} \\mathrm{tr} \\left( A^{\\mathsf{T}}X \\right)$</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> S.V.D </p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial \\lVert X \\rVert_{\\ast}}{\\partial X} = \\frac{\\partial \\mathrm{tr} (\\Sigma)}{\\partial X} = \\frac{\\mathrm{tr} (\\partial \\Sigma)}{\\partial X}</script><p> $\\partial \\Sigma$ <br> $X=U \\Sigma V^{\\mathsf{T}}$ </p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n& & \\partial X & = (\\partial U )\\Sigma V^{\\mathsf{T}} + U (\\partial \\Sigma) V^{\\mathsf{T}} + U \\Sigma (\\partial V^{\\mathsf{T}}) \\\\\n&\\Rightarrow \\, & \\partial \\Sigma & = U^{\\mathsf{T}} (\\partial X) V - U^{\\mathsf{T}} (\\partial U) \\Sigma - \\Sigma (\\partial V^{\\mathsf{T}}) V \\\\\n& & & = U^{\\mathsf{T}} (\\partial X) V \\qquad (- U^{\\mathsf{T}} (\\partial U) \\Sigma - \\Sigma (\\partial V^{\\mathsf{T}}) V = 0)\n\\end{aligned}</script><p></p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial \\lVert X \\rVert_{\\ast}}{\\partial X} = \\frac{\\mathrm{tr} (\\partial \\Sigma)}{\\partial X} = \\frac{\\mathrm{tr} (U^{\\mathsf{T}} (\\partial X)V)}{\\partial X} = \\frac{\\mathrm{tr} (V U^{\\mathsf{T}} (\\partial X))}{\\partial X} = (V U^{\\mathsf{T}})^{\\mathsf{T}} = U V^{\\mathsf{T}}</script><h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p><br></p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n&\\mathrm{minimize} \\quad  \\lVert Z \\rVert_{\\ast} \\\\\n&\\text{subject to} \\quad  \\sum_{(i,j):\\text{Observed}} (Z_{ij} - X_{ij})^{2} \\le \\delta\n\\end{aligned} \\\\\n\\text{Impute missing } X_{ij} \\text{ with } Z_{ij}</script>"},{"title":" Softmax   Sigmoid  ReLU","date":"2017-08-11T11:40:01.000Z","description":"   Softmax ","_content":"\n Logistic  Sigmoid \n Sigmoid  S  Sigmoid  Logistic \n $\\sigma(x)$ \n$$\n\\sigma(x) = \\frac{1}{1+e^{-x}}\n$$\n\n\n- \n-  $[-\\infty, +\\infty]$ \n-  $(0, 1)$  $(0, 1)$ \n\n\n\n\n\n# \n\n\n\nneural nets\nactivation function\n\n\n\n- \n- \n- learning rate\n\n $f(x) = x$  Sigmoid \n\n# Sigmoid \n\n Sigmoid saturation 0 kill gradient\n\n$$\n\\sigma(x)' = \\sigma(x)(1 - \\sigma(x))\n$$\nSigmoid  $1/4$  0 \n\nSigmoid  0  1\nbatch kill gradient \n\n\nfeature\n Dropout random forest Bagging \n\n# \n\n- tanh\n\n   0  Sigmoid \n\n- ReLU\n\n  $f(x) = \\max(0, x)$ \n\n  ReLU\n\n  1. \n  2. \n  3. 0  1\n  4.  ReLU die 0\n\n- Leaky-ReLUP-ReLUR-ReLUMaxout\n\n   wiki ReLU \n  dying ReLU\n  \n  No free lunch theoren\n\n  ****\n\n  Maxout  Maxout  Maxout  ReLU  ReLU  Dropout \n\n\n\n# \n\n\n\n ReLU \ndying ReLU Leaky ReLUPReLU  Maxout\n\n Sigmoid tanh\n\n","source":"_posts/thinking-from-softmax-03.md","raw":"---\ntitle:  Softmax   Sigmoid  ReLU\ndate: 2017-08-11 19:40:01\ncategories: ML\ntags:\n     - Deep learning\n     - Activation function\ndescription:    Softmax \n---\n\n Logistic  Sigmoid \n Sigmoid  S  Sigmoid  Logistic \n $\\sigma(x)$ \n$$\n\\sigma(x) = \\frac{1}{1+e^{-x}}\n$$\n\n\n- \n-  $[-\\infty, +\\infty]$ \n-  $(0, 1)$  $(0, 1)$ \n\n\n\n\n\n# \n\n\n\nneural nets\nactivation function\n\n\n\n- \n- \n- learning rate\n\n $f(x) = x$  Sigmoid \n\n# Sigmoid \n\n Sigmoid saturation 0 kill gradient\n\n$$\n\\sigma(x)' = \\sigma(x)(1 - \\sigma(x))\n$$\nSigmoid  $1/4$  0 \n\nSigmoid  0  1\nbatch kill gradient \n\n\nfeature\n Dropout random forest Bagging \n\n# \n\n- tanh\n\n   0  Sigmoid \n\n- ReLU\n\n  $f(x) = \\max(0, x)$ \n\n  ReLU\n\n  1. \n  2. \n  3. 0  1\n  4.  ReLU die 0\n\n- Leaky-ReLUP-ReLUR-ReLUMaxout\n\n   wiki ReLU \n  dying ReLU\n  \n  No free lunch theoren\n\n  ****\n\n  Maxout  Maxout  Maxout  ReLU  ReLU  Dropout \n\n\n\n# \n\n\n\n ReLU \ndying ReLU Leaky ReLUPReLU  Maxout\n\n Sigmoid tanh\n\n","slug":"thinking-from-softmax-03","published":1,"updated":"2021-07-19T13:03:31.699Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckwrktivi000mqxt6qboj1gfp","content":"<p> Logistic  Sigmoid <br> Sigmoid  S  Sigmoid  Logistic <br> $\\sigma(x)$ </p>\n<script type=\"math/tex; mode=display\">\n\\sigma(x) = \\frac{1}{1+e^{-x}}</script><p></p>\n<ul>\n<li></li>\n<li> $[-\\infty, +\\infty]$ </li>\n<li> $(0, 1)$  $(0, 1)$ </li>\n</ul>\n<p></p>\n<p></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p><br><br>neural nets<br>activation function</p>\n<p></p>\n<ul>\n<li></li>\n<li></li>\n<li>learning rate</li>\n</ul>\n<p> $f(x) = x$  Sigmoid </p>\n<h1 id=\"Sigmoid-\"><a href=\"#Sigmoid-\" class=\"headerlink\" title=\"Sigmoid \"></a>Sigmoid </h1><p> Sigmoid saturation 0 kill gradient<br></p>\n<script type=\"math/tex; mode=display\">\n\\sigma(x)' = \\sigma(x)(1 - \\sigma(x))</script><p>Sigmoid  $1/4$  0 </p>\n<p>Sigmoid  0  1<br>batch kill gradient </p>\n<p><br>feature<br> Dropout random forest Bagging </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><ul>\n<li><p>tanh</p>\n<p> 0  Sigmoid </p>\n</li>\n<li><p>ReLU</p>\n<p>$f(x) = \\max(0, x)$ </p>\n<p>ReLU</p>\n<ol>\n<li></li>\n<li></li>\n<li>0  1</li>\n<li> ReLU die 0</li>\n</ol>\n</li>\n<li><p>Leaky-ReLUP-ReLUR-ReLUMaxout</p>\n<p> wiki ReLU <br>dying ReLU<br><br>No free lunch theoren</p>\n<p><strong></strong></p>\n<p>Maxout  Maxout  Maxout  ReLU  ReLU  Dropout </p>\n</li>\n</ul>\n<p></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n<p> ReLU <br>dying ReLU Leaky ReLUPReLU  Maxout</p>\n<p> Sigmoid tanh</p>\n<p></p>\n","site":{"data":{}},"excerpt":"","more":"<p> Logistic  Sigmoid <br> Sigmoid  S  Sigmoid  Logistic <br> $\\sigma(x)$ </p>\n<script type=\"math/tex; mode=display\">\n\\sigma(x) = \\frac{1}{1+e^{-x}}</script><p></p>\n<ul>\n<li></li>\n<li> $[-\\infty, +\\infty]$ </li>\n<li> $(0, 1)$  $(0, 1)$ </li>\n</ul>\n<p></p>\n<p></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p><br><br>neural nets<br>activation function</p>\n<p></p>\n<ul>\n<li></li>\n<li></li>\n<li>learning rate</li>\n</ul>\n<p> $f(x) = x$  Sigmoid </p>\n<h1 id=\"Sigmoid-\"><a href=\"#Sigmoid-\" class=\"headerlink\" title=\"Sigmoid \"></a>Sigmoid </h1><p> Sigmoid saturation 0 kill gradient<br></p>\n<script type=\"math/tex; mode=display\">\n\\sigma(x)' = \\sigma(x)(1 - \\sigma(x))</script><p>Sigmoid  $1/4$  0 </p>\n<p>Sigmoid  0  1<br>batch kill gradient </p>\n<p><br>feature<br> Dropout random forest Bagging </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><ul>\n<li><p>tanh</p>\n<p> 0  Sigmoid </p>\n</li>\n<li><p>ReLU</p>\n<p>$f(x) = \\max(0, x)$ </p>\n<p>ReLU</p>\n<ol>\n<li></li>\n<li></li>\n<li>0  1</li>\n<li> ReLU die 0</li>\n</ol>\n</li>\n<li><p>Leaky-ReLUP-ReLUR-ReLUMaxout</p>\n<p> wiki ReLU <br>dying ReLU<br><br>No free lunch theoren</p>\n<p><strong></strong></p>\n<p>Maxout  Maxout  Maxout  ReLU  ReLU  Dropout </p>\n</li>\n</ul>\n<p></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n<p> ReLU <br>dying ReLU Leaky ReLUPReLU  Maxout</p>\n<p> Sigmoid tanh</p>\n<p></p>\n"},{"title":" Softmax  Logistic  Loss ","date":"2017-08-10T14:18:09.000Z","description":"   Softmax  Logistic  Loss ","_content":"\n# Logistic  Loss \n\nLogistic  Loss \n\n$$\n\\begin{aligned}\nLoss(h_{\\theta}, \\, y) &=\\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{2}[h_{\\theta}(x_{i})-y_{i}]^{2} \\\\\n& =\\frac{1}{2}\\left(\\frac{1}{1+e^{-\\theta^{\\mathsf{T}}x}}-y\\right)^{2}\n\\end{aligned}\n$$\n $1/2$ \n$n$ \n\n\n Loss \n$$\nLoss = \n\\left \\{ \\begin{aligned}\n& -\\log (h_{\\theta}(x)) & \\qquad \\text{if }y=1 \\\\\n& -\\log (1-h_{\\theta}(x)) & \\qquad \\text{if }y=0\n\\end{aligned} \\right.\n$$\n\n\n****\n Loss  $-\\log$ \n\n#   \n\n\nconvex optimization\n\nSVM  Perceptron\n\n\n\ngradient descentGD\n$f(y) \\ge f(x) + \\nabla f(x)^{\\mathsf{T}}(y-x)$  $f$ \n$\\nabla^{2}f(x) \\ge 0$ \n\n\n $y=|x|$ \nneural netNNactivation FunctionReLU\n\nsubgradient\n $g$ $f(y) \\ge f(x) + g^{\\mathsf{T}}(y-x)$ \n\n $y=|x|$  0ReLU $1/2$ \n\n\n{% asset_img subgradient.png Subgradient %}\n\n [](https://baike.baidu.com/item//13882223?fr=aladdin)\n\n# Softmax  Loss  Log-Sum-Exp \n\n wiki\n Logistic  Loss  $\\log$ $\\exp$ \n\n\n\n $f(x) = \\log (\\sum \\alpha_{i} e^{x_{i}})$  Affine composition  Log-Sum-Exp \nAffine composition $f(x)$  $\\Leftrightarrow f(Ax+b)$ \n\n\n$$\n\\begin{aligned}\n\\nabla_{i} f & = e^{x_{i}}\\Big/\\sum_{k} e^{x_{k}}  & \\\\\n\\nabla_{i}^{2} f & = \\mathrm{diag}(z)-zz^{\\mathsf{T}}, \\, z_{i} = e^{x_{i}}\\Big/\\sum_{k} e^{x_{k}}\n\\end{aligned}\n$$\n$\\nabla_{i}^{2} f$ diagonally dominant matrix\n $A$  $A_{ii} \\ge \\sum_{i \\ne j} |A_{ij}|$ \n$\\nabla_{i}^{2} f$ positive semidefinite matrixp.s.d\n Log-Sum-Exp ","source":"_posts/thinking-from-softmax-02.md","raw":"---\ntitle:  Softmax  Logistic  Loss \ndate: 2017-08-10 22:18:09\ncategories: ML\ntags: \n     - Logistic\n     - Softmax\n     - Convex optimization\n     - Gradient\n     - Subgradient\ndescription:    Softmax  Logistic  Loss \n---\n\n# Logistic  Loss \n\nLogistic  Loss \n\n$$\n\\begin{aligned}\nLoss(h_{\\theta}, \\, y) &=\\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{2}[h_{\\theta}(x_{i})-y_{i}]^{2} \\\\\n& =\\frac{1}{2}\\left(\\frac{1}{1+e^{-\\theta^{\\mathsf{T}}x}}-y\\right)^{2}\n\\end{aligned}\n$$\n $1/2$ \n$n$ \n\n\n Loss \n$$\nLoss = \n\\left \\{ \\begin{aligned}\n& -\\log (h_{\\theta}(x)) & \\qquad \\text{if }y=1 \\\\\n& -\\log (1-h_{\\theta}(x)) & \\qquad \\text{if }y=0\n\\end{aligned} \\right.\n$$\n\n\n****\n Loss  $-\\log$ \n\n#   \n\n\nconvex optimization\n\nSVM  Perceptron\n\n\n\ngradient descentGD\n$f(y) \\ge f(x) + \\nabla f(x)^{\\mathsf{T}}(y-x)$  $f$ \n$\\nabla^{2}f(x) \\ge 0$ \n\n\n $y=|x|$ \nneural netNNactivation FunctionReLU\n\nsubgradient\n $g$ $f(y) \\ge f(x) + g^{\\mathsf{T}}(y-x)$ \n\n $y=|x|$  0ReLU $1/2$ \n\n\n{% asset_img subgradient.png Subgradient %}\n\n [](https://baike.baidu.com/item//13882223?fr=aladdin)\n\n# Softmax  Loss  Log-Sum-Exp \n\n wiki\n Logistic  Loss  $\\log$ $\\exp$ \n\n\n\n $f(x) = \\log (\\sum \\alpha_{i} e^{x_{i}})$  Affine composition  Log-Sum-Exp \nAffine composition $f(x)$  $\\Leftrightarrow f(Ax+b)$ \n\n\n$$\n\\begin{aligned}\n\\nabla_{i} f & = e^{x_{i}}\\Big/\\sum_{k} e^{x_{k}}  & \\\\\n\\nabla_{i}^{2} f & = \\mathrm{diag}(z)-zz^{\\mathsf{T}}, \\, z_{i} = e^{x_{i}}\\Big/\\sum_{k} e^{x_{k}}\n\\end{aligned}\n$$\n$\\nabla_{i}^{2} f$ diagonally dominant matrix\n $A$  $A_{ii} \\ge \\sum_{i \\ne j} |A_{ij}|$ \n$\\nabla_{i}^{2} f$ positive semidefinite matrixp.s.d\n Log-Sum-Exp ","slug":"thinking-from-softmax-02","published":1,"updated":"2021-07-21T12:39:17.075Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckwrktivj000nqxt6n4qyrcij","content":"<h1 id=\"Logistic--Loss-\"><a href=\"#Logistic--Loss-\" class=\"headerlink\" title=\"Logistic  Loss \"></a>Logistic  Loss </h1><p>Logistic  Loss <br></p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nLoss(h_{\\theta}, \\, y) &=\\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{2}[h_{\\theta}(x_{i})-y_{i}]^{2} \\\\\n& =\\frac{1}{2}\\left(\\frac{1}{1+e^{-\\theta^{\\mathsf{T}}x}}-y\\right)^{2}\n\\end{aligned}</script><p> $1/2$ <br>$n$ <br></p>\n<p> Loss </p>\n<script type=\"math/tex; mode=display\">\nLoss = \n\\left \\{ \\begin{aligned}\n& -\\log (h_{\\theta}(x)) & \\qquad \\text{if }y=1 \\\\\n& -\\log (1-h_{\\theta}(x)) & \\qquad \\text{if }y=0\n\\end{aligned} \\right.</script><p></p>\n<p><strong></strong><br> Loss  $-\\log$ </p>\n<h1 id=\"--\"><a href=\"#--\" class=\"headerlink\" title=\"  \"></a>  </h1><p><br>convex optimization</p>\n<p>SVM  Perceptron<br><br></p>\n<p>gradient descentGD<br>$f(y) \\ge f(x) + \\nabla f(x)^{\\mathsf{T}}(y-x)$  $f$ <br>$\\nabla^{2}f(x) \\ge 0$ </p>\n<p><br> $y=|x|$ <br>neural netNNactivation FunctionReLU</p>\n<p>subgradient<br> $g$ $f(y) \\ge f(x) + g^{\\mathsf{T}}(y-x)$ <br><br> $y=|x|$  0ReLU $1/2$ <br></p>\n<img src=\"/2017/08/10/thinking-from-softmax-02/subgradient.png\" title=\"Subgradient\">\n<p> <a href=\"https://baike.baidu.com/item//13882223?fr=aladdin\" target=\"_blank\" rel=\"noopener\"></a></p>\n<h1 id=\"Softmax--Loss--Log-Sum-Exp-\"><a href=\"#Softmax--Loss--Log-Sum-Exp-\" class=\"headerlink\" title=\"Softmax  Loss  Log-Sum-Exp \"></a>Softmax  Loss  Log-Sum-Exp </h1><p> wiki<br> Logistic  Loss  $\\log$ $\\exp$ </p>\n<p></p>\n<p> $f(x) = \\log (\\sum \\alpha_{i} e^{x_{i}})$  Affine composition  Log-Sum-Exp <br>Affine composition $f(x)$  $\\Leftrightarrow f(Ax+b)$ </p>\n<p></p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\nabla_{i} f & = e^{x_{i}}\\Big/\\sum_{k} e^{x_{k}}  & \\\\\n\\nabla_{i}^{2} f & = \\mathrm{diag}(z)-zz^{\\mathsf{T}}, \\, z_{i} = e^{x_{i}}\\Big/\\sum_{k} e^{x_{k}}\n\\end{aligned}</script><p>$\\nabla_{i}^{2} f$ diagonally dominant matrix<br> $A$  $A_{ii} \\ge \\sum_{i \\ne j} |A_{ij}|$ <br>$\\nabla_{i}^{2} f$ positive semidefinite matrixp.s.d<br> Log-Sum-Exp </p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Logistic--Loss-\"><a href=\"#Logistic--Loss-\" class=\"headerlink\" title=\"Logistic  Loss \"></a>Logistic  Loss </h1><p>Logistic  Loss <br></p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nLoss(h_{\\theta}, \\, y) &=\\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{2}[h_{\\theta}(x_{i})-y_{i}]^{2} \\\\\n& =\\frac{1}{2}\\left(\\frac{1}{1+e^{-\\theta^{\\mathsf{T}}x}}-y\\right)^{2}\n\\end{aligned}</script><p> $1/2$ <br>$n$ <br></p>\n<p> Loss </p>\n<script type=\"math/tex; mode=display\">\nLoss = \n\\left \\{ \\begin{aligned}\n& -\\log (h_{\\theta}(x)) & \\qquad \\text{if }y=1 \\\\\n& -\\log (1-h_{\\theta}(x)) & \\qquad \\text{if }y=0\n\\end{aligned} \\right.</script><p></p>\n<p><strong></strong><br> Loss  $-\\log$ </p>\n<h1 id=\"--\"><a href=\"#--\" class=\"headerlink\" title=\"  \"></a>  </h1><p><br>convex optimization</p>\n<p>SVM  Perceptron<br><br></p>\n<p>gradient descentGD<br>$f(y) \\ge f(x) + \\nabla f(x)^{\\mathsf{T}}(y-x)$  $f$ <br>$\\nabla^{2}f(x) \\ge 0$ </p>\n<p><br> $y=|x|$ <br>neural netNNactivation FunctionReLU</p>\n<p>subgradient<br> $g$ $f(y) \\ge f(x) + g^{\\mathsf{T}}(y-x)$ <br><br> $y=|x|$  0ReLU $1/2$ <br></p>\n<img src=\"/2017/08/10/thinking-from-softmax-02/subgradient.png\" title=\"Subgradient\">\n<p> <a href=\"https://baike.baidu.com/item//13882223?fr=aladdin\" target=\"_blank\" rel=\"noopener\"></a></p>\n<h1 id=\"Softmax--Loss--Log-Sum-Exp-\"><a href=\"#Softmax--Loss--Log-Sum-Exp-\" class=\"headerlink\" title=\"Softmax  Loss  Log-Sum-Exp \"></a>Softmax  Loss  Log-Sum-Exp </h1><p> wiki<br> Logistic  Loss  $\\log$ $\\exp$ </p>\n<p></p>\n<p> $f(x) = \\log (\\sum \\alpha_{i} e^{x_{i}})$  Affine composition  Log-Sum-Exp <br>Affine composition $f(x)$  $\\Leftrightarrow f(Ax+b)$ </p>\n<p></p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\nabla_{i} f & = e^{x_{i}}\\Big/\\sum_{k} e^{x_{k}}  & \\\\\n\\nabla_{i}^{2} f & = \\mathrm{diag}(z)-zz^{\\mathsf{T}}, \\, z_{i} = e^{x_{i}}\\Big/\\sum_{k} e^{x_{k}}\n\\end{aligned}</script><p>$\\nabla_{i}^{2} f$ diagonally dominant matrix<br> $A$  $A_{ii} \\ge \\sum_{i \\ne j} |A_{ij}|$ <br>$\\nabla_{i}^{2} f$ positive semidefinite matrixp.s.d<br> Log-Sum-Exp </p>\n"}],"PostAsset":[{"_id":"source/_posts/about-kernel-02/linear_space.png","slug":"linear_space.png","post":"ckwrktivg000hqxt6ns6fqudw","modified":0,"renderable":0},{"_id":"source/_posts/thinking-from-softmax-02/subgradient.png","slug":"subgradient.png","post":"ckwrktivj000nqxt6n4qyrcij","modified":0,"renderable":0}],"PostCategory":[{"post_id":"ckwrktiv30001qxt6wl19k31f","category_id":"ckwrktiyx000oqxt67y30h1ok","_id":"ckwrktj0f002mqxt6kulcj146"},{"post_id":"ckwrktiv60003qxt6hkkafdl8","category_id":"ckwrktiz4000qqxt6lt5mcuhc","_id":"ckwrktj0g002oqxt6q60nihf8"},{"post_id":"ckwrktiv70004qxt6ceoapanx","category_id":"ckwrktiz4000qqxt6lt5mcuhc","_id":"ckwrktj0g002qqxt6n8spepox"},{"post_id":"ckwrktiv80005qxt6q11ojijw","category_id":"ckwrktiz4000qqxt6lt5mcuhc","_id":"ckwrktj0g002sqxt6z41pmz26"},{"post_id":"ckwrktiv80006qxt6bf7p7wb0","category_id":"ckwrktiz4000qqxt6lt5mcuhc","_id":"ckwrktj0g002uqxt6jmqbuzti"},{"post_id":"ckwrktiv90007qxt6yf4w573m","category_id":"ckwrktiyx000oqxt67y30h1ok","_id":"ckwrktj0g002wqxt6kjg4cg4x"},{"post_id":"ckwrktiva0008qxt6d5wsigmj","category_id":"ckwrktiz4000qqxt6lt5mcuhc","_id":"ckwrktj0g002yqxt6ft2cbmre"},{"post_id":"ckwrktiva0009qxt6bpod90eh","category_id":"ckwrktiz4000qqxt6lt5mcuhc","_id":"ckwrktj0h0030qxt6ld18wgb0"},{"post_id":"ckwrktivb000aqxt6iflya024","category_id":"ckwrktiz4000qqxt6lt5mcuhc","_id":"ckwrktj0h0032qxt6xg6a1kpk"},{"post_id":"ckwrktivc000bqxt6rmriycof","category_id":"ckwrktiz4000qqxt6lt5mcuhc","_id":"ckwrktj0h0034qxt64o2o2c92"},{"post_id":"ckwrktivd000cqxt6tjtqoylw","category_id":"ckwrktiyx000oqxt67y30h1ok","_id":"ckwrktj0h0036qxt6a0ci1q7p"},{"post_id":"ckwrktive000dqxt6y2wq3vso","category_id":"ckwrktiz4000qqxt6lt5mcuhc","_id":"ckwrktj0h0038qxt6t4b9ifzp"},{"post_id":"ckwrktivf000eqxt6fxi9az8z","category_id":"ckwrktiz4000qqxt6lt5mcuhc","_id":"ckwrktj0h003aqxt6culsrugf"},{"post_id":"ckwrktivf000fqxt6livwxo6n","category_id":"ckwrktizc001eqxt6kmyi94x1","_id":"ckwrktj0h003cqxt6ajoihq9m"},{"post_id":"ckwrktivg000gqxt66mnw2k4u","category_id":"ckwrktiz4000qqxt6lt5mcuhc","_id":"ckwrktj0i003eqxt6w07b0dun"},{"post_id":"ckwrktivg000hqxt6ns6fqudw","category_id":"ckwrktiz4000qqxt6lt5mcuhc","_id":"ckwrktj0i003gqxt616pw3i1p"},{"post_id":"ckwrktivh000iqxt6qn87def1","category_id":"ckwrktiz4000qqxt6lt5mcuhc","_id":"ckwrktj0i003iqxt6vfjwcbxi"},{"post_id":"ckwrktivh000jqxt6azgq5ni1","category_id":"ckwrktiz4000qqxt6lt5mcuhc","_id":"ckwrktj0i003kqxt6ngonobq6"},{"post_id":"ckwrktivi000kqxt6u0339qal","category_id":"ckwrktize001nqxt6j88gtts3","_id":"ckwrktj0i003mqxt67nniz0l4"},{"post_id":"ckwrktivi000lqxt60e8fbxb4","category_id":"ckwrktiz4000qqxt6lt5mcuhc","_id":"ckwrktj0i003oqxt6d02sgqjy"},{"post_id":"ckwrktivi000mqxt6qboj1gfp","category_id":"ckwrktiz4000qqxt6lt5mcuhc","_id":"ckwrktj0i003qqxt68nri4oky"},{"post_id":"ckwrktivj000nqxt6n4qyrcij","category_id":"ckwrktiz4000qqxt6lt5mcuhc","_id":"ckwrktj0j003sqxt6y4bt1c3e"}],"PostTag":[{"post_id":"ckwrktiv30001qxt6wl19k31f","tag_id":"ckwrktiz3000pqxt63c5uah7z","_id":"ckwrktj0f002lqxt6p2r3cscm"},{"post_id":"ckwrktiv60003qxt6hkkafdl8","tag_id":"ckwrktiz4000rqxt6lxuj61wa","_id":"ckwrktj0g002nqxt6mvudkqy6"},{"post_id":"ckwrktiv60003qxt6hkkafdl8","tag_id":"ckwrktiz4000tqxt6aiwqca4q","_id":"ckwrktj0g002pqxt6l8r34gwq"},{"post_id":"ckwrktiv70004qxt6ceoapanx","tag_id":"ckwrktiz4000rqxt6lxuj61wa","_id":"ckwrktj0g002rqxt61hyl6rr5"},{"post_id":"ckwrktiv70004qxt6ceoapanx","tag_id":"ckwrktiz4000tqxt6aiwqca4q","_id":"ckwrktj0g002tqxt6xkab4oms"},{"post_id":"ckwrktiv80005qxt6q11ojijw","tag_id":"ckwrktiz7000zqxt65cqh7kpt","_id":"ckwrktj0g002vqxt6y5h4sdbp"},{"post_id":"ckwrktiv80005qxt6q11ojijw","tag_id":"ckwrktiz80011qxt6dgazljpq","_id":"ckwrktj0g002xqxt6l1cb9qtr"},{"post_id":"ckwrktiv80006qxt6bf7p7wb0","tag_id":"ckwrktiz90013qxt6w7jtjv5n","_id":"ckwrktj0h002zqxt6kdbgghva"},{"post_id":"ckwrktiv80006qxt6bf7p7wb0","tag_id":"ckwrktiz90015qxt67lfh5zuy","_id":"ckwrktj0h0031qxt6xo2k77q2"},{"post_id":"ckwrktiv80006qxt6bf7p7wb0","tag_id":"ckwrktiza0017qxt60y75snzu","_id":"ckwrktj0h0033qxt61eidjqhj"},{"post_id":"ckwrktiv90007qxt6yf4w573m","tag_id":"ckwrktiz3000pqxt63c5uah7z","_id":"ckwrktj0h0035qxt6f0853xqm"},{"post_id":"ckwrktiva0008qxt6d5wsigmj","tag_id":"ckwrktiz4000rqxt6lxuj61wa","_id":"ckwrktj0h0037qxt6iumln12e"},{"post_id":"ckwrktiva0008qxt6d5wsigmj","tag_id":"ckwrktizc001dqxt6yd7l7qon","_id":"ckwrktj0h0039qxt6c1m85098"},{"post_id":"ckwrktiva0009qxt6bpod90eh","tag_id":"ckwrktizc001fqxt6s8qmnm9e","_id":"ckwrktj0h003bqxt62qbs05h7"},{"post_id":"ckwrktiva0009qxt6bpod90eh","tag_id":"ckwrktizd001hqxt68k0s9xbf","_id":"ckwrktj0i003dqxt6pe4jii4e"},{"post_id":"ckwrktivb000aqxt6iflya024","tag_id":"ckwrktizd001kqxt6jgq8xvl9","_id":"ckwrktj0i003fqxt6s3bk3bpj"},{"post_id":"ckwrktivb000aqxt6iflya024","tag_id":"ckwrktize001mqxt6ajbyk5mi","_id":"ckwrktj0i003hqxt62yr1f9bi"},{"post_id":"ckwrktivc000bqxt6rmriycof","tag_id":"ckwrktizf001oqxt6uc8bsut3","_id":"ckwrktj0i003jqxt6ly8d9ogd"},{"post_id":"ckwrktivc000bqxt6rmriycof","tag_id":"ckwrktizf001qqxt6a5oymae8","_id":"ckwrktj0i003lqxt64trdwwlh"},{"post_id":"ckwrktivc000bqxt6rmriycof","tag_id":"ckwrktizg001sqxt67cn35n1s","_id":"ckwrktj0i003nqxt6kwp8beyn"},{"post_id":"ckwrktive000dqxt6y2wq3vso","tag_id":"ckwrktizg001uqxt6lrqcwr7d","_id":"ckwrktj0i003pqxt67i0kxrv2"},{"post_id":"ckwrktive000dqxt6y2wq3vso","tag_id":"ckwrktizh001vqxt6tzke8zwc","_id":"ckwrktj0j003rqxt6wsnc3smq"},{"post_id":"ckwrktive000dqxt6y2wq3vso","tag_id":"ckwrktizh001wqxt66gok9sen","_id":"ckwrktj0k003tqxt63wgppan3"},{"post_id":"ckwrktivf000eqxt6fxi9az8z","tag_id":"ckwrktizh001xqxt65416tcg4","_id":"ckwrktj0k003uqxt6k8yupve8"},{"post_id":"ckwrktivf000eqxt6fxi9az8z","tag_id":"ckwrktizh001yqxt685mz31w1","_id":"ckwrktj0k003vqxt6jtt203pp"},{"post_id":"ckwrktivf000fqxt6livwxo6n","tag_id":"ckwrktizi001zqxt6e5lzwq03","_id":"ckwrktj0k003wqxt6hy5bwvzd"},{"post_id":"ckwrktivg000gqxt66mnw2k4u","tag_id":"ckwrktizh001xqxt65416tcg4","_id":"ckwrktj0k003xqxt60vdkrvp0"},{"post_id":"ckwrktivg000gqxt66mnw2k4u","tag_id":"ckwrktizi0021qxt6eh5tez5x","_id":"ckwrktj0k003yqxt60i5bc2j8"},{"post_id":"ckwrktivg000hqxt6ns6fqudw","tag_id":"ckwrktiz90013qxt6w7jtjv5n","_id":"ckwrktj0k003zqxt64swbb2ip"},{"post_id":"ckwrktivg000hqxt6ns6fqudw","tag_id":"ckwrktiza0017qxt60y75snzu","_id":"ckwrktj0k0040qxt6orczmsxg"},{"post_id":"ckwrktivg000hqxt6ns6fqudw","tag_id":"ckwrktizk0024qxt6saswmsz8","_id":"ckwrktj0k0041qxt6mufd3oh2"},{"post_id":"ckwrktivh000iqxt6qn87def1","tag_id":"ckwrktizk0025qxt6zscreet0","_id":"ckwrktj0k0042qxt65olz7ejy"},{"post_id":"ckwrktivh000jqxt6azgq5ni1","tag_id":"ckwrktizk0026qxt6gmpbr7dm","_id":"ckwrktj0l0043qxt60ld3ueen"},{"post_id":"ckwrktivh000jqxt6azgq5ni1","tag_id":"ckwrktizk0027qxt6hjcrosdc","_id":"ckwrktj0l0044qxt6s6z2oya8"},{"post_id":"ckwrktivh000jqxt6azgq5ni1","tag_id":"ckwrktizl0028qxt60n2hc2ut","_id":"ckwrktj0l0045qxt6ahtiv24u"},{"post_id":"ckwrktivh000jqxt6azgq5ni1","tag_id":"ckwrktizl0029qxt6ex7iizex","_id":"ckwrktj0l0046qxt66ydq17b8"},{"post_id":"ckwrktivi000kqxt6u0339qal","tag_id":"ckwrktizl002aqxt6q7bscsy7","_id":"ckwrktj0l0047qxt6hyetkk5l"},{"post_id":"ckwrktivi000lqxt60e8fbxb4","tag_id":"ckwrktizh001xqxt65416tcg4","_id":"ckwrktj0l0048qxt6r5q54jyf"},{"post_id":"ckwrktivi000lqxt60e8fbxb4","tag_id":"ckwrktizh001yqxt685mz31w1","_id":"ckwrktj0l0049qxt6q38k8jom"},{"post_id":"ckwrktivi000lqxt60e8fbxb4","tag_id":"ckwrktizi0021qxt6eh5tez5x","_id":"ckwrktj0l004aqxt617avtjij"},{"post_id":"ckwrktivi000mqxt6qboj1gfp","tag_id":"ckwrktiz4000rqxt6lxuj61wa","_id":"ckwrktj0l004bqxt6pzhulbip"},{"post_id":"ckwrktivi000mqxt6qboj1gfp","tag_id":"ckwrktizn002fqxt6lgbihn0v","_id":"ckwrktj0m004cqxt6mjs2ofod"},{"post_id":"ckwrktivj000nqxt6n4qyrcij","tag_id":"ckwrktizl0028qxt60n2hc2ut","_id":"ckwrktj0m004dqxt6wlkg5swk"},{"post_id":"ckwrktivj000nqxt6n4qyrcij","tag_id":"ckwrktizl0029qxt6ex7iizex","_id":"ckwrktj0m004eqxt6ubz5ssas"},{"post_id":"ckwrktivj000nqxt6n4qyrcij","tag_id":"ckwrktizh001yqxt685mz31w1","_id":"ckwrktj0m004fqxt6lp4ayu8a"},{"post_id":"ckwrktivj000nqxt6n4qyrcij","tag_id":"ckwrktizo002jqxt6gbvfjvjz","_id":"ckwrktj0m004gqxt6fptyidux"},{"post_id":"ckwrktivj000nqxt6n4qyrcij","tag_id":"ckwrktizq002kqxt6z2rqjs8t","_id":"ckwrktj0m004hqxt6zu29w0z1"}],"Tag":[{"name":"Tips","_id":"ckwrktiz3000pqxt63c5uah7z"},{"name":"Deep learning","_id":"ckwrktiz4000rqxt6lxuj61wa"},{"name":"GAN","_id":"ckwrktiz4000tqxt6aiwqca4q"},{"name":"GD","_id":"ckwrktiz7000zqxt65cqh7kpt"},{"name":"Momentum","_id":"ckwrktiz80011qxt6dgazljpq"},{"name":"Kernel","_id":"ckwrktiz90013qxt6w7jtjv5n"},{"name":"SVM","_id":"ckwrktiz90015qxt67lfh5zuy"},{"name":"RKHS","_id":"ckwrktiza0017qxt60y75snzu"},{"name":"Loss function","_id":"ckwrktizc001dqxt6yd7l7qon"},{"name":"PCA","_id":"ckwrktizc001fqxt6s8qmnm9e"},{"name":"LFM","_id":"ckwrktizd001hqxt68k0s9xbf"},{"name":"Discriminative model","_id":"ckwrktizd001kqxt6jgq8xvl9"},{"name":"Generative model","_id":"ckwrktize001mqxt6ajbyk5mi"},{"name":"Ensemble","_id":"ckwrktizf001oqxt6uc8bsut3"},{"name":"Boost","_id":"ckwrktizf001qqxt6a5oymae8"},{"name":"GBDT/GBRT","_id":"ckwrktizg001sqxt67cn35n1s"},{"name":"K-means","_id":"ckwrktizg001uqxt6lrqcwr7d"},{"name":"GMM","_id":"ckwrktizh001vqxt6tzke8zwc"},{"name":"EM Algorithm","_id":"ckwrktizh001wqxt66gok9sen"},{"name":"Norm regularization","_id":"ckwrktizh001xqxt65416tcg4"},{"name":"Convex optimization","_id":"ckwrktizh001yqxt685mz31w1"},{"name":"GC","_id":"ckwrktizi001zqxt6e5lzwq03"},{"name":"Matrix theory","_id":"ckwrktizi0021qxt6eh5tez5x"},{"name":"Functional analysis","_id":"ckwrktizk0024qxt6saswmsz8"},{"name":"Rating","_id":"ckwrktizk0025qxt6zscreet0"},{"name":"Generalized Linear Model","_id":"ckwrktizk0026qxt6gmpbr7dm"},{"name":"Exponential family distribution","_id":"ckwrktizk0027qxt6hjcrosdc"},{"name":"Logistic","_id":"ckwrktizl0028qxt60n2hc2ut"},{"name":"Softmax","_id":"ckwrktizl0029qxt6ex7iizex"},{"name":"Statistics","_id":"ckwrktizl002aqxt6q7bscsy7"},{"name":"Activation function","_id":"ckwrktizn002fqxt6lgbihn0v"},{"name":"Gradient","_id":"ckwrktizo002jqxt6gbvfjvjz"},{"name":"Subgradient","_id":"ckwrktizq002kqxt6z2rqjs8t"}]}}