{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/CNAME","path":"CNAME","modified":1,"renderable":0},{"_id":"source/README.md","path":"README.md","modified":1,"renderable":0},{"_id":"source/baidu_verify_aX3upgcPux.html","path":"baidu_verify_aX3upgcPux.html","modified":1,"renderable":0},{"_id":"source/robots.txt","path":"robots.txt","modified":1,"renderable":0},{"_id":"source/images/linear_space.eddx","path":"images/linear_space.eddx","modified":1,"renderable":0},{"_id":"source/uploads/avatar.png","path":"uploads/avatar.png","modified":1,"renderable":0},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","path":"lib/canvas-ribbon/canvas-ribbon.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/LICENSE","path":"lib/fastclick/LICENSE","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/README.md","path":"lib/fastclick/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/bower.json","path":"lib/fastclick/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","path":"lib/jquery_lazyload/CONTRIBUTING.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","path":"lib/jquery_lazyload/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","path":"lib/jquery_lazyload/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","path":"lib/jquery_lazyload/jquery.lazyload.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","path":"lib/jquery_lazyload/jquery.scrollstop.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","path":"lib/pace/pace-theme-barber-shop.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","path":"lib/pace/pace-theme-big-counter.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","path":"lib/pace/pace-theme-bounce.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","path":"lib/pace/pace-theme-center-atom.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","path":"lib/pace/pace-theme-center-circle.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","path":"lib/pace/pace-theme-center-radar.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","path":"lib/pace/pace-theme-center-simple.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","path":"lib/pace/pace-theme-corner-indicator.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","path":"lib/pace/pace-theme-fill-left.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","path":"lib/pace/pace-theme-flash.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","path":"lib/pace/pace-theme-loading-bar.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","path":"lib/pace/pace-theme-mac-osx.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","path":"lib/pace/pace-theme-minimal.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace.min.js","path":"lib/pace/pace.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","path":"lib/three/canvas_lines.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","path":"lib/three/canvas_sphere.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three-waves.min.js","path":"lib/three/three-waves.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/bower.json","path":"lib/velocity/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.js","path":"lib/Han/dist/han.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.css","path":"lib/Han/dist/han.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","path":"lib/fancybox/source/blank.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","path":"lib/fancybox/source/fancybox_loading@2x.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","path":"lib/fancybox/source/fancybox_loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","path":"lib/fancybox/source/fancybox_sprite.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","path":"lib/fancybox/source/fancybox_overlay.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","path":"lib/fancybox/source/fancybox_sprite@2x.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","path":"lib/fastclick/lib/fastclick.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","path":"lib/fastclick/lib/fastclick.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.css","path":"lib/Han/dist/han.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","path":"lib/Han/dist/font/han-space.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","path":"lib/Han/dist/font/han-space.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","path":"lib/Han/dist/font/han.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","path":"lib/Han/dist/font/han.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.js","path":"lib/Han/dist/han.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","path":"lib/fancybox/source/helpers/fancybox_buttons.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","path":"lib/fancybox/source/helpers/jquery.fancybox-media.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three.min.js","path":"lib/three/three.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":1,"renderable":1}],"Cache":[{"_id":"themes/next/.bowerrc","hash":"3228a58ed0ece9f85e1e3136352094080b8dece1","modified":1626699811721},{"_id":"themes/next/.gitignore","hash":"32ea93f21d8693d5d8fa4eef1c51a21ad0670047","modified":1626699811722},{"_id":"themes/next/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1626699811721},{"_id":"themes/next/.hound.yml","hash":"b76daa84c9ca3ad292c78412603370a367cc2bc3","modified":1626699811722},{"_id":"themes/next/.gitattributes","hash":"44bd4729c74ccb88110804f41746fec07bf487d4","modified":1626699811721},{"_id":"themes/next/.javascript_ignore","hash":"8a224b381155f10e6eb132a4d815c5b52962a9d1","modified":1626699811723},{"_id":"themes/next/.jshintrc","hash":"9928f81bd822f6a8d67fdbc909b517178533bca9","modified":1626699811723},{"_id":"themes/next/.stylintrc","hash":"b28e24704a5d8de08346c45286574c8e76cc109f","modified":1626699811723},{"_id":"themes/next/LICENSE","hash":"f293bcfcdc06c0b77ba13570bb8af55eb5c059fd","modified":1626699811723},{"_id":"themes/next/.travis.yml","hash":"c42d9608c8c7fe90de7b1581a8dc3886e90c179e","modified":1626699811723},{"_id":"themes/next/README.en.md","hash":"32d6cdfec1447f54aae1d7f1365ce6733dfcec8f","modified":1626699811723},{"_id":"themes/next/README.md","hash":"500b5606eb6a09c979d16128f8b00f4bf9bc95ac","modified":1626699811724},{"_id":"themes/next/_config.yml","hash":"9d65902c4cca23978cc237fa17e870235742c002","modified":1626699811724},{"_id":"themes/next/bower.json","hash":"7d7938f9da896fe710aa0e9120140e528bf058df","modified":1626699811724},{"_id":"themes/next/gulpfile.coffee","hash":"031bffc483e417b20e90eceb6cf358e7596d2e69","modified":1626699811724},{"_id":"themes/next/package.json","hash":"193dad6f59a588908fac082cc46fe067dac1b84d","modified":1626699811740},{"_id":"source/CNAME","hash":"685ff10641d5fffd71eec3979dab6fd0f10c9ea7","modified":1626699811695},{"_id":"source/README.md","hash":"876c607060f4a864761fd1d8cc982edba012f251","modified":1626699811695},{"_id":"source/baidu_verify_aX3upgcPux.html","hash":"344502cc0cb9646f0643ff932d5c7adb1438cd5f","modified":1626699811699},{"_id":"source/robots.txt","hash":"7858025995a22727d626cdda854f061df5ba8fb9","modified":1626699811699},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"3b5eafd32abb718e56ccf8d1cee0607ad8ce611d","modified":1626699811721},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"b56c01cdfc6ee7ffea8a8a9fa149263f368caef6","modified":1626699811722},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"37bd0ec1d655c601946fc5f5ac2fe8ed1e529b77","modified":1626699811722},{"_id":"themes/next/languages/de.yml","hash":"306db8c865630f32c6b6260ade9d3209fbec8011","modified":1626699811725},{"_id":"themes/next/languages/default.yml","hash":"4cc6aeb1ac09a58330e494c8771773758ab354af","modified":1626699811725},{"_id":"themes/next/languages/en.yml","hash":"e7def07a709ef55684490b700a06998c67f35f39","modified":1626699811725},{"_id":"themes/next/languages/fr-FR.yml","hash":"24180322c83587a153cea110e74e96eacc3355ad","modified":1626699811725},{"_id":"themes/next/languages/ja.yml","hash":"1c3a05ab80a6f8be63268b66da6f19da7aa2c638","modified":1626699811726},{"_id":"themes/next/languages/id.yml","hash":"2835ea80dadf093fcf47edd957680973f1fb6b85","modified":1626699811725},{"_id":"themes/next/languages/ko.yml","hash":"be150543379150f78329815af427bf152c0e9431","modified":1626699811726},{"_id":"themes/next/languages/pt-BR.yml","hash":"958e49571818a34fdf4af3232a07a024050f8f4e","modified":1626699811726},{"_id":"themes/next/languages/pt.yml","hash":"36c8f60dacbe5d27d84d0e0d6974d7679f928da0","modified":1626699811726},{"_id":"themes/next/languages/ru.yml","hash":"1549a7c2fe23caa7cbedcd0aa2b77c46e57caf27","modified":1626699811726},{"_id":"themes/next/languages/zh-Hans.yml","hash":"3c0c7dfd0256457ee24df9e9879226c58cb084b5","modified":1626699811726},{"_id":"themes/next/languages/zh-hk.yml","hash":"1c917997413bf566cb79e0975789f3c9c9128ccd","modified":1626699811726},{"_id":"themes/next/languages/zh-tw.yml","hash":"0b2c18aa76570364003c8d1cd429fa158ae89022","modified":1626699811726},{"_id":"themes/next/layout/_layout.swig","hash":"06b1eab2e00273e0b94bd32dc682bd92c1e0a747","modified":1626699811727},{"_id":"themes/next/layout/archive.swig","hash":"383f64deab105724fd5512371963bd9e9aafbffd","modified":1626699811738},{"_id":"themes/next/layout/category.swig","hash":"4472255f4a3e3dd6d79201523a9526dcabdfbf18","modified":1626699811739},{"_id":"themes/next/layout/index.swig","hash":"c3762a6028d8ca79f9a07e9520ef9c612e7e193c","modified":1626699811739},{"_id":"themes/next/layout/page.swig","hash":"37c874cd720acf0eda8d26e063278f2b6ae8d3a6","modified":1626699811739},{"_id":"themes/next/layout/post.swig","hash":"2d5f8d7f0a96b611e2d5a5e4d111fc17726a990f","modified":1626699811739},{"_id":"themes/next/layout/schedule.swig","hash":"d86f8de4e118f8c4d778b285c140474084a271db","modified":1626699811739},{"_id":"themes/next/layout/tag.swig","hash":"7e0a7d7d832883eddb1297483ad22c184e4368de","modified":1626699811740},{"_id":"themes/next/scripts/merge-configs.js","hash":"13c8b3a2d9fce06c2488820d9248d190c8100e0a","modified":1626699811740},{"_id":"themes/next/scripts/merge.js","hash":"9130dabe6a674c54b535f322b17d75fe6081472f","modified":1626699811740},{"_id":"themes/next/test/.jshintrc","hash":"19f93d13d1689fe033c82eb2d5f3ce30b6543cc0","modified":1626699811795},{"_id":"themes/next/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1626699811796},{"_id":"themes/next/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1626699811796},{"_id":"source/_posts/5-steps-for-asking-good-questions.md","hash":"efa55e28d49e4fdf225049831531d2d8c0b74b1a","modified":1626699811695},{"_id":"source/_posts/GAN-01.md","hash":"0e85b1c8c4c1ee9252b919c46446de5926c22535","modified":1626699811695},{"_id":"source/_posts/GAN-02.md","hash":"dcae32a27b5391dc22a6b24f17e9db14b291cfe7","modified":1626699811695},{"_id":"source/_posts/GD-Series.md","hash":"ea008fd53dff233ccbf385e91796d273525679a9","modified":1626699811695},{"_id":"source/_posts/about-kernel-01.md","hash":"2661ad052f1bc224bb37a3bdcc48db3caddfa4f9","modified":1626699811696},{"_id":"source/_posts/about-kernel-02.md","hash":"ff411ec92ac1358a4c7ffd34708e7e20f0f6ceaf","modified":1626699811696},{"_id":"source/_posts/alterable-vs-permanent.md","hash":"eabae0c31231edc870b3a7a2cb0e5e2b07a784bb","modified":1626699811696},{"_id":"source/_posts/common-ground-of-mse-and-cee-in-nn.md","hash":"49be8210766564efa21a4943cf3ed6c27cf391c5","modified":1626699811696},{"_id":"source/_posts/compare-between-PCA-and-LFM.md","hash":"9fc369ce6dcbee1c3277392a2a44b53168cc878e","modified":1626699811696},{"_id":"source/_posts/discriminative-or-generative-model.md","hash":"d46e0751660e31f94e5471485fef2b0512f72395","modified":1626699811696},{"_id":"source/_posts/from-boost-to-Adaboost-to-GBT.md","hash":"b5eed46a2eac5907759b948534af50b227c0ed77","modified":1626699811697},{"_id":"source/_posts/init-blog.md","hash":"a601cdb073a44c5a819e5e9670641223ff4c07cd","modified":1626699811697},{"_id":"source/_posts/norm-regularization-01.md","hash":"568be0ec8bcc3a353a03eeb1b9d2e09182c74d4c","modified":1626699811697},{"_id":"source/_posts/kmeans-is-a-gmm.md","hash":"badb8938705da8a504e13bbcebdc24cc25db6367","modified":1626699811697},{"_id":"source/_posts/norm-regularization-02.md","hash":"1a5c45319762c05f6feef23f6a4d03463f2534f3","modified":1626699811697},{"_id":"source/_posts/norm-regularization-appendix.md","hash":"d5e2422500ffdf93bec04b4e55169a173ab60637","modified":1626699811697},{"_id":"source/_posts/rating-model-considering-user-count.md","hash":"d6bc256e093c6f25312ff67d0e3821e3efaa0e30","modified":1626699811698},{"_id":"source/_posts/statistical-formulars-for-programmers.md","hash":"dbb6411bf38d79a79fffc99eeeae1474a94098db","modified":1626699811698},{"_id":"source/_posts/thinking-from-softmax-01.md","hash":"a29c7daef5bc9d2fba01ce29e79347d3713d008f","modified":1626699811698},{"_id":"source/_posts/thinking-from-softmax-02.md","hash":"864918414815b2cbb974e6d47d660d094acec0af","modified":1626699811698},{"_id":"source/_posts/thinking-from-softmax-03.md","hash":"eb3cde57768cb935b36ac3b32dee386d5d7fdc81","modified":1626699811699},{"_id":"source/categories/index.md","hash":"e0146dd9fb3b393359217a035341b8f26423420a","modified":1626699811699},{"_id":"source/images/linear_space.eddx","hash":"8910cfe98f066b604fbbb11ddc2b972c17164796","modified":1626699811699},{"_id":"source/tags/index.md","hash":"2cf9b26cbef3a6bbdf6a70647fe3974a83615d82","modified":1626699811699},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1626699811762},{"_id":"source/uploads/avatar.png","hash":"2e89df33605a967680602579ff84d138f02cabf8","modified":1626699811700},{"_id":"themes/next/layout/_custom/header.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1626699811727},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1626699811727},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"31322a7f57936cf2dc62e824af5490da5354cf02","modified":1626699811727},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"53d4f83b2b7fb4387dfc9fe81519abd56fbce4ae","modified":1626699811727},{"_id":"themes/next/layout/_macro/post.swig","hash":"911363776867d9523a3e322cdf591d49cd166403","modified":1626699811727},{"_id":"themes/next/layout/_macro/reward.swig","hash":"5d5f70deb6074cb4dd0438463e14ccf89213c282","modified":1626699811727},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"98962d59c744ea6b0bbac9be0b02033ed39964aa","modified":1626699811728},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"39852700e4084ecccffa6d4669168e5cc0514c9e","modified":1626699811728},{"_id":"themes/next/layout/_partials/comments.swig","hash":"ce7094ee05878161e7568a6dfae5b56ff3fbd6e1","modified":1626699811728},{"_id":"themes/next/layout/_partials/footer.swig","hash":"51d1dd1258e0525da5d846f7827e5ae942d02ab3","modified":1626699811728},{"_id":"themes/next/layout/_partials/head.swig","hash":"1f14d3f494b2dbbcee802fd6f6d1abd5b7e2304c","modified":1626699811728},{"_id":"themes/next/layout/_partials/header.swig","hash":"a1ffbb691dfad3eaf2832a11766e58a179003b8b","modified":1626699811729},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"1efd925d34a5d4ba2dc0838d9c86ba911e705fc9","modified":1626699811729},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"9e8e21d194ef44d271b1cca0bc1448c14d7edf4f","modified":1626699811729},{"_id":"themes/next/layout/_partials/search.swig","hash":"9dbd378e94abfcb3f864a5b8dbbf18d212ca2ee0","modified":1626699811729},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"03aaebe9d50f6acb007ec38cc04acd1cfceb404d","modified":1626699811731},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"766b2bdda29523ed6cd8d7aa197f996022f8fd94","modified":1626699811731},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"9baf90f7c40b3b10f288e9268c3191e895890cea","modified":1626699811732},{"_id":"themes/next/layout/_third-party/duoshuo-hot-articles.swig","hash":"5d4638c46aef65bf32a01681495b62416ccc98db","modified":1626699811736},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"7c04a42319d728be356746363aff8ea247791d24","modified":1626699811736},{"_id":"themes/next/layout/_third-party/mathjax.swig","hash":"6d25596d6a7c57700d37b607f8d9a62d89708683","modified":1626699811736},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"fc93b1a7e6aed0dddb1f3910142b48d8ab61174e","modified":1626699811737},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"22369026c87fc23893c35a7f250b42f3bb1b60f1","modified":1626699811737},{"_id":"themes/next/layout/_third-party/scroll-cookie.swig","hash":"1ddb2336a1a19b47af3017047012c01ec5f54529","modified":1626699811737},{"_id":"themes/next/scripts/tags/button.js","hash":"62e6dbeb53d07627a048132c79630b45d9a8f2cc","modified":1626699811741},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"535fc542781021c4326dec24d8495cbb1387634a","modified":1626699811741},{"_id":"themes/next/scripts/tags/exturl.js","hash":"8d7e60f60779bde050d20fd76f6fdc36fc85e06d","modified":1626699811741},{"_id":"themes/next/scripts/tags/full-image.js","hash":"8eeb3fb89540299bdbb799edfdfdac3743b50596","modified":1626699811741},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"49252824cd53184dc9b97b2f2d87ff28e1b3ef27","modified":1626699811741},{"_id":"themes/next/scripts/tags/label.js","hash":"2f8f41a7316372f0d1ed6b51190dc4acd3e16fff","modified":1626699811742},{"_id":"themes/next/scripts/tags/lazy-image.js","hash":"eeeabede68cf263de9e6593ecf682f620da16f0a","modified":1626699811742},{"_id":"themes/next/scripts/tags/note.js","hash":"64de4e9d01cf3b491ffc7d53afdf148ee5ad9779","modified":1626699811742},{"_id":"themes/next/scripts/tags/tabs.js","hash":"5786545d51c38e8ca38d1bfc7dd9e946fc70a316","modified":1626699811742},{"_id":"themes/next/source/css/main.styl","hash":"20702c48d6053c92c5bcdbc68e8d0ef1369848a0","modified":1626699811762},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1626699811762},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1626699811763},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1626699811763},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1626699811763},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1626699811763},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1626699811764},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1626699811764},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1626699811764},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1626699811764},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1626699811764},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1626699811764},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1626699811765},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1626699811765},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1626699811765},{"_id":"source/_posts/about-kernel-02/linear_space.png","hash":"3fba384aa3bb9b905e3e70d11013dc4a0cfeb0a9","modified":1626699811696},{"_id":"source/_posts/thinking-from-softmax-02/subgradient.png","hash":"62fddffc69ca5fb2184acbbf60771a666e5e8514","modified":1626699811698},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1626699811732},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1626699811732},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1626699811756},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1626699811756},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1626699811757},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1626699811761},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1626699811762},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"9e1b9666efa77f4cf8d8261bcfa445a9ac608e53","modified":1626699811729},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"bd07518060a73795d1250d93a74186444b292a9f","modified":1626699811729},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"957701729b85fb0c5bfcf2fb99c19d54582f91ed","modified":1626699811730},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"959b7e04a96a5596056e4009b73b6489c117597e","modified":1626699811730},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"eefe2388ff3d424694045eda21346989b123977c","modified":1626699811730},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"23e23dc0f76ef3c631f24c65277adf7ea517b383","modified":1626699811730},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"1f1107468aaf03f7d0dcd7eb2b653e2813a675b4","modified":1626699811730},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1626699811730},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"63315fcf210799f894208c9f512737096df84962","modified":1626699811731},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"069d1357c717572256e5cdee09574ebce529cbae","modified":1626699811731},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1626699811732},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1626699811732},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"60426bf73f8a89ba61fb1be2df3ad5398e32c4ef","modified":1626699811733},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"deda6a814ed48debc694c4e0c466f06c127163d0","modified":1626699811733},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"18e7bef8923d83ea42df6c97405e515a876cede4","modified":1626699811733},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"8160b27bee0aa372c7dc7c8476c05bae57f58d0f","modified":1626699811733},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"a234c5cd1f75ca5731e814d0dbb92fdcf9240d1b","modified":1626699811733},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"5d9943d74cc2e0a91badcf4f755c6de77eab193a","modified":1626699811733},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"3358d11b9a26185a2d36c96049e4340e701646e4","modified":1626699811733},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"fc65b9c98a0a8ab43a5e7aabff6c5f03838e09c8","modified":1626699811734},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"3658414379e0e8a34c45c40feadc3edc8dc55f88","modified":1626699811734},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"0ddc94ed4ba0c19627765fdf1abc4d8efbe53d5a","modified":1626699811734},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"c3971fd154d781088e1cc665035f8561a4098f4c","modified":1626699811734},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"0e3378f7c39b2b0f69638290873ede6b6b6825c0","modified":1626699811735},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"c316758546dc9ba6c60cb4d852c17ca6bb6d6724","modified":1626699811735},{"_id":"themes/next/layout/_third-party/comments/duoshuo.swig","hash":"a356b2185d40914447fde817eb3d358ab6b3e4c3","modified":1626699811735},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"3e8dc5c6c912628a37e3b5f886bec7b2e5ed14ea","modified":1626699811735},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"ee63aa2e49507b884a2d56778479cf01c723d751","modified":1626699811735},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"8a2e393d2e49f7bf560766d8a07cd461bf3fce4f","modified":1626699811736},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"8b6650f77fe0a824c8075b2659e0403e0c78a705","modified":1626699811736},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"c747fb5c6b1f500e8f0c583e44195878b66e4e29","modified":1626699811738},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"385c066af96bee30be2459dbec8aae1f15d382f5","modified":1626699811738},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1626699811738},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"c057b17f79e8261680fbae8dc4e81317a127c799","modified":1626699811738},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"328d9a9696cc2ccf59c67d3c26000d569f46344c","modified":1626699811756},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"2aa5b7166a85a8aa34b17792ae4f58a5a96df6cc","modified":1626699811756},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"eaedfaf06dae94ba77a8f4893e2e434bf8859bac","modified":1626699811756},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"82f9055955920ed88a2ab6a20ab02169abb2c634","modified":1626699811757},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"49b5210fa62d6cbc6a98f57d89d5067a06ab3561","modified":1626699811761},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"c8d35a6b9e3bff6d8fdb66de853065af9d37562d","modified":1626699811761},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"cfee25d790e4f9b7d57f0dc7e2ea9c1649f08f11","modified":1626699811762},{"_id":"themes/next/source/css/_variables/base.styl","hash":"d477196c5699c8261b08e993a77ef67054d86166","modified":1626699811762},{"_id":"themes/next/source/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1626699811765},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1626699811765},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"6117f97b4984b8e33f21c726132da64ba678e4ed","modified":1626699811766},{"_id":"themes/next/source/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1626699811766},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1626699811766},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1626699811766},{"_id":"themes/next/source/js/src/motion.js","hash":"dc0365b2fb315a8b43d3ef19b59d3a82a366fcc1","modified":1626699811766},{"_id":"themes/next/source/js/src/post-details.js","hash":"0693695a9512641daff63d99da772625a058ab18","modified":1626699811767},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1626699811767},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1626699811767},{"_id":"themes/next/source/js/src/utils.js","hash":"2917c39c75b14b6dab7e1c46ab4d87b4df9fcd5d","modified":1626699811767},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1626699811771},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1626699811774},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","hash":"7fd2f3e2773555392ef40df40cae3bedb884f17a","modified":1626699811774},{"_id":"themes/next/source/lib/fastclick/.bower.json","hash":"93ebd5b35e632f714dcf1753e1f6db77ec74449b","modified":1626699811777},{"_id":"themes/next/source/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1626699811777},{"_id":"themes/next/source/lib/fastclick/README.md","hash":"1decd8e1adad2cd6db0ab50cf56de6035156f4ea","modified":1626699811778},{"_id":"themes/next/source/lib/fastclick/bower.json","hash":"13379463c7463b4b96d13556b46faa4cc38d81e6","modified":1626699811778},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"a2aaaf12378db56bd10596ba3daae30950eac051","modified":1626699811779},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1626699811779},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1626699811779},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1626699811779},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1626699811779},{"_id":"themes/next/source/lib/jquery/.bower.json","hash":"91745c2cc6c946c7275f952b2b0760b880cea69e","modified":1626699811784},{"_id":"themes/next/source/lib/jquery_lazyload/.bower.json","hash":"b7638afc93e9cd350d0783565ee9a7da6805ad8e","modified":1626699811785},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","hash":"4891864c24c28efecd81a6a8d3f261145190f901","modified":1626699811786},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","hash":"895d50fa29759af7835256522e9dd7dac597765c","modified":1626699811786},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","hash":"65bc85d12197e71c40a55c0cd7f6823995a05222","modified":1626699811786},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1626699811786},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1626699811786},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1626699811787},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1626699811787},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1626699811787},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1626699811787},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1626699811787},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1626699811787},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1626699811787},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1626699811788},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1626699811788},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1626699811788},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1626699811788},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1626699811788},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1626699811788},{"_id":"themes/next/source/lib/pace/pace.min.js","hash":"9944dfb7814b911090e96446cea4d36e2b487234","modified":1626699811789},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1626699811789},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1626699811789},{"_id":"themes/next/source/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1626699811789},{"_id":"themes/next/source/lib/velocity/.bower.json","hash":"05f960846f1c7a93dab1d3f9a1121e86812e8c88","modified":1626699811793},{"_id":"themes/next/source/lib/velocity/bower.json","hash":"2ec99573e84c7117368beccb9e94b6bf35d2db03","modified":1626699811793},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1626699811795},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1626699811795},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1626699811795},{"_id":"themes/next/source/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1626699811785},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"28ff4ed6714c59124569ffcbd10f1173d53ca923","modified":1626699811737},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"ba698f49dd3a868c95b240d802f5b1b24ff287e4","modified":1626699811737},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"d026c8489f66ab6c12ad04bd37f1d5b6f2f3f0d1","modified":1626699811743},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"31050fc7a25784805b4843550151c93bfa55c9c8","modified":1626699811743},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"0dfb4b3ba3180d7285e66f270e1d3fa0f132c3d2","modified":1626699811743},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1626699811743},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"a6bb5256be6195e76addbda12f4ed7c662d65e7a","modified":1626699811743},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"711c8830886619d4f4a0598b0cde5499dce50c62","modified":1626699811746},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1626699811751},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"2186be20e317505cd31886f1291429cc21f76703","modified":1626699811754},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"2915df7152ea095a6290ef69157fd67669e0e793","modified":1626699811755},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"9c25c75311e1bd4d68df031d3f2ae6d141a90766","modified":1626699811755},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"86b6fd7f1b1be3ae98f8af6b23a6b1299c670ce9","modified":1626699811755},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1626699811755},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a280a583b7615e939aaddbf778f5c108ef8a2a6c","modified":1626699811755},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"64f5d56c08d74a338813df1265580ca0cbf0190b","modified":1626699811756},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"bc8c388553bbcf95897459a466ba35bffd5ec5f0","modified":1626699811757},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"c2d079788d6fc2e9a191ccdae94e50d55bf849dc","modified":1626699811757},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"5ae7906dc7c1d9468c7f4b4a6feddddc555797a1","modified":1626699811758},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1626699811758},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"b0dcca862cd0cc6e732e33d975b476d744911742","modified":1626699811758},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"fda14bc35be2e1b332809b55b3d07155a833dbf4","modified":1626699811758},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1626699811758},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9a5581a770af8964064fef7afd3e16963e45547f","modified":1626699811758},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"0efa036a15c18f5abb058b7c0fad1dd9ac5eed4c","modified":1626699811759},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"8829bc556ca38bfec4add4f15a2f028092ac6d46","modified":1626699811759},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"82bbaa6322764779a1ac2e2c8390ce901c7972e2","modified":1626699811759},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1626699811759},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"a0e2030a606c934fb2c5c7373aaae04a1caac4c5","modified":1626699811760},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"c4ed249798296f60bda02351fe6404fb3ef2126f","modified":1626699811760},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"5b93958239d3d2bf9aeaede44eced2434d784462","modified":1626699811760},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"215de948be49bcf14f06d500cef9f7035e406a43","modified":1626699811760},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"2f878213cb24c5ddc18877f6d15ec5c5f57745ac","modified":1626699811761},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"e3e23751d4ad24e8714b425d768cf68e37de7ded","modified":1626699811761},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"69ecd6c97e7cdfd822ac8102b45ad0ede85050db","modified":1626699811761},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"79da92119bc246fe05d1626ac98426a83ec90a94","modified":1626699811767},{"_id":"themes/next/source/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1626699811771},{"_id":"themes/next/source/lib/Han/dist/han.min.css","hash":"a0c9e32549a8b8cf327ab9227b037f323cdb60ee","modified":1626699811770},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1626699811774},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1626699811775},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1626699811774},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1626699811775},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1626699811775},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1626699811775},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1626699811777},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1626699811777},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1626699811777},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1626699811778},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1626699811778},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1626699811780},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1626699811780},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1626699811780},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1626699811793},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1626699811793},{"_id":"themes/next/source/lib/Han/dist/han.css","hash":"bd40da3fba8735df5850956814e312bd7b3193d7","modified":1626699811769},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1626699811784},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1626699811794},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"8994ffcce84deac0471532f270f97c44fea54dc0","modified":1626699811744},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"ae1ca14e51de67b07dba8f61ec79ee0e2e344574","modified":1626699811744},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1626699811744},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"8a2421cb9005352905fae9d41a847ae56957247e","modified":1626699811744},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"6c00f6e0978f4d8f9a846a15579963728aaa6a17","modified":1626699811744},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"49c2b2c14a1e7fcc810c6be4b632975d0204c281","modified":1626699811745},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"96f32ea6c3265a3889e6abe57587f6e2a2a40dfb","modified":1626699811745},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"740d37f428b8f4574a76fc95cc25e50e0565f45e","modified":1626699811745},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"b76387934fb6bb75212b23c1a194486892cc495e","modified":1626699811745},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"7778920dd105fa4de3a7ab206eeba30b1a7bac45","modified":1626699811745},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"2039590632bba3943c39319d80ef630af7928185","modified":1626699811746},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1626699811745},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1626699811746},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"a82afbb72d83ee394aedc7b37ac0008a9823b4f4","modified":1626699811746},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"beccb53dcd658136fb91a0c5678dea8f37d6e0b6","modified":1626699811746},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"0f7f522cc6bfb3401d5afd62b0fcdf48bb2d604b","modified":1626699811746},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"f54367c0feda6986c030cc4d15a0ca6ceea14bcb","modified":1626699811747},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"2cdc094ecf907a02fce25ad4a607cd5c40da0f2b","modified":1626699811747},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"88c7d75646b66b168213190ee4cd874609afd5e3","modified":1626699811747},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1626699811747},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"ed88c8b51d0517759c777e71a6bfbe2907bcd994","modified":1626699811747},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"a5d8617a24d7cb6c5ad91ea621183ca2c0917331","modified":1626699811747},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"ee554b1031ef0070a5916477939021800e3c9d27","modified":1626699811747},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"017074ef58166e2d69c53bb7590a0e7a8947a1ed","modified":1626699811748},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1626699811748},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"963105a531403d7aad6d9e5e23e3bfabb8ec065a","modified":1626699811748},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"10251257aceecb117233c9554dcf8ecfef8e2104","modified":1626699811748},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"08a500b2984f109b751f3697ca33172d1340591a","modified":1626699811748},{"_id":"themes/next/source/css/_common/components/post/post-wordcount.styl","hash":"4fda5d38c6c8d910e3bf5c74a48a8d4a3f3dc73d","modified":1626699811748},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"51eca243220cf57133a4becae9b78514bcfdc723","modified":1626699811749},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"65a64d5662637b66e2f039a5f58217afe7a6e800","modified":1626699811749},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"5f6ea57aabfa30a437059bf8352f1ad829dbd4ff","modified":1626699811749},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"920343e41c124221a17f050bbb989494d44f7a24","modified":1626699811749},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"efa5e5022e205b52786ce495d4879f5e7b8f84b2","modified":1626699811750},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"9486ddd2cb255227db102d09a7df4cae0fabad72","modified":1626699811750},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"45fa7193435a8eae9960267438750b4c9fa9587f","modified":1626699811750},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"77c92a449ce84d558d26d052681f2e0dd77c70c9","modified":1626699811750},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"f7784aba0c1cd20d824c918c120012d57a5eaa2a","modified":1626699811750},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"57d2c8a060f5e4e1a0aef9aae11a0016cf7ac5ba","modified":1626699811750},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"3623e7fa4324ec1307370f33d8f287a9e20a5578","modified":1626699811750},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"c2abe4d87148e23e15d49ee225bc650de60baf46","modified":1626699811751},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"b8969e1654eec89a0fd10d88b337fee9cb03cd44","modified":1626699811751},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"1b3cc9f4e5a7f6e05b4100e9990b37b20d4a2005","modified":1626699811751},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1626699811752},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"4a457d265d62f287c63d48764ce45d9bcfc9ec5a","modified":1626699811752},{"_id":"themes/next/source/css/_common/components/tags/note-modern.styl","hash":"45df0cf4c97b47e05573bcd41028ee50f3fdf432","modified":1626699811752},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"32c9156bea5bac9e9ad0b4c08ffbca8b3d9aac4b","modified":1626699811752},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"4ab5deed8c3b0c338212380f678f8382672e1bcb","modified":1626699811752},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"ead0d0f2321dc71505788c7f689f92257cf14947","modified":1626699811752},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"fd42777b9125fd8969dc39d4f15473e2b91b4142","modified":1626699811753},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"93b08815c4d17e2b96fef8530ec1f1064dede6ef","modified":1626699811753},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"d4e6d8d7b34dc69994593c208f875ae8f7e8a3ae","modified":1626699811753},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"2340dd9b3202c61d73cc708b790fac5adddbfc7f","modified":1626699811753},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"cce6772e2cdb4db85d35486ae4c6c59367fbdd40","modified":1626699811754},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1626699811754},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"d89c4b562b528e4746696b2ad8935764d133bdae","modified":1626699811754},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"aeff0e6e23725e8baea27c890ccbbf466024f767","modified":1626699811754},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1626699811759},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1626699811759},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1626699811760},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1626699811768},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1626699811768},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1626699811769},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1626699811769},{"_id":"themes/next/source/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1626699811770},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1626699811776},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1626699811776},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1626699811776},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1626699811776},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1626699811776},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1626699811780},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1626699811776},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1626699811783},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1626699811784},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1626699811781},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1626699811773},{"_id":"themes/next/source/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1626699811792},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1626699811782},{"_id":"public/baidusitemap.xml","hash":"4f0cdb2ee771203d1f80ed059c824870d00f7f77","modified":1626700290718},{"_id":"public/search.xml","hash":"10cac93dcce290f6656b68a8aa01630514f73fb4","modified":1626700290808},{"_id":"public/sitemap.xml","hash":"8588e382df05f2699589e78039bf2bdfea7983f6","modified":1626700290814},{"_id":"public/categories/index.html","hash":"443c509dce430ccb98b309f3469c2996646075d6","modified":1626700290832},{"_id":"public/2020/01/12/alterable-vs-permanent/index.html","hash":"df86ee2e1d2e4014d4947a720e59b271439a756b","modified":1626700290833},{"_id":"public/tags/index.html","hash":"80cb1ae82cb239c0f2b9859a5d1cbb074e43d301","modified":1626700290833},{"_id":"public/2019/09/10/5-steps-for-asking-good-questions/index.html","hash":"4a99b305f324f7f1497965ac0412fcdc2e820c50","modified":1626700290833},{"_id":"public/2017/12/27/rating-model-considering-user-count/index.html","hash":"936ac72ff9507488a8c022f166aee88d99589eba","modified":1626700290833},{"_id":"public/2017/12/23/GD-Series/index.html","hash":"80bd43669b6b7e927e0c2785364a638f2a4ddaa7","modified":1626700290833},{"_id":"public/2018/02/04/discriminative-or-generative-model/index.html","hash":"2a74e803f385fa99161078eda2a1cb2800ba2a8a","modified":1626700290833},{"_id":"public/2017/12/03/common-ground-of-mse-and-cee-in-nn/index.html","hash":"f1ead5f03d9ba7fe5be4bf33b5bb7df429dfae02","modified":1626700290833},{"_id":"public/2017/11/04/GAN-02/index.html","hash":"533f7d240dafd06456c6ed51729dc91fbd7543e5","modified":1626700290833},{"_id":"public/2017/11/01/GAN-01/index.html","hash":"3e63362fb5e99dd2ce28a46e1db99c3a9a90cbba","modified":1626700290833},{"_id":"public/2017/10/23/kmeans-is-a-gmm/index.html","hash":"cba5a5dde448f06380f3e6ed67c0f749026a1b96","modified":1626700290833},{"_id":"public/2017/10/15/from-boost-to-Adaboost-to-GBT/index.html","hash":"dc59fa273f9c4cd9f33d0e01313e285b21d61058","modified":1626700290833},{"_id":"public/2017/10/12/statistical-formulars-for-programmers/index.html","hash":"43c5bb39d22e19138d27d678575383a78e835904","modified":1626700290833},{"_id":"public/2017/09/13/norm-regularization-appendix/index.html","hash":"c45afb99d8108a96fa46ec39fcb8a88f4dab389a","modified":1626700290833},{"_id":"public/2017/09/09/norm-regularization-02/index.html","hash":"75bcae04e206a5ba3a6939644db31e6cd8462931","modified":1626700290834},{"_id":"public/2017/09/07/norm-regularization-01/index.html","hash":"8a15f744022ef3ce0939371607093770b4bdcab2","modified":1626700290834},{"_id":"public/2017/08/13/about-kernel-02/index.html","hash":"0824c5662239a9b94b0329466e94b901a006f3f3","modified":1626700290834},{"_id":"public/2017/08/13/about-kernel-01/index.html","hash":"6da4a83f009a9e7815ffe1370928bab60b7eae97","modified":1626700290834},{"_id":"public/2017/08/11/thinking-from-softmax-03/index.html","hash":"d40882e06e88fb9ea592b16849845a7096fa2d76","modified":1626700290834},{"_id":"public/2017/08/10/thinking-from-softmax-02/index.html","hash":"22467aa54fac1b02179384ace57e34bb7febc893","modified":1626700290834},{"_id":"public/2017/08/10/thinking-from-softmax-01/index.html","hash":"bd65850c1958fa0899370d83fc693bd725005d1f","modified":1626700290834},{"_id":"public/2017/08/08/compare-between-PCA-and-LFM/index.html","hash":"50c6b4acd92dbed3dbfe6b280f3b0824ae321060","modified":1626700290834},{"_id":"public/2017/08/07/init-blog/index.html","hash":"5867790b51fd9f377f8d6b3df073f8d25ad8c3b8","modified":1626700290834},{"_id":"public/index.html","hash":"4ef7f2e3949f8f4a85ea5f5e2840977ce516533d","modified":1626700290834},{"_id":"public/page/2/index.html","hash":"112e13ccb7e6214b5bfccbff5c76db0eb0b3b817","modified":1626700290834},{"_id":"public/page/3/index.html","hash":"cec35bf3c5bfdb9c91fd3466fab579bfecd1d322","modified":1626700290834},{"_id":"public/archives/index.html","hash":"cd85d7006134ea05caa46ec0d8cf2871743c9ea4","modified":1626700290834},{"_id":"public/archives/page/2/index.html","hash":"b1bf1107684afe17065ae7a2905bdb147fb7a75c","modified":1626700290834},{"_id":"public/archives/page/3/index.html","hash":"b25265b8033aeef3572bd7aa649e233e236cfe69","modified":1626700290834},{"_id":"public/archives/2017/index.html","hash":"1c4c4d6ca37b933d0440379c8814140bc596e25f","modified":1626700290835},{"_id":"public/archives/2017/page/2/index.html","hash":"ead4db036237d2e3388801d79e96cbda32b2f05e","modified":1626700290835},{"_id":"public/archives/2017/08/index.html","hash":"fab48958082425bdb5055302fdf0217bea955743","modified":1626700290835},{"_id":"public/archives/2017/09/index.html","hash":"d3aa242b1b6daf2331d176ca21a68bee927361db","modified":1626700290835},{"_id":"public/archives/2017/10/index.html","hash":"5ac10840fb3a4404724261d79b1f76f7e873f949","modified":1626700290835},{"_id":"public/archives/2017/11/index.html","hash":"dd3b9cb2c562f9af21351ed1fd12743d6169b649","modified":1626700290835},{"_id":"public/archives/2017/12/index.html","hash":"15c9af85fe59d8b5c6183d736a66095c4da596ce","modified":1626700290835},{"_id":"public/archives/2018/index.html","hash":"add2cf3b04a198784d029155f8a5fd6515814015","modified":1626700290835},{"_id":"public/archives/2018/02/index.html","hash":"ce8e9cce987a6db8f29a8fc141d0b55a47507c11","modified":1626700290835},{"_id":"public/archives/2019/index.html","hash":"be755f6fa79372e1f09d2f5688687140797138d6","modified":1626700290835},{"_id":"public/archives/2019/09/index.html","hash":"09504a146798d250fbd127bd61a2fc3302813a4c","modified":1626700290835},{"_id":"public/archives/2020/index.html","hash":"3a61b3ec0d3ded62cc9b7282101f1837ed790690","modified":1626700290835},{"_id":"public/archives/2020/01/index.html","hash":"03093e4cb3e084f1eb49ef5b252e3c184e3f0569","modified":1626700290835},{"_id":"public/categories/Life/index.html","hash":"db826d39accaa7b2a03b685daee8e14c340132c6","modified":1626700290835},{"_id":"public/categories/ML/index.html","hash":"c2bb6971da2c559baea61755bb80e186f3519be7","modified":1626700290835},{"_id":"public/categories/ML/page/2/index.html","hash":"79b8ae8666b9e516ed4b290b1f89d0eabbc06a2b","modified":1626700290835},{"_id":"public/categories/Mathematics/index.html","hash":"1d56da8355a64573cdf467d7283621003c0a7bb7","modified":1626700290835},{"_id":"public/tags/Tips/index.html","hash":"92872efefeea78683b0912b75866ef5aa8bf7861","modified":1626700290835},{"_id":"public/tags/Deep-learning/index.html","hash":"9bb393037ea22dc03fcdebc5a9d35d7b1bcf84c7","modified":1626700290836},{"_id":"public/tags/GAN/index.html","hash":"018e92925dc870d3bb1f6a4dce73740738d1bbda","modified":1626700290836},{"_id":"public/tags/GD/index.html","hash":"85b396169d38537dd64935765c9f525ab9c00d4f","modified":1626700290836},{"_id":"public/tags/Momentum/index.html","hash":"7c1e24639c8966be3192a4253357c320ae1587bd","modified":1626700290836},{"_id":"public/tags/Kernel/index.html","hash":"5d51d70cc5cfa5500830a039a7cad8f38680b656","modified":1626700290836},{"_id":"public/tags/SVM/index.html","hash":"6f90d068e07d62f548e2572feecb2ece7414a07d","modified":1626700290836},{"_id":"public/tags/RKHS/index.html","hash":"fb0aafeec857e585815e49886f4acf936821eb1c","modified":1626700290836},{"_id":"public/tags/Functional-analysis/index.html","hash":"080f2f638dc32a7487c51c77a032d74d874ae74b","modified":1626700290836},{"_id":"public/tags/Loss-function/index.html","hash":"7d9d38704064c6f2d901d6f9affc1b8b5fd4aaa3","modified":1626700290836},{"_id":"public/tags/PCA/index.html","hash":"3abe7c823440c185364e2e6aaae254607bc44318","modified":1626700290836},{"_id":"public/tags/LFM/index.html","hash":"d610b2bca1171e3c44839b32fcd9f4a570484d1a","modified":1626700290836},{"_id":"public/tags/Discriminative-model/index.html","hash":"58300eda38122cea706472ab16f0c6e8aacb4de9","modified":1626700290836},{"_id":"public/tags/Generative-model/index.html","hash":"453d7edffca1600935419376f7eb6659a5e30560","modified":1626700290836},{"_id":"public/tags/Ensemble/index.html","hash":"a9fbc440c55e59f2e18443ad1568d3bf7c769d9b","modified":1626700290836},{"_id":"public/tags/Boost/index.html","hash":"58b026e135500433206ece3bb0d75670f680d328","modified":1626700290836},{"_id":"public/tags/GBDT-GBRT/index.html","hash":"f39e43c888b0bf6f40fb76564b290cec337de941","modified":1626700290836},{"_id":"public/tags/Norm-regularization/index.html","hash":"2d7659c2981a02e3d0f2a3df3fc3a33e34c4d74e","modified":1626700290836},{"_id":"public/tags/Convex-optimization/index.html","hash":"3988e58c67d9af7760b352c8e2086973099ce4f0","modified":1626700290837},{"_id":"public/tags/K-means/index.html","hash":"e039434262f6520a67f07f5d9d609cd1709d8418","modified":1626700290837},{"_id":"public/tags/GMM/index.html","hash":"d99b0a936153c330d724fca62d8d0ce2f7a79bde","modified":1626700290837},{"_id":"public/tags/EM-Algorithm/index.html","hash":"57840c2ab83885216d777268e34bfd08bdfdf919","modified":1626700290837},{"_id":"public/tags/Matrix-theory/index.html","hash":"d05c6ca9b71b5ac77a47a40228d0b6a79566c961","modified":1626700290837},{"_id":"public/tags/Rating/index.html","hash":"4a4e413722c037ec8a6688e8cc2ca3994d2a1a29","modified":1626700290837},{"_id":"public/tags/Statistics/index.html","hash":"a962f96cdcb14e6a13411c4b4518a402b70930ad","modified":1626700290837},{"_id":"public/tags/Generalized-Linear-Model/index.html","hash":"bc65eaafa02bd18ecdd35ecddd26def0838a2855","modified":1626700290837},{"_id":"public/tags/Exponential-family-distribution/index.html","hash":"d92eae7f4f211a52fafc7a0e3f0669525f406584","modified":1626700290837},{"_id":"public/tags/Logistic/index.html","hash":"2c158a877dbcb10a98737ffd88ce5a8d88093793","modified":1626700290837},{"_id":"public/tags/Softmax/index.html","hash":"dac14faa6491229f79cb586a7309e9e512083257","modified":1626700290837},{"_id":"public/tags/Gradient/index.html","hash":"9ad67646d3643225c9710318987a1bc6afa84e0b","modified":1626700290837},{"_id":"public/tags/Subgradient/index.html","hash":"145492472ba603f328b612d7054a4c47c0ba0e01","modified":1626700290837},{"_id":"public/tags/Activation-function/index.html","hash":"384cc9baf6f22645c5c33d6930fd711cc4f1290a","modified":1626700290837},{"_id":"public/README.md","hash":"876c607060f4a864761fd1d8cc982edba012f251","modified":1626700290844},{"_id":"public/CNAME","hash":"685ff10641d5fffd71eec3979dab6fd0f10c9ea7","modified":1626700290844},{"_id":"public/baidu_verify_aX3upgcPux.html","hash":"344502cc0cb9646f0643ff932d5c7adb1438cd5f","modified":1626700290844},{"_id":"public/robots.txt","hash":"7858025995a22727d626cdda854f061df5ba8fb9","modified":1626700290844},{"_id":"public/images/linear_space.eddx","hash":"8910cfe98f066b604fbbb11ddc2b972c17164796","modified":1626700290844},{"_id":"public/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1626700290844},{"_id":"public/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1626700290844},{"_id":"public/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1626700290845},{"_id":"public/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1626700290845},{"_id":"public/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1626700290845},{"_id":"public/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1626700290845},{"_id":"public/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1626700290845},{"_id":"public/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1626700290845},{"_id":"public/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1626700290845},{"_id":"public/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1626700290845},{"_id":"public/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1626700290845},{"_id":"public/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1626700290845},{"_id":"public/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1626700290845},{"_id":"public/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1626700290845},{"_id":"public/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1626700290845},{"_id":"public/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1626700290845},{"_id":"public/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1626700290845},{"_id":"public/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1626700290845},{"_id":"public/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1626700290845},{"_id":"public/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1626700290845},{"_id":"public/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1626700290845},{"_id":"public/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1626700290845},{"_id":"public/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1626700290845},{"_id":"public/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1626700290845},{"_id":"public/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1626700290845},{"_id":"public/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1626700290846},{"_id":"public/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1626700290846},{"_id":"public/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1626700290846},{"_id":"public/2017/08/13/about-kernel-02/linear_space.png","hash":"3fba384aa3bb9b905e3e70d11013dc4a0cfeb0a9","modified":1626700290846},{"_id":"public/2017/08/10/thinking-from-softmax-02/subgradient.png","hash":"62fddffc69ca5fb2184acbbf60771a666e5e8514","modified":1626700290846},{"_id":"public/uploads/avatar.png","hash":"2e89df33605a967680602579ff84d138f02cabf8","modified":1626700291241},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1626700291246},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1626700291250},{"_id":"public/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1626700291259},{"_id":"public/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1626700291259},{"_id":"public/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1626700291259},{"_id":"public/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1626700291260},{"_id":"public/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1626700291260},{"_id":"public/js/src/bootstrap.js","hash":"6117f97b4984b8e33f21c726132da64ba678e4ed","modified":1626700291260},{"_id":"public/js/src/motion.js","hash":"dc0365b2fb315a8b43d3ef19b59d3a82a366fcc1","modified":1626700291260},{"_id":"public/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1626700291260},{"_id":"public/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1626700291260},{"_id":"public/js/src/post-details.js","hash":"0693695a9512641daff63d99da772625a058ab18","modified":1626700291260},{"_id":"public/js/src/utils.js","hash":"2917c39c75b14b6dab7e1c46ab4d87b4df9fcd5d","modified":1626700291260},{"_id":"public/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1626700291260},{"_id":"public/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1626700291260},{"_id":"public/lib/canvas-ribbon/canvas-ribbon.js","hash":"7fd2f3e2773555392ef40df40cae3bedb884f17a","modified":1626700291260},{"_id":"public/lib/fastclick/bower.json","hash":"4dcecf83afddba148464d5339c93f6d0aa9f42e9","modified":1626700291260},{"_id":"public/lib/font-awesome/bower.json","hash":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad","modified":1626700291260},{"_id":"public/lib/jquery_lazyload/bower.json","hash":"ae3c3b61e6e7f9e1d7e3585ad854380ecc04cf53","modified":1626700291260},{"_id":"public/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1626700291260},{"_id":"public/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1626700291260},{"_id":"public/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1626700291260},{"_id":"public/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1626700291260},{"_id":"public/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1626700291260},{"_id":"public/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1626700291260},{"_id":"public/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1626700291260},{"_id":"public/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1626700291260},{"_id":"public/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1626700291260},{"_id":"public/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1626700291260},{"_id":"public/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1626700291261},{"_id":"public/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1626700291261},{"_id":"public/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1626700291261},{"_id":"public/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1626700291261},{"_id":"public/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1626700291261},{"_id":"public/lib/pace/pace.min.js","hash":"9944dfb7814b911090e96446cea4d36e2b487234","modified":1626700291261},{"_id":"public/lib/velocity/bower.json","hash":"0ef14e7ccdfba5db6eb3f8fc6aa3b47282c36409","modified":1626700291261},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1626700291261},{"_id":"public/js/src/schemes/pisces.js","hash":"79da92119bc246fe05d1626ac98426a83ec90a94","modified":1626700291261},{"_id":"public/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1626700291261},{"_id":"public/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1626700291261},{"_id":"public/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1626700291261},{"_id":"public/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1626700291261},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1626700291261},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1626700291261},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1626700291261},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1626700291261},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1626700291261},{"_id":"public/lib/fastclick/README.html","hash":"287b2e24cae1f7d01877dda79b76c24f81123895","modified":1626700291261},{"_id":"public/lib/jquery_lazyload/CONTRIBUTING.html","hash":"a6358170d346af13b1452ac157b60505bec7015c","modified":1626700291261},{"_id":"public/lib/jquery_lazyload/README.html","hash":"bfb6e229fc36e493d45499b4ac30a7c298786ab4","modified":1626700291261},{"_id":"public/css/main.css","hash":"ecc71aa23fbdc6869c911e009c24890bf88213ec","modified":1626700291261},{"_id":"public/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1626700291261},{"_id":"public/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1626700291261},{"_id":"public/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1626700291261},{"_id":"public/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1626700291261},{"_id":"public/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1626700291261},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1626700291261},{"_id":"public/lib/Han/dist/han.min.css","hash":"a0c9e32549a8b8cf327ab9227b037f323cdb60ee","modified":1626700291262},{"_id":"public/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1626700291262},{"_id":"public/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1626700291262},{"_id":"public/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1626700291262},{"_id":"public/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1626700291262},{"_id":"public/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1626700291262},{"_id":"public/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1626700291262},{"_id":"public/lib/Han/dist/han.css","hash":"bd40da3fba8735df5850956814e312bd7b3193d7","modified":1626700291262},{"_id":"public/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1626700291262},{"_id":"public/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1626700291262},{"_id":"public/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1626700291262},{"_id":"public/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1626700291262},{"_id":"public/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1626700291262},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1626700291263},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1626700291263},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1626700291298}],"Category":[{"name":"Life","_id":"ckrancn990003g0vwura362bx"},{"name":"ML","_id":"ckrancn9d0008g0vwe20xrm6l"},{"name":"Mathematics","_id":"ckrancna0001gg0vwdpmw7qwd"}],"Data":[],"Page":[{"title":"","date":"2017-08-07T14:13:45.000Z","type":"categories","comments":0,"_content":"","source":"categories/index.md","raw":"---\ntitle: \ndate: 2017-08-07 22:13:45\ntype: \"categories\"\ncomments: false\n---\n","updated":"2021-07-19T13:03:31.699Z","path":"categories/index.html","layout":"page","_id":"ckrancn940001g0vw466tt4d7","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"","date":"2017-08-07T14:12:32.000Z","type":"tags","comments":0,"_content":"","source":"tags/index.md","raw":"---\ntitle: \ndate: 2017-08-07 22:12:32\ntype: \"tags\"\ncomments: false\n---\n","updated":"2021-07-19T13:03:31.699Z","path":"tags/index.html","layout":"page","_id":"ckrancnb0003ug0vwvuot6zr1","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":" 5 ","date":"2019-09-10T12:05:04.000Z","description":" 5 ","_content":"\n\n\n 5 \n\n1. ****\n2. ****\n3. ****\n4. ****\n5. ****\n\n\n\n","source":"_posts/5-steps-for-asking-good-questions.md","raw":"---\ntitle:  5 \ndate: 2019-09-10 20:05:04\ncategories: Life\ntags:\n- Tips\ndescription:  5 \n---\n\n\n\n 5 \n\n1. ****\n2. ****\n3. ****\n4. ****\n5. ****\n\n\n\n","slug":"5-steps-for-asking-good-questions","published":1,"updated":"2021-07-19T13:03:31.695Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrancn900000g0vwtw0cs3mu","content":"<p></p>\n<p> 5 </p>\n<ol>\n<li><strong></strong></li>\n<li><strong></strong></li>\n<li><strong></strong></li>\n<li><strong></strong></li>\n<li><strong></strong></li>\n</ol>\n<p></p>\n","site":{"data":{}},"excerpt":"","more":"<p></p>\n<p> 5 </p>\n<ol>\n<li><strong></strong></li>\n<li><strong></strong></li>\n<li><strong></strong></li>\n<li><strong></strong></li>\n<li><strong></strong></li>\n</ol>\n<p></p>\n"},{"title":" GAN  TensorFlow ","date":"2017-11-01T08:09:01.000Z","description":"GAN TensorFlow ","_content":"\n# GAN\n\n2014 Goodfellow **GAN**\n\n- \n- \n-  D G\n- G  D \n\nG  D G D  D  G D  $1/2$  G \n\n $x$  $p_g(x)$  $p_z(z)$  $G(z;\\theta_g)$  $G$  $D(x;\\theta_d)$  $x$  $D$  $\\log (1-D(G(z)))$ G  $G$ \n$$\n\\min_G \\max_D V(D,G) = \\mathbb{E}_{x \\sim p_{\\text{data}}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_{z}(z)}[\\log (1-D(G(z)))]\n$$\n\n# \n\n\n$$\n\\mathbb{E}_{z \\sim p_{z}(z)}[\\log (1-D(G(z)))] = \\mathbb{E}_{x \\sim p_{g}(x)}[\\log (1-D(x)]\n$$\n $V(D,G)$  $x$ \n$$\nV(G, D) = \\int_{x} p_{\\text{data}}(x) \\log (D(x)) + p_g(x) \\log (1-D(x)) \\, dx\n$$\n $p_{\\text{data}}(x)$  $p_g(x)$ \n$$\nf(y) = a \\log y + b \\log (1-y)\n$$\n\n$$\nf'(y) = 0 \\Rightarrow \\frac{a}{y}-\\frac{b}{1-y}=0 \\Rightarrow y=\\frac{a}{a+b}\n$$\n\n$$\nf''(y) \\Big|_{y=\\frac{a}{a+b}} = -\\frac{a}{\\left( \\frac{a}{a+b} \\right)^{2}}-\\frac{b}{1-\\left( \\frac{a}{a+b} \\right)^{2}}<0\n$$\n $\\frac{a}{a+b}$  $a=p_{\\text{data}}(x), b=p_g(x), y=D(x)$ \n$$\nD(x) = \\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)}\n$$\n\n# \n\nGAN  $p_{\\text{data}}=p_g$ \n$$\nD_G^{\\ast}=\\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)}=\\frac{1}{2}\n$$\nGoodfellow  min max \n\n#  TensorFlow  MNIST \n\n CNN G  D \n\n```python\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.examples.tutorials.mnist import input_data\n# from PIL import Image\n\nmnist = input_data.read_data_sets(\"MNIST_data/\")\nimages = mnist.train.images\n\n\ndef xavier_initializer(shape):\n    return tf.random_normal(shape=shape, stddev=1.0 / shape[0])\n\n\n# Generator\nz_size = 100  # maybe larger\ng_w1_size = 400\ng_out_size = 28 * 28\n\n# Discriminator\nx_size = 28 * 28\nd_w1_size = 400\nd_out_size = 1\n\nz = tf.placeholder('float', shape=(None, z_size))\nX = tf.placeholder('float', shape=(None, x_size))\n\n# use dict to share variables\ng_weights = {\n    'w1': tf.Variable(xavier_initializer(shape=(z_size, g_w1_size))),\n    'b1': tf.Variable(tf.zeros(shape=[g_w1_size])),\n    'out': tf.Variable(xavier_initializer(shape=(g_w1_size, g_out_size))),\n    'b2': tf.Variable(tf.zeros(shape=[g_out_size])),\n}\n\nd_weights = {\n    'w1': tf.Variable(xavier_initializer(shape=(x_size, d_w1_size))),\n    'b1': tf.Variable(tf.zeros(shape=[d_w1_size])),\n    'out': tf.Variable(xavier_initializer(shape=(d_w1_size, d_out_size))),\n    'b2': tf.Variable(tf.zeros(shape=[d_out_size])),\n}\n\n\ndef G(z, w=g_weights):\n    # here tanh is better than relu\n    h1 = tf.tanh(tf.matmul(z, w['w1']) + w['b1'])\n    # pixel output is in range [0, 255]\n    return tf.sigmoid(tf.matmul(h1, w['out']) + w['b2']) * 255\n\n\ndef D(x, w=d_weights):\n    # here tanh is better than relu\n    h1 = tf.tanh(tf.matmul(x, w['w1']) + w['b1'])\n    h2 = tf.matmul(h1, w['out']) + w['b2']\n    return h2  # use h2 to calculate logits loss\n\n\ndef generate_z(n=1):\n    return np.random.normal(size=(n, z_size))\n\n\nsample = G(z)\n\ndout_real = D(X)\ndout_fake = D(G(z))\n\nG_obj = tf.reduce_mean(\n    tf.nn.sigmoid_cross_entropy_with_logits(logits=dout_fake, labels=tf.ones_like(dout_fake)))\nD_obj_real = tf.reduce_mean(  # use single side smoothing\n    tf.nn.sigmoid_cross_entropy_with_logits(logits=dout_real, labels=(tf.ones_like(dout_real) - 0.1)))\nD_obj_fake = tf.reduce_mean(\n    tf.nn.sigmoid_cross_entropy_with_logits(logits=dout_fake, labels=tf.zeros_like(dout_fake)))\nD_obj = D_obj_real + D_obj_fake\n\nG_opt = tf.train.AdamOptimizer().minimize(G_obj, var_list=g_weights.values())\nD_opt = tf.train.AdamOptimizer().minimize(D_obj, var_list=d_weights.values())\n\n# Training\nbatch_size = 128\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n\n    for i in range(200):\n        sess.run(D_opt, feed_dict={\n            X: images[np.random.choice(range(len(images)), batch_size)].reshape(batch_size, x_size),\n            z: generate_z(batch_size),\n        })\n        # run two phases of generator\n        sess.run(G_opt, feed_dict={\n            z: generate_z(batch_size)\n        })\n        sess.run(G_opt, feed_dict={\n            z: generate_z(batch_size)\n        })\n\n        g_cost = sess.run(G_obj, feed_dict={z: generate_z(batch_size)})\n        d_cost = sess.run(D_obj, feed_dict={\n            X: images[np.random.choice(range(len(images)), batch_size)].reshape(batch_size, x_size),\n            z: generate_z(batch_size),\n        })\n        image = sess.run(G(z), feed_dict={z: generate_z()})\n        df = sess.run(tf.sigmoid(dout_fake), feed_dict={z: generate_z()})\n        # print i, G cost, D cost, image max pixel, D output of fake\n        print(i, g_cost, d_cost, image.max(), df[0][0])\n\n    # You may wish to save or plot the image generated\n    # to see how it looks like\n    image = sess.run(G(z), feed_dict={z: generate_z()})\n    image1 = image[0].reshape([28, 28])\n    # print(image1)\n    # im = Image.fromarray(image1)\n    # im.show()\n```\n\n","source":"_posts/GAN-01.md","raw":"---\ntitle:  GAN  TensorFlow \ndate: 2017-11-01 16:09:01\ncategories: ML\ntags:\n     - Deep learning\n     - GAN\ndescription: GAN TensorFlow \n---\n\n# GAN\n\n2014 Goodfellow **GAN**\n\n- \n- \n-  D G\n- G  D \n\nG  D G D  D  G D  $1/2$  G \n\n $x$  $p_g(x)$  $p_z(z)$  $G(z;\\theta_g)$  $G$  $D(x;\\theta_d)$  $x$  $D$  $\\log (1-D(G(z)))$ G  $G$ \n$$\n\\min_G \\max_D V(D,G) = \\mathbb{E}_{x \\sim p_{\\text{data}}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_{z}(z)}[\\log (1-D(G(z)))]\n$$\n\n# \n\n\n$$\n\\mathbb{E}_{z \\sim p_{z}(z)}[\\log (1-D(G(z)))] = \\mathbb{E}_{x \\sim p_{g}(x)}[\\log (1-D(x)]\n$$\n $V(D,G)$  $x$ \n$$\nV(G, D) = \\int_{x} p_{\\text{data}}(x) \\log (D(x)) + p_g(x) \\log (1-D(x)) \\, dx\n$$\n $p_{\\text{data}}(x)$  $p_g(x)$ \n$$\nf(y) = a \\log y + b \\log (1-y)\n$$\n\n$$\nf'(y) = 0 \\Rightarrow \\frac{a}{y}-\\frac{b}{1-y}=0 \\Rightarrow y=\\frac{a}{a+b}\n$$\n\n$$\nf''(y) \\Big|_{y=\\frac{a}{a+b}} = -\\frac{a}{\\left( \\frac{a}{a+b} \\right)^{2}}-\\frac{b}{1-\\left( \\frac{a}{a+b} \\right)^{2}}<0\n$$\n $\\frac{a}{a+b}$  $a=p_{\\text{data}}(x), b=p_g(x), y=D(x)$ \n$$\nD(x) = \\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)}\n$$\n\n# \n\nGAN  $p_{\\text{data}}=p_g$ \n$$\nD_G^{\\ast}=\\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)}=\\frac{1}{2}\n$$\nGoodfellow  min max \n\n#  TensorFlow  MNIST \n\n CNN G  D \n\n```python\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.examples.tutorials.mnist import input_data\n# from PIL import Image\n\nmnist = input_data.read_data_sets(\"MNIST_data/\")\nimages = mnist.train.images\n\n\ndef xavier_initializer(shape):\n    return tf.random_normal(shape=shape, stddev=1.0 / shape[0])\n\n\n# Generator\nz_size = 100  # maybe larger\ng_w1_size = 400\ng_out_size = 28 * 28\n\n# Discriminator\nx_size = 28 * 28\nd_w1_size = 400\nd_out_size = 1\n\nz = tf.placeholder('float', shape=(None, z_size))\nX = tf.placeholder('float', shape=(None, x_size))\n\n# use dict to share variables\ng_weights = {\n    'w1': tf.Variable(xavier_initializer(shape=(z_size, g_w1_size))),\n    'b1': tf.Variable(tf.zeros(shape=[g_w1_size])),\n    'out': tf.Variable(xavier_initializer(shape=(g_w1_size, g_out_size))),\n    'b2': tf.Variable(tf.zeros(shape=[g_out_size])),\n}\n\nd_weights = {\n    'w1': tf.Variable(xavier_initializer(shape=(x_size, d_w1_size))),\n    'b1': tf.Variable(tf.zeros(shape=[d_w1_size])),\n    'out': tf.Variable(xavier_initializer(shape=(d_w1_size, d_out_size))),\n    'b2': tf.Variable(tf.zeros(shape=[d_out_size])),\n}\n\n\ndef G(z, w=g_weights):\n    # here tanh is better than relu\n    h1 = tf.tanh(tf.matmul(z, w['w1']) + w['b1'])\n    # pixel output is in range [0, 255]\n    return tf.sigmoid(tf.matmul(h1, w['out']) + w['b2']) * 255\n\n\ndef D(x, w=d_weights):\n    # here tanh is better than relu\n    h1 = tf.tanh(tf.matmul(x, w['w1']) + w['b1'])\n    h2 = tf.matmul(h1, w['out']) + w['b2']\n    return h2  # use h2 to calculate logits loss\n\n\ndef generate_z(n=1):\n    return np.random.normal(size=(n, z_size))\n\n\nsample = G(z)\n\ndout_real = D(X)\ndout_fake = D(G(z))\n\nG_obj = tf.reduce_mean(\n    tf.nn.sigmoid_cross_entropy_with_logits(logits=dout_fake, labels=tf.ones_like(dout_fake)))\nD_obj_real = tf.reduce_mean(  # use single side smoothing\n    tf.nn.sigmoid_cross_entropy_with_logits(logits=dout_real, labels=(tf.ones_like(dout_real) - 0.1)))\nD_obj_fake = tf.reduce_mean(\n    tf.nn.sigmoid_cross_entropy_with_logits(logits=dout_fake, labels=tf.zeros_like(dout_fake)))\nD_obj = D_obj_real + D_obj_fake\n\nG_opt = tf.train.AdamOptimizer().minimize(G_obj, var_list=g_weights.values())\nD_opt = tf.train.AdamOptimizer().minimize(D_obj, var_list=d_weights.values())\n\n# Training\nbatch_size = 128\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n\n    for i in range(200):\n        sess.run(D_opt, feed_dict={\n            X: images[np.random.choice(range(len(images)), batch_size)].reshape(batch_size, x_size),\n            z: generate_z(batch_size),\n        })\n        # run two phases of generator\n        sess.run(G_opt, feed_dict={\n            z: generate_z(batch_size)\n        })\n        sess.run(G_opt, feed_dict={\n            z: generate_z(batch_size)\n        })\n\n        g_cost = sess.run(G_obj, feed_dict={z: generate_z(batch_size)})\n        d_cost = sess.run(D_obj, feed_dict={\n            X: images[np.random.choice(range(len(images)), batch_size)].reshape(batch_size, x_size),\n            z: generate_z(batch_size),\n        })\n        image = sess.run(G(z), feed_dict={z: generate_z()})\n        df = sess.run(tf.sigmoid(dout_fake), feed_dict={z: generate_z()})\n        # print i, G cost, D cost, image max pixel, D output of fake\n        print(i, g_cost, d_cost, image.max(), df[0][0])\n\n    # You may wish to save or plot the image generated\n    # to see how it looks like\n    image = sess.run(G(z), feed_dict={z: generate_z()})\n    image1 = image[0].reshape([28, 28])\n    # print(image1)\n    # im = Image.fromarray(image1)\n    # im.show()\n```\n\n","slug":"GAN-01","published":1,"updated":"2021-07-19T13:03:31.695Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrancn970002g0vwyu6y20ry","content":"<h1 id=\"GAN\"><a href=\"#GAN\" class=\"headerlink\" title=\"GAN\"></a>GAN</h1><p>2014 Goodfellow <strong>GAN</strong></p>\n<ul>\n<li></li>\n<li></li>\n<li> D G</li>\n<li>G  D </li>\n</ul>\n<p>G  D G D  D  G D  $1/2$  G </p>\n<p> $x$  $p_g(x)$  $p_z(z)$  $G(z;\\theta_g)$  $G$  $D(x;\\theta_d)$  $x$  $D$  $\\log (1-D(G(z)))$ G  $G$ <br>$$<br>\\min_G \\max_D V(D,G) = \\mathbb{E}<em>{x \\sim p</em>{\\text{data}}(x)}[\\log D(x)] + \\mathbb{E}<em>{z \\sim p</em>{z}(z)}[\\log (1-D(G(z)))]<br>$$</p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p><br>$$<br>\\mathbb{E}<em>{z \\sim p</em>{z}(z)}[\\log (1-D(G(z)))] = \\mathbb{E}<em>{x \\sim p</em>{g}(x)}[\\log (1-D(x)]<br>$$<br> $V(D,G)$  $x$ <br>$$<br>V(G, D) = \\int_{x} p_{\\text{data}}(x) \\log (D(x)) + p_g(x) \\log (1-D(x)) \\, dx<br>$$<br> $p_{\\text{data}}(x)$  $p_g(x)$ <br>$$<br>f(y) = a \\log y + b \\log (1-y)<br>$$<br><br>$$<br>f(y) = 0 \\Rightarrow \\frac{a}{y}-\\frac{b}{1-y}=0 \\Rightarrow y=\\frac{a}{a+b}<br>$$<br><br>$$<br>f(y) \\Big|<em>{y=\\frac{a}{a+b}} = -\\frac{a}{\\left( \\frac{a}{a+b} \\right)^{2}}-\\frac{b}{1-\\left( \\frac{a}{a+b} \\right)^{2}}&lt;0<br>$$<br> $\\frac{a}{a+b}$  $a=p</em>{\\text{data}}(x), b=p_g(x), y=D(x)$ <br>$$<br>D(x) = \\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)}<br>$$</p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>GAN  $p_{\\text{data}}=p_g$ <br>$$<br>D_G^{\\ast}=\\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)}=\\frac{1}{2}<br>$$<br>Goodfellow  min max </p>\n<h1 id=\"-TensorFlow--MNIST-\"><a href=\"#-TensorFlow--MNIST-\" class=\"headerlink\" title=\" TensorFlow  MNIST \"></a> TensorFlow  MNIST </h1><p> CNN G  D </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">from</span> tensorflow.examples.tutorials.mnist <span class=\"keyword\">import</span> input_data</span><br><span class=\"line\"><span class=\"comment\"># from PIL import Image</span></span><br><span class=\"line\"></span><br><span class=\"line\">mnist = input_data.read_data_sets(<span class=\"string\">\"MNIST_data/\"</span>)</span><br><span class=\"line\">images = mnist.train.images</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">xavier_initializer</span><span class=\"params\">(shape)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> tf.random_normal(shape=shape, stddev=<span class=\"number\">1.0</span> / shape[<span class=\"number\">0</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Generator</span></span><br><span class=\"line\">z_size = <span class=\"number\">100</span>  <span class=\"comment\"># maybe larger</span></span><br><span class=\"line\">g_w1_size = <span class=\"number\">400</span></span><br><span class=\"line\">g_out_size = <span class=\"number\">28</span> * <span class=\"number\">28</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Discriminator</span></span><br><span class=\"line\">x_size = <span class=\"number\">28</span> * <span class=\"number\">28</span></span><br><span class=\"line\">d_w1_size = <span class=\"number\">400</span></span><br><span class=\"line\">d_out_size = <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">z = tf.placeholder(<span class=\"string\">'float'</span>, shape=(<span class=\"literal\">None</span>, z_size))</span><br><span class=\"line\">X = tf.placeholder(<span class=\"string\">'float'</span>, shape=(<span class=\"literal\">None</span>, x_size))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># use dict to share variables</span></span><br><span class=\"line\">g_weights = &#123;</span><br><span class=\"line\">    <span class=\"string\">'w1'</span>: tf.Variable(xavier_initializer(shape=(z_size, g_w1_size))),</span><br><span class=\"line\">    <span class=\"string\">'b1'</span>: tf.Variable(tf.zeros(shape=[g_w1_size])),</span><br><span class=\"line\">    <span class=\"string\">'out'</span>: tf.Variable(xavier_initializer(shape=(g_w1_size, g_out_size))),</span><br><span class=\"line\">    <span class=\"string\">'b2'</span>: tf.Variable(tf.zeros(shape=[g_out_size])),</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">d_weights = &#123;</span><br><span class=\"line\">    <span class=\"string\">'w1'</span>: tf.Variable(xavier_initializer(shape=(x_size, d_w1_size))),</span><br><span class=\"line\">    <span class=\"string\">'b1'</span>: tf.Variable(tf.zeros(shape=[d_w1_size])),</span><br><span class=\"line\">    <span class=\"string\">'out'</span>: tf.Variable(xavier_initializer(shape=(d_w1_size, d_out_size))),</span><br><span class=\"line\">    <span class=\"string\">'b2'</span>: tf.Variable(tf.zeros(shape=[d_out_size])),</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">G</span><span class=\"params\">(z, w=g_weights)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># here tanh is better than relu</span></span><br><span class=\"line\">    h1 = tf.tanh(tf.matmul(z, w[<span class=\"string\">'w1'</span>]) + w[<span class=\"string\">'b1'</span>])</span><br><span class=\"line\">    <span class=\"comment\"># pixel output is in range [0, 255]</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> tf.sigmoid(tf.matmul(h1, w[<span class=\"string\">'out'</span>]) + w[<span class=\"string\">'b2'</span>]) * <span class=\"number\">255</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">D</span><span class=\"params\">(x, w=d_weights)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># here tanh is better than relu</span></span><br><span class=\"line\">    h1 = tf.tanh(tf.matmul(x, w[<span class=\"string\">'w1'</span>]) + w[<span class=\"string\">'b1'</span>])</span><br><span class=\"line\">    h2 = tf.matmul(h1, w[<span class=\"string\">'out'</span>]) + w[<span class=\"string\">'b2'</span>]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> h2  <span class=\"comment\"># use h2 to calculate logits loss</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generate_z</span><span class=\"params\">(n=<span class=\"number\">1</span>)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> np.random.normal(size=(n, z_size))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">sample = G(z)</span><br><span class=\"line\"></span><br><span class=\"line\">dout_real = D(X)</span><br><span class=\"line\">dout_fake = D(G(z))</span><br><span class=\"line\"></span><br><span class=\"line\">G_obj = tf.reduce_mean(</span><br><span class=\"line\">    tf.nn.sigmoid_cross_entropy_with_logits(logits=dout_fake, labels=tf.ones_like(dout_fake)))</span><br><span class=\"line\">D_obj_real = tf.reduce_mean(  <span class=\"comment\"># use single side smoothing</span></span><br><span class=\"line\">    tf.nn.sigmoid_cross_entropy_with_logits(logits=dout_real, labels=(tf.ones_like(dout_real) - <span class=\"number\">0.1</span>)))</span><br><span class=\"line\">D_obj_fake = tf.reduce_mean(</span><br><span class=\"line\">    tf.nn.sigmoid_cross_entropy_with_logits(logits=dout_fake, labels=tf.zeros_like(dout_fake)))</span><br><span class=\"line\">D_obj = D_obj_real + D_obj_fake</span><br><span class=\"line\"></span><br><span class=\"line\">G_opt = tf.train.AdamOptimizer().minimize(G_obj, var_list=g_weights.values())</span><br><span class=\"line\">D_opt = tf.train.AdamOptimizer().minimize(D_obj, var_list=d_weights.values())</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Training</span></span><br><span class=\"line\">batch_size = <span class=\"number\">128</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">with</span> tf.Session() <span class=\"keyword\">as</span> sess:</span><br><span class=\"line\">    sess.run(tf.global_variables_initializer())</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">200</span>):</span><br><span class=\"line\">        sess.run(D_opt, feed_dict=&#123;</span><br><span class=\"line\">            X: images[np.random.choice(range(len(images)), batch_size)].reshape(batch_size, x_size),</span><br><span class=\"line\">            z: generate_z(batch_size),</span><br><span class=\"line\">        &#125;)</span><br><span class=\"line\">        <span class=\"comment\"># run two phases of generator</span></span><br><span class=\"line\">        sess.run(G_opt, feed_dict=&#123;</span><br><span class=\"line\">            z: generate_z(batch_size)</span><br><span class=\"line\">        &#125;)</span><br><span class=\"line\">        sess.run(G_opt, feed_dict=&#123;</span><br><span class=\"line\">            z: generate_z(batch_size)</span><br><span class=\"line\">        &#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">        g_cost = sess.run(G_obj, feed_dict=&#123;z: generate_z(batch_size)&#125;)</span><br><span class=\"line\">        d_cost = sess.run(D_obj, feed_dict=&#123;</span><br><span class=\"line\">            X: images[np.random.choice(range(len(images)), batch_size)].reshape(batch_size, x_size),</span><br><span class=\"line\">            z: generate_z(batch_size),</span><br><span class=\"line\">        &#125;)</span><br><span class=\"line\">        image = sess.run(G(z), feed_dict=&#123;z: generate_z()&#125;)</span><br><span class=\"line\">        df = sess.run(tf.sigmoid(dout_fake), feed_dict=&#123;z: generate_z()&#125;)</span><br><span class=\"line\">        <span class=\"comment\"># print i, G cost, D cost, image max pixel, D output of fake</span></span><br><span class=\"line\">        print(i, g_cost, d_cost, image.max(), df[<span class=\"number\">0</span>][<span class=\"number\">0</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># You may wish to save or plot the image generated</span></span><br><span class=\"line\">    <span class=\"comment\"># to see how it looks like</span></span><br><span class=\"line\">    image = sess.run(G(z), feed_dict=&#123;z: generate_z()&#125;)</span><br><span class=\"line\">    image1 = image[<span class=\"number\">0</span>].reshape([<span class=\"number\">28</span>, <span class=\"number\">28</span>])</span><br><span class=\"line\">    <span class=\"comment\"># print(image1)</span></span><br><span class=\"line\">    <span class=\"comment\"># im = Image.fromarray(image1)</span></span><br><span class=\"line\">    <span class=\"comment\"># im.show()</span></span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"GAN\"><a href=\"#GAN\" class=\"headerlink\" title=\"GAN\"></a>GAN</h1><p>2014 Goodfellow <strong>GAN</strong></p>\n<ul>\n<li></li>\n<li></li>\n<li> D G</li>\n<li>G  D </li>\n</ul>\n<p>G  D G D  D  G D  $1/2$  G </p>\n<p> $x$  $p_g(x)$  $p_z(z)$  $G(z;\\theta_g)$  $G$  $D(x;\\theta_d)$  $x$  $D$  $\\log (1-D(G(z)))$ G  $G$ <br>$$<br>\\min_G \\max_D V(D,G) = \\mathbb{E}<em>{x \\sim p</em>{\\text{data}}(x)}[\\log D(x)] + \\mathbb{E}<em>{z \\sim p</em>{z}(z)}[\\log (1-D(G(z)))]<br>$$</p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p><br>$$<br>\\mathbb{E}<em>{z \\sim p</em>{z}(z)}[\\log (1-D(G(z)))] = \\mathbb{E}<em>{x \\sim p</em>{g}(x)}[\\log (1-D(x)]<br>$$<br> $V(D,G)$  $x$ <br>$$<br>V(G, D) = \\int_{x} p_{\\text{data}}(x) \\log (D(x)) + p_g(x) \\log (1-D(x)) \\, dx<br>$$<br> $p_{\\text{data}}(x)$  $p_g(x)$ <br>$$<br>f(y) = a \\log y + b \\log (1-y)<br>$$<br><br>$$<br>f(y) = 0 \\Rightarrow \\frac{a}{y}-\\frac{b}{1-y}=0 \\Rightarrow y=\\frac{a}{a+b}<br>$$<br><br>$$<br>f(y) \\Big|<em>{y=\\frac{a}{a+b}} = -\\frac{a}{\\left( \\frac{a}{a+b} \\right)^{2}}-\\frac{b}{1-\\left( \\frac{a}{a+b} \\right)^{2}}&lt;0<br>$$<br> $\\frac{a}{a+b}$  $a=p</em>{\\text{data}}(x), b=p_g(x), y=D(x)$ <br>$$<br>D(x) = \\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)}<br>$$</p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>GAN  $p_{\\text{data}}=p_g$ <br>$$<br>D_G^{\\ast}=\\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)}=\\frac{1}{2}<br>$$<br>Goodfellow  min max </p>\n<h1 id=\"-TensorFlow--MNIST-\"><a href=\"#-TensorFlow--MNIST-\" class=\"headerlink\" title=\" TensorFlow  MNIST \"></a> TensorFlow  MNIST </h1><p> CNN G  D </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">from</span> tensorflow.examples.tutorials.mnist <span class=\"keyword\">import</span> input_data</span><br><span class=\"line\"><span class=\"comment\"># from PIL import Image</span></span><br><span class=\"line\"></span><br><span class=\"line\">mnist = input_data.read_data_sets(<span class=\"string\">\"MNIST_data/\"</span>)</span><br><span class=\"line\">images = mnist.train.images</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">xavier_initializer</span><span class=\"params\">(shape)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> tf.random_normal(shape=shape, stddev=<span class=\"number\">1.0</span> / shape[<span class=\"number\">0</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Generator</span></span><br><span class=\"line\">z_size = <span class=\"number\">100</span>  <span class=\"comment\"># maybe larger</span></span><br><span class=\"line\">g_w1_size = <span class=\"number\">400</span></span><br><span class=\"line\">g_out_size = <span class=\"number\">28</span> * <span class=\"number\">28</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Discriminator</span></span><br><span class=\"line\">x_size = <span class=\"number\">28</span> * <span class=\"number\">28</span></span><br><span class=\"line\">d_w1_size = <span class=\"number\">400</span></span><br><span class=\"line\">d_out_size = <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">z = tf.placeholder(<span class=\"string\">'float'</span>, shape=(<span class=\"literal\">None</span>, z_size))</span><br><span class=\"line\">X = tf.placeholder(<span class=\"string\">'float'</span>, shape=(<span class=\"literal\">None</span>, x_size))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># use dict to share variables</span></span><br><span class=\"line\">g_weights = &#123;</span><br><span class=\"line\">    <span class=\"string\">'w1'</span>: tf.Variable(xavier_initializer(shape=(z_size, g_w1_size))),</span><br><span class=\"line\">    <span class=\"string\">'b1'</span>: tf.Variable(tf.zeros(shape=[g_w1_size])),</span><br><span class=\"line\">    <span class=\"string\">'out'</span>: tf.Variable(xavier_initializer(shape=(g_w1_size, g_out_size))),</span><br><span class=\"line\">    <span class=\"string\">'b2'</span>: tf.Variable(tf.zeros(shape=[g_out_size])),</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">d_weights = &#123;</span><br><span class=\"line\">    <span class=\"string\">'w1'</span>: tf.Variable(xavier_initializer(shape=(x_size, d_w1_size))),</span><br><span class=\"line\">    <span class=\"string\">'b1'</span>: tf.Variable(tf.zeros(shape=[d_w1_size])),</span><br><span class=\"line\">    <span class=\"string\">'out'</span>: tf.Variable(xavier_initializer(shape=(d_w1_size, d_out_size))),</span><br><span class=\"line\">    <span class=\"string\">'b2'</span>: tf.Variable(tf.zeros(shape=[d_out_size])),</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">G</span><span class=\"params\">(z, w=g_weights)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># here tanh is better than relu</span></span><br><span class=\"line\">    h1 = tf.tanh(tf.matmul(z, w[<span class=\"string\">'w1'</span>]) + w[<span class=\"string\">'b1'</span>])</span><br><span class=\"line\">    <span class=\"comment\"># pixel output is in range [0, 255]</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> tf.sigmoid(tf.matmul(h1, w[<span class=\"string\">'out'</span>]) + w[<span class=\"string\">'b2'</span>]) * <span class=\"number\">255</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">D</span><span class=\"params\">(x, w=d_weights)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># here tanh is better than relu</span></span><br><span class=\"line\">    h1 = tf.tanh(tf.matmul(x, w[<span class=\"string\">'w1'</span>]) + w[<span class=\"string\">'b1'</span>])</span><br><span class=\"line\">    h2 = tf.matmul(h1, w[<span class=\"string\">'out'</span>]) + w[<span class=\"string\">'b2'</span>]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> h2  <span class=\"comment\"># use h2 to calculate logits loss</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generate_z</span><span class=\"params\">(n=<span class=\"number\">1</span>)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> np.random.normal(size=(n, z_size))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">sample = G(z)</span><br><span class=\"line\"></span><br><span class=\"line\">dout_real = D(X)</span><br><span class=\"line\">dout_fake = D(G(z))</span><br><span class=\"line\"></span><br><span class=\"line\">G_obj = tf.reduce_mean(</span><br><span class=\"line\">    tf.nn.sigmoid_cross_entropy_with_logits(logits=dout_fake, labels=tf.ones_like(dout_fake)))</span><br><span class=\"line\">D_obj_real = tf.reduce_mean(  <span class=\"comment\"># use single side smoothing</span></span><br><span class=\"line\">    tf.nn.sigmoid_cross_entropy_with_logits(logits=dout_real, labels=(tf.ones_like(dout_real) - <span class=\"number\">0.1</span>)))</span><br><span class=\"line\">D_obj_fake = tf.reduce_mean(</span><br><span class=\"line\">    tf.nn.sigmoid_cross_entropy_with_logits(logits=dout_fake, labels=tf.zeros_like(dout_fake)))</span><br><span class=\"line\">D_obj = D_obj_real + D_obj_fake</span><br><span class=\"line\"></span><br><span class=\"line\">G_opt = tf.train.AdamOptimizer().minimize(G_obj, var_list=g_weights.values())</span><br><span class=\"line\">D_opt = tf.train.AdamOptimizer().minimize(D_obj, var_list=d_weights.values())</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Training</span></span><br><span class=\"line\">batch_size = <span class=\"number\">128</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">with</span> tf.Session() <span class=\"keyword\">as</span> sess:</span><br><span class=\"line\">    sess.run(tf.global_variables_initializer())</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">200</span>):</span><br><span class=\"line\">        sess.run(D_opt, feed_dict=&#123;</span><br><span class=\"line\">            X: images[np.random.choice(range(len(images)), batch_size)].reshape(batch_size, x_size),</span><br><span class=\"line\">            z: generate_z(batch_size),</span><br><span class=\"line\">        &#125;)</span><br><span class=\"line\">        <span class=\"comment\"># run two phases of generator</span></span><br><span class=\"line\">        sess.run(G_opt, feed_dict=&#123;</span><br><span class=\"line\">            z: generate_z(batch_size)</span><br><span class=\"line\">        &#125;)</span><br><span class=\"line\">        sess.run(G_opt, feed_dict=&#123;</span><br><span class=\"line\">            z: generate_z(batch_size)</span><br><span class=\"line\">        &#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">        g_cost = sess.run(G_obj, feed_dict=&#123;z: generate_z(batch_size)&#125;)</span><br><span class=\"line\">        d_cost = sess.run(D_obj, feed_dict=&#123;</span><br><span class=\"line\">            X: images[np.random.choice(range(len(images)), batch_size)].reshape(batch_size, x_size),</span><br><span class=\"line\">            z: generate_z(batch_size),</span><br><span class=\"line\">        &#125;)</span><br><span class=\"line\">        image = sess.run(G(z), feed_dict=&#123;z: generate_z()&#125;)</span><br><span class=\"line\">        df = sess.run(tf.sigmoid(dout_fake), feed_dict=&#123;z: generate_z()&#125;)</span><br><span class=\"line\">        <span class=\"comment\"># print i, G cost, D cost, image max pixel, D output of fake</span></span><br><span class=\"line\">        print(i, g_cost, d_cost, image.max(), df[<span class=\"number\">0</span>][<span class=\"number\">0</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># You may wish to save or plot the image generated</span></span><br><span class=\"line\">    <span class=\"comment\"># to see how it looks like</span></span><br><span class=\"line\">    image = sess.run(G(z), feed_dict=&#123;z: generate_z()&#125;)</span><br><span class=\"line\">    image1 = image[<span class=\"number\">0</span>].reshape([<span class=\"number\">28</span>, <span class=\"number\">28</span>])</span><br><span class=\"line\">    <span class=\"comment\"># print(image1)</span></span><br><span class=\"line\">    <span class=\"comment\"># im = Image.fromarray(image1)</span></span><br><span class=\"line\">    <span class=\"comment\"># im.show()</span></span><br></pre></td></tr></table></figure>\n"},{"title":" Wasserstein GAN  TensorFlow ","date":"2017-11-04T03:21:51.000Z","description":" Wasserstein GAN  TensorFlow ","_content":"\n#  GAN \n\n loss  $P_{\\text{data}}$  $P_g$  JS  $P_{\\text{data}}$  $P_g$ JS  $\\log 2$  $0$ \n\n\n$$\nKL(P_{g} \\Vert P_{\\text{data}}) - 2 JS(P_{\\text{data}} \\Vert P_{g})\n$$\n KL  JS  collapse mode\n\n# Wasserstein GAN \n\nWasserstein GAN  GAN \n\n-  sigmoid\n-  loss  log\n-  c\n-  momentum  Adam RMSPropSGD \n\nPS Adam loss  Adam  cos  loss  RMSProp \n\nPPS learning rate $\\alpha=0.00005$ \n\n#  TensorFlow  MNIST \n\n```python\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport os\n\n\nmb_size = 32\nX_dim = 784\nz_dim = 10\nh_dim = 128\n\nmnist = input_data.read_data_sets('MNIST_data/', one_hot=True)\n\n\ndef plot(samples):\n    fig = plt.figure(figsize=(4, 4))\n    gs = gridspec.GridSpec(4, 4)\n    gs.update(wspace=0.05, hspace=0.05)\n\n    for i, sample in enumerate(samples):\n        ax = plt.subplot(gs[i])\n        plt.axis('off')\n        ax.set_xticklabels([])\n        ax.set_yticklabels([])\n        ax.set_aspect('equal')\n        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n\n    return fig\n\n\ndef xavier_init(size):\n    in_dim = size[0]\n    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n    return tf.random_normal(shape=size, stddev=xavier_stddev)\n\n\nX = tf.placeholder(tf.float32, shape=[None, X_dim])\n\nD_W1 = tf.Variable(xavier_init([X_dim, h_dim]))\nD_b1 = tf.Variable(tf.zeros(shape=[h_dim]))\n\nD_W2 = tf.Variable(xavier_init([h_dim, 1]))\nD_b2 = tf.Variable(tf.zeros(shape=[1]))\n\ntheta_D = [D_W1, D_W2, D_b1, D_b2]\n\n\nz = tf.placeholder(tf.float32, shape=[None, z_dim])\n\nG_W1 = tf.Variable(xavier_init([z_dim, h_dim]))\nG_b1 = tf.Variable(tf.zeros(shape=[h_dim]))\n\nG_W2 = tf.Variable(xavier_init([h_dim, X_dim]))\nG_b2 = tf.Variable(tf.zeros(shape=[X_dim]))\n\ntheta_G = [G_W1, G_W2, G_b1, G_b2]\n\n\ndef sample_z(m, n):\n    return np.random.uniform(-1., 1., size=[m, n])\n\n\ndef generator(z):\n    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)\n    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2\n    G_prob = tf.nn.sigmoid(G_log_prob)\n    return G_prob\n\n\ndef discriminator(x):\n    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)\n    out = tf.matmul(D_h1, D_W2) + D_b2\n    return out\n\n\nG_sample = generator(z)\nD_real = discriminator(X)\nD_fake = discriminator(G_sample)\n\nD_loss = tf.reduce_mean(D_real) - tf.reduce_mean(D_fake)\nG_loss = -tf.reduce_mean(D_fake)\n\nD_solver = (tf.train.RMSPropOptimizer(learning_rate=1e-4)\n            .minimize(-D_loss, var_list=theta_D))\nG_solver = (tf.train.RMSPropOptimizer(learning_rate=1e-4)\n            .minimize(G_loss, var_list=theta_G))\n\nclip_D = [p.assign(tf.clip_by_value(p, -0.01, 0.01)) for p in theta_D]\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\nif not os.path.exists('out/'):\n    os.makedirs('out/')\n\ni = 0\n\nfor it in range(1000000):\n    for _ in range(5):\n        X_mb, _ = mnist.train.next_batch(mb_size)\n\n        _, D_loss_curr, _ = sess.run(\n            [D_solver, D_loss, clip_D],\n            feed_dict={X: X_mb, z: sample_z(mb_size, z_dim)}\n        )\n\n    _, G_loss_curr = sess.run(\n        [G_solver, G_loss],\n        feed_dict={z: sample_z(mb_size, z_dim)}\n    )\n\n    if it % 100 == 0:\n        print('Iter: {}; D loss: {:.4}; G_loss: {:.4}'.format(it, D_loss_curr, G_loss_curr))\n\n        if it % 1000 == 0:\n            samples = sess.run(G_sample, feed_dict={z: sample_z(16, z_dim)})\n\n            fig = plot(samples)\n            plt.savefig('out/{}.png'.format(str(i).zfill(3)), bbox_inches='tight')\n            i += 1\n            plt.close(fig)\n```\n\n","source":"_posts/GAN-02.md","raw":"---\ntitle:  Wasserstein GAN  TensorFlow \ndate: 2017-11-04 11:21:51\ncategories: ML\ntags:\n     - Deep learning\n     - GAN\ndescription:  Wasserstein GAN  TensorFlow \n---\n\n#  GAN \n\n loss  $P_{\\text{data}}$  $P_g$  JS  $P_{\\text{data}}$  $P_g$ JS  $\\log 2$  $0$ \n\n\n$$\nKL(P_{g} \\Vert P_{\\text{data}}) - 2 JS(P_{\\text{data}} \\Vert P_{g})\n$$\n KL  JS  collapse mode\n\n# Wasserstein GAN \n\nWasserstein GAN  GAN \n\n-  sigmoid\n-  loss  log\n-  c\n-  momentum  Adam RMSPropSGD \n\nPS Adam loss  Adam  cos  loss  RMSProp \n\nPPS learning rate $\\alpha=0.00005$ \n\n#  TensorFlow  MNIST \n\n```python\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport os\n\n\nmb_size = 32\nX_dim = 784\nz_dim = 10\nh_dim = 128\n\nmnist = input_data.read_data_sets('MNIST_data/', one_hot=True)\n\n\ndef plot(samples):\n    fig = plt.figure(figsize=(4, 4))\n    gs = gridspec.GridSpec(4, 4)\n    gs.update(wspace=0.05, hspace=0.05)\n\n    for i, sample in enumerate(samples):\n        ax = plt.subplot(gs[i])\n        plt.axis('off')\n        ax.set_xticklabels([])\n        ax.set_yticklabels([])\n        ax.set_aspect('equal')\n        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n\n    return fig\n\n\ndef xavier_init(size):\n    in_dim = size[0]\n    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n    return tf.random_normal(shape=size, stddev=xavier_stddev)\n\n\nX = tf.placeholder(tf.float32, shape=[None, X_dim])\n\nD_W1 = tf.Variable(xavier_init([X_dim, h_dim]))\nD_b1 = tf.Variable(tf.zeros(shape=[h_dim]))\n\nD_W2 = tf.Variable(xavier_init([h_dim, 1]))\nD_b2 = tf.Variable(tf.zeros(shape=[1]))\n\ntheta_D = [D_W1, D_W2, D_b1, D_b2]\n\n\nz = tf.placeholder(tf.float32, shape=[None, z_dim])\n\nG_W1 = tf.Variable(xavier_init([z_dim, h_dim]))\nG_b1 = tf.Variable(tf.zeros(shape=[h_dim]))\n\nG_W2 = tf.Variable(xavier_init([h_dim, X_dim]))\nG_b2 = tf.Variable(tf.zeros(shape=[X_dim]))\n\ntheta_G = [G_W1, G_W2, G_b1, G_b2]\n\n\ndef sample_z(m, n):\n    return np.random.uniform(-1., 1., size=[m, n])\n\n\ndef generator(z):\n    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)\n    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2\n    G_prob = tf.nn.sigmoid(G_log_prob)\n    return G_prob\n\n\ndef discriminator(x):\n    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)\n    out = tf.matmul(D_h1, D_W2) + D_b2\n    return out\n\n\nG_sample = generator(z)\nD_real = discriminator(X)\nD_fake = discriminator(G_sample)\n\nD_loss = tf.reduce_mean(D_real) - tf.reduce_mean(D_fake)\nG_loss = -tf.reduce_mean(D_fake)\n\nD_solver = (tf.train.RMSPropOptimizer(learning_rate=1e-4)\n            .minimize(-D_loss, var_list=theta_D))\nG_solver = (tf.train.RMSPropOptimizer(learning_rate=1e-4)\n            .minimize(G_loss, var_list=theta_G))\n\nclip_D = [p.assign(tf.clip_by_value(p, -0.01, 0.01)) for p in theta_D]\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\nif not os.path.exists('out/'):\n    os.makedirs('out/')\n\ni = 0\n\nfor it in range(1000000):\n    for _ in range(5):\n        X_mb, _ = mnist.train.next_batch(mb_size)\n\n        _, D_loss_curr, _ = sess.run(\n            [D_solver, D_loss, clip_D],\n            feed_dict={X: X_mb, z: sample_z(mb_size, z_dim)}\n        )\n\n    _, G_loss_curr = sess.run(\n        [G_solver, G_loss],\n        feed_dict={z: sample_z(mb_size, z_dim)}\n    )\n\n    if it % 100 == 0:\n        print('Iter: {}; D loss: {:.4}; G_loss: {:.4}'.format(it, D_loss_curr, G_loss_curr))\n\n        if it % 1000 == 0:\n            samples = sess.run(G_sample, feed_dict={z: sample_z(16, z_dim)})\n\n            fig = plot(samples)\n            plt.savefig('out/{}.png'.format(str(i).zfill(3)), bbox_inches='tight')\n            i += 1\n            plt.close(fig)\n```\n\n","slug":"GAN-02","published":1,"updated":"2021-07-19T13:03:31.695Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrancn9a0005g0vwizrjyp4n","content":"<h1 id=\"-GAN-\"><a href=\"#-GAN-\" class=\"headerlink\" title=\" GAN \"></a> GAN </h1><p> loss  $P_{\\text{data}}$  $P_g$  JS  $P_{\\text{data}}$  $P_g$ JS  $\\log 2$  $0$ </p>\n<p><br>$$<br>KL(P_{g} \\Vert P_{\\text{data}}) - 2 JS(P_{\\text{data}} \\Vert P_{g})<br>$$<br> KL  JS  collapse mode</p>\n<h1 id=\"Wasserstein-GAN-\"><a href=\"#Wasserstein-GAN-\" class=\"headerlink\" title=\"Wasserstein GAN \"></a>Wasserstein GAN </h1><p>Wasserstein GAN  GAN </p>\n<ul>\n<li> sigmoid</li>\n<li> loss  log</li>\n<li> c</li>\n<li> momentum  Adam RMSPropSGD </li>\n</ul>\n<p>PS Adam loss  Adam  cos  loss  RMSProp </p>\n<p>PPS learning rate $\\alpha=0.00005$ </p>\n<h1 id=\"-TensorFlow--MNIST-\"><a href=\"#-TensorFlow--MNIST-\" class=\"headerlink\" title=\" TensorFlow  MNIST \"></a> TensorFlow  MNIST </h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf</span><br><span class=\"line\"><span class=\"keyword\">from</span> tensorflow.examples.tutorials.mnist <span class=\"keyword\">import</span> input_data</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.gridspec <span class=\"keyword\">as</span> gridspec</span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">mb_size = <span class=\"number\">32</span></span><br><span class=\"line\">X_dim = <span class=\"number\">784</span></span><br><span class=\"line\">z_dim = <span class=\"number\">10</span></span><br><span class=\"line\">h_dim = <span class=\"number\">128</span></span><br><span class=\"line\"></span><br><span class=\"line\">mnist = input_data.read_data_sets(<span class=\"string\">'MNIST_data/'</span>, one_hot=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">plot</span><span class=\"params\">(samples)</span>:</span></span><br><span class=\"line\">    fig = plt.figure(figsize=(<span class=\"number\">4</span>, <span class=\"number\">4</span>))</span><br><span class=\"line\">    gs = gridspec.GridSpec(<span class=\"number\">4</span>, <span class=\"number\">4</span>)</span><br><span class=\"line\">    gs.update(wspace=<span class=\"number\">0.05</span>, hspace=<span class=\"number\">0.05</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i, sample <span class=\"keyword\">in</span> enumerate(samples):</span><br><span class=\"line\">        ax = plt.subplot(gs[i])</span><br><span class=\"line\">        plt.axis(<span class=\"string\">'off'</span>)</span><br><span class=\"line\">        ax.set_xticklabels([])</span><br><span class=\"line\">        ax.set_yticklabels([])</span><br><span class=\"line\">        ax.set_aspect(<span class=\"string\">'equal'</span>)</span><br><span class=\"line\">        plt.imshow(sample.reshape(<span class=\"number\">28</span>, <span class=\"number\">28</span>), cmap=<span class=\"string\">'Greys_r'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> fig</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">xavier_init</span><span class=\"params\">(size)</span>:</span></span><br><span class=\"line\">    in_dim = size[<span class=\"number\">0</span>]</span><br><span class=\"line\">    xavier_stddev = <span class=\"number\">1.</span> / tf.sqrt(in_dim / <span class=\"number\">2.</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> tf.random_normal(shape=size, stddev=xavier_stddev)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">X = tf.placeholder(tf.float32, shape=[<span class=\"literal\">None</span>, X_dim])</span><br><span class=\"line\"></span><br><span class=\"line\">D_W1 = tf.Variable(xavier_init([X_dim, h_dim]))</span><br><span class=\"line\">D_b1 = tf.Variable(tf.zeros(shape=[h_dim]))</span><br><span class=\"line\"></span><br><span class=\"line\">D_W2 = tf.Variable(xavier_init([h_dim, <span class=\"number\">1</span>]))</span><br><span class=\"line\">D_b2 = tf.Variable(tf.zeros(shape=[<span class=\"number\">1</span>]))</span><br><span class=\"line\"></span><br><span class=\"line\">theta_D = [D_W1, D_W2, D_b1, D_b2]</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">z = tf.placeholder(tf.float32, shape=[<span class=\"literal\">None</span>, z_dim])</span><br><span class=\"line\"></span><br><span class=\"line\">G_W1 = tf.Variable(xavier_init([z_dim, h_dim]))</span><br><span class=\"line\">G_b1 = tf.Variable(tf.zeros(shape=[h_dim]))</span><br><span class=\"line\"></span><br><span class=\"line\">G_W2 = tf.Variable(xavier_init([h_dim, X_dim]))</span><br><span class=\"line\">G_b2 = tf.Variable(tf.zeros(shape=[X_dim]))</span><br><span class=\"line\"></span><br><span class=\"line\">theta_G = [G_W1, G_W2, G_b1, G_b2]</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sample_z</span><span class=\"params\">(m, n)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> np.random.uniform(<span class=\"number\">-1.</span>, <span class=\"number\">1.</span>, size=[m, n])</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generator</span><span class=\"params\">(z)</span>:</span></span><br><span class=\"line\">    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)</span><br><span class=\"line\">    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2</span><br><span class=\"line\">    G_prob = tf.nn.sigmoid(G_log_prob)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> G_prob</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">discriminator</span><span class=\"params\">(x)</span>:</span></span><br><span class=\"line\">    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)</span><br><span class=\"line\">    out = tf.matmul(D_h1, D_W2) + D_b2</span><br><span class=\"line\">    <span class=\"keyword\">return</span> out</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">G_sample = generator(z)</span><br><span class=\"line\">D_real = discriminator(X)</span><br><span class=\"line\">D_fake = discriminator(G_sample)</span><br><span class=\"line\"></span><br><span class=\"line\">D_loss = tf.reduce_mean(D_real) - tf.reduce_mean(D_fake)</span><br><span class=\"line\">G_loss = -tf.reduce_mean(D_fake)</span><br><span class=\"line\"></span><br><span class=\"line\">D_solver = (tf.train.RMSPropOptimizer(learning_rate=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\">            .minimize(-D_loss, var_list=theta_D))</span><br><span class=\"line\">G_solver = (tf.train.RMSPropOptimizer(learning_rate=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\">            .minimize(G_loss, var_list=theta_G))</span><br><span class=\"line\"></span><br><span class=\"line\">clip_D = [p.assign(tf.clip_by_value(p, <span class=\"number\">-0.01</span>, <span class=\"number\">0.01</span>)) <span class=\"keyword\">for</span> p <span class=\"keyword\">in</span> theta_D]</span><br><span class=\"line\"></span><br><span class=\"line\">sess = tf.Session()</span><br><span class=\"line\">sess.run(tf.global_variables_initializer())</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> <span class=\"keyword\">not</span> os.path.exists(<span class=\"string\">'out/'</span>):</span><br><span class=\"line\">    os.makedirs(<span class=\"string\">'out/'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">i = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> it <span class=\"keyword\">in</span> range(<span class=\"number\">1000000</span>):</span><br><span class=\"line\">    <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> range(<span class=\"number\">5</span>):</span><br><span class=\"line\">        X_mb, _ = mnist.train.next_batch(mb_size)</span><br><span class=\"line\"></span><br><span class=\"line\">        _, D_loss_curr, _ = sess.run(</span><br><span class=\"line\">            [D_solver, D_loss, clip_D],</span><br><span class=\"line\">            feed_dict=&#123;X: X_mb, z: sample_z(mb_size, z_dim)&#125;</span><br><span class=\"line\">        )</span><br><span class=\"line\"></span><br><span class=\"line\">    _, G_loss_curr = sess.run(</span><br><span class=\"line\">        [G_solver, G_loss],</span><br><span class=\"line\">        feed_dict=&#123;z: sample_z(mb_size, z_dim)&#125;</span><br><span class=\"line\">    )</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> it % <span class=\"number\">100</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">        print(<span class=\"string\">'Iter: &#123;&#125;; D loss: &#123;:.4&#125;; G_loss: &#123;:.4&#125;'</span>.format(it, D_loss_curr, G_loss_curr))</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> it % <span class=\"number\">1000</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            samples = sess.run(G_sample, feed_dict=&#123;z: sample_z(<span class=\"number\">16</span>, z_dim)&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">            fig = plot(samples)</span><br><span class=\"line\">            plt.savefig(<span class=\"string\">'out/&#123;&#125;.png'</span>.format(str(i).zfill(<span class=\"number\">3</span>)), bbox_inches=<span class=\"string\">'tight'</span>)</span><br><span class=\"line\">            i += <span class=\"number\">1</span></span><br><span class=\"line\">            plt.close(fig)</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"-GAN-\"><a href=\"#-GAN-\" class=\"headerlink\" title=\" GAN \"></a> GAN </h1><p> loss  $P_{\\text{data}}$  $P_g$  JS  $P_{\\text{data}}$  $P_g$ JS  $\\log 2$  $0$ </p>\n<p><br>$$<br>KL(P_{g} \\Vert P_{\\text{data}}) - 2 JS(P_{\\text{data}} \\Vert P_{g})<br>$$<br> KL  JS  collapse mode</p>\n<h1 id=\"Wasserstein-GAN-\"><a href=\"#Wasserstein-GAN-\" class=\"headerlink\" title=\"Wasserstein GAN \"></a>Wasserstein GAN </h1><p>Wasserstein GAN  GAN </p>\n<ul>\n<li> sigmoid</li>\n<li> loss  log</li>\n<li> c</li>\n<li> momentum  Adam RMSPropSGD </li>\n</ul>\n<p>PS Adam loss  Adam  cos  loss  RMSProp </p>\n<p>PPS learning rate $\\alpha=0.00005$ </p>\n<h1 id=\"-TensorFlow--MNIST-\"><a href=\"#-TensorFlow--MNIST-\" class=\"headerlink\" title=\" TensorFlow  MNIST \"></a> TensorFlow  MNIST </h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf</span><br><span class=\"line\"><span class=\"keyword\">from</span> tensorflow.examples.tutorials.mnist <span class=\"keyword\">import</span> input_data</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.gridspec <span class=\"keyword\">as</span> gridspec</span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">mb_size = <span class=\"number\">32</span></span><br><span class=\"line\">X_dim = <span class=\"number\">784</span></span><br><span class=\"line\">z_dim = <span class=\"number\">10</span></span><br><span class=\"line\">h_dim = <span class=\"number\">128</span></span><br><span class=\"line\"></span><br><span class=\"line\">mnist = input_data.read_data_sets(<span class=\"string\">'MNIST_data/'</span>, one_hot=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">plot</span><span class=\"params\">(samples)</span>:</span></span><br><span class=\"line\">    fig = plt.figure(figsize=(<span class=\"number\">4</span>, <span class=\"number\">4</span>))</span><br><span class=\"line\">    gs = gridspec.GridSpec(<span class=\"number\">4</span>, <span class=\"number\">4</span>)</span><br><span class=\"line\">    gs.update(wspace=<span class=\"number\">0.05</span>, hspace=<span class=\"number\">0.05</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i, sample <span class=\"keyword\">in</span> enumerate(samples):</span><br><span class=\"line\">        ax = plt.subplot(gs[i])</span><br><span class=\"line\">        plt.axis(<span class=\"string\">'off'</span>)</span><br><span class=\"line\">        ax.set_xticklabels([])</span><br><span class=\"line\">        ax.set_yticklabels([])</span><br><span class=\"line\">        ax.set_aspect(<span class=\"string\">'equal'</span>)</span><br><span class=\"line\">        plt.imshow(sample.reshape(<span class=\"number\">28</span>, <span class=\"number\">28</span>), cmap=<span class=\"string\">'Greys_r'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> fig</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">xavier_init</span><span class=\"params\">(size)</span>:</span></span><br><span class=\"line\">    in_dim = size[<span class=\"number\">0</span>]</span><br><span class=\"line\">    xavier_stddev = <span class=\"number\">1.</span> / tf.sqrt(in_dim / <span class=\"number\">2.</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> tf.random_normal(shape=size, stddev=xavier_stddev)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">X = tf.placeholder(tf.float32, shape=[<span class=\"literal\">None</span>, X_dim])</span><br><span class=\"line\"></span><br><span class=\"line\">D_W1 = tf.Variable(xavier_init([X_dim, h_dim]))</span><br><span class=\"line\">D_b1 = tf.Variable(tf.zeros(shape=[h_dim]))</span><br><span class=\"line\"></span><br><span class=\"line\">D_W2 = tf.Variable(xavier_init([h_dim, <span class=\"number\">1</span>]))</span><br><span class=\"line\">D_b2 = tf.Variable(tf.zeros(shape=[<span class=\"number\">1</span>]))</span><br><span class=\"line\"></span><br><span class=\"line\">theta_D = [D_W1, D_W2, D_b1, D_b2]</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">z = tf.placeholder(tf.float32, shape=[<span class=\"literal\">None</span>, z_dim])</span><br><span class=\"line\"></span><br><span class=\"line\">G_W1 = tf.Variable(xavier_init([z_dim, h_dim]))</span><br><span class=\"line\">G_b1 = tf.Variable(tf.zeros(shape=[h_dim]))</span><br><span class=\"line\"></span><br><span class=\"line\">G_W2 = tf.Variable(xavier_init([h_dim, X_dim]))</span><br><span class=\"line\">G_b2 = tf.Variable(tf.zeros(shape=[X_dim]))</span><br><span class=\"line\"></span><br><span class=\"line\">theta_G = [G_W1, G_W2, G_b1, G_b2]</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sample_z</span><span class=\"params\">(m, n)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> np.random.uniform(<span class=\"number\">-1.</span>, <span class=\"number\">1.</span>, size=[m, n])</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generator</span><span class=\"params\">(z)</span>:</span></span><br><span class=\"line\">    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)</span><br><span class=\"line\">    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2</span><br><span class=\"line\">    G_prob = tf.nn.sigmoid(G_log_prob)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> G_prob</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">discriminator</span><span class=\"params\">(x)</span>:</span></span><br><span class=\"line\">    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)</span><br><span class=\"line\">    out = tf.matmul(D_h1, D_W2) + D_b2</span><br><span class=\"line\">    <span class=\"keyword\">return</span> out</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">G_sample = generator(z)</span><br><span class=\"line\">D_real = discriminator(X)</span><br><span class=\"line\">D_fake = discriminator(G_sample)</span><br><span class=\"line\"></span><br><span class=\"line\">D_loss = tf.reduce_mean(D_real) - tf.reduce_mean(D_fake)</span><br><span class=\"line\">G_loss = -tf.reduce_mean(D_fake)</span><br><span class=\"line\"></span><br><span class=\"line\">D_solver = (tf.train.RMSPropOptimizer(learning_rate=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\">            .minimize(-D_loss, var_list=theta_D))</span><br><span class=\"line\">G_solver = (tf.train.RMSPropOptimizer(learning_rate=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\">            .minimize(G_loss, var_list=theta_G))</span><br><span class=\"line\"></span><br><span class=\"line\">clip_D = [p.assign(tf.clip_by_value(p, <span class=\"number\">-0.01</span>, <span class=\"number\">0.01</span>)) <span class=\"keyword\">for</span> p <span class=\"keyword\">in</span> theta_D]</span><br><span class=\"line\"></span><br><span class=\"line\">sess = tf.Session()</span><br><span class=\"line\">sess.run(tf.global_variables_initializer())</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> <span class=\"keyword\">not</span> os.path.exists(<span class=\"string\">'out/'</span>):</span><br><span class=\"line\">    os.makedirs(<span class=\"string\">'out/'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">i = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> it <span class=\"keyword\">in</span> range(<span class=\"number\">1000000</span>):</span><br><span class=\"line\">    <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> range(<span class=\"number\">5</span>):</span><br><span class=\"line\">        X_mb, _ = mnist.train.next_batch(mb_size)</span><br><span class=\"line\"></span><br><span class=\"line\">        _, D_loss_curr, _ = sess.run(</span><br><span class=\"line\">            [D_solver, D_loss, clip_D],</span><br><span class=\"line\">            feed_dict=&#123;X: X_mb, z: sample_z(mb_size, z_dim)&#125;</span><br><span class=\"line\">        )</span><br><span class=\"line\"></span><br><span class=\"line\">    _, G_loss_curr = sess.run(</span><br><span class=\"line\">        [G_solver, G_loss],</span><br><span class=\"line\">        feed_dict=&#123;z: sample_z(mb_size, z_dim)&#125;</span><br><span class=\"line\">    )</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> it % <span class=\"number\">100</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">        print(<span class=\"string\">'Iter: &#123;&#125;; D loss: &#123;:.4&#125;; G_loss: &#123;:.4&#125;'</span>.format(it, D_loss_curr, G_loss_curr))</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> it % <span class=\"number\">1000</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            samples = sess.run(G_sample, feed_dict=&#123;z: sample_z(<span class=\"number\">16</span>, z_dim)&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">            fig = plot(samples)</span><br><span class=\"line\">            plt.savefig(<span class=\"string\">'out/&#123;&#125;.png'</span>.format(str(i).zfill(<span class=\"number\">3</span>)), bbox_inches=<span class=\"string\">'tight'</span>)</span><br><span class=\"line\">            i += <span class=\"number\">1</span></span><br><span class=\"line\">            plt.close(fig)</span><br></pre></td></tr></table></figure>\n"},{"title":"","date":"2017-12-23T11:51:16.000Z","description":"","_content":"\n# \n\n SGDSGDMNAGAdaGradAdaDeltaAdamNadam Momentum\n\n\n\n\n\n- $w$\n- $f(w)$\n- $\\alpha$\n\n epoch $t$ \n\n> 1. $g_{t} = \\nabla f(w_{t})$\n> 2. $m_{t} = \\phi(g_{1}, g_{2}, \\cdots , g_{t}); \\, V_{t} = \\psi(g_{1}, g_{2}, \\cdots , g_{t})$\n> 3. $\\eta_{t} = \\alpha \\cdot m_{t} \\Big/ \\sqrt{V_{t}}$\n> 4. $w_{t+1} = w_{t} - \\eta_{t}$\n\n## SGD\n\nSGD $m_{t} = g_{t}; \\, V_{t} = I^{2}$\n\n$\\eta_{t} = \\alpha \\cdot g_{t}$\n\nSGD \n\n## SGD with Momentum\n\n SGD SGDM $m_{t} = \\beta_{1} \\cdot m_{t-1} + (1 - \\beta_{1}) \\cdot g_{t}$\n\n $1 / (1 - \\beta_{1})$ \n\n$\\beta_{1}$  0.9 \n\n## SGD with Nesterov Acceleration\n\nSGD \n\nNesterov  1 $g_{t} = \\nabla f(w_{t} - \\alpha \\cdot m_{t-1})$ \n\n SGDM  $m_{t}$ \n\n## AdaGrad\n\n\n\n\n\n$V_{t} = \\sum_{\\tau = 1}^{t} g_{\\tau}^{2}$ \n\n 3 $\\eta_{t} = \\alpha \\cdot m_{t} \\Big/ \\sqrt{V_{t}}$ Ada = Adaptive 0 \n\n 0 \n\n## AdaDelta / RMSProp\n\n AdaGrad  Delta \n\n$V_{t} = \\beta_{2} \\cdot V_{t-1} + (1 - \\beta_{2}) \\cdot g_{t}^{2}$ \n\n## Adam / Nadam\n\n Adam  Nadam \n\nAdam = Adaptive + Momentum\n\nNadam = Nesterov + Adam\n\n $\\beta_{1}$  $\\beta_{2}$ \n\n#  SGD\n\n Adam  Nadam  SGD \n\n\n\nSGD \n\n\n\n# \n\nNo free lunch theorem","source":"_posts/GD-Series.md","raw":"---\ntitle: \ndate: 2017-12-23 19:51:16\ncategories: ML\ntags:\n     - GD\n     - Momentum\ndescription: \n---\n\n# \n\n SGDSGDMNAGAdaGradAdaDeltaAdamNadam Momentum\n\n\n\n\n\n- $w$\n- $f(w)$\n- $\\alpha$\n\n epoch $t$ \n\n> 1. $g_{t} = \\nabla f(w_{t})$\n> 2. $m_{t} = \\phi(g_{1}, g_{2}, \\cdots , g_{t}); \\, V_{t} = \\psi(g_{1}, g_{2}, \\cdots , g_{t})$\n> 3. $\\eta_{t} = \\alpha \\cdot m_{t} \\Big/ \\sqrt{V_{t}}$\n> 4. $w_{t+1} = w_{t} - \\eta_{t}$\n\n## SGD\n\nSGD $m_{t} = g_{t}; \\, V_{t} = I^{2}$\n\n$\\eta_{t} = \\alpha \\cdot g_{t}$\n\nSGD \n\n## SGD with Momentum\n\n SGD SGDM $m_{t} = \\beta_{1} \\cdot m_{t-1} + (1 - \\beta_{1}) \\cdot g_{t}$\n\n $1 / (1 - \\beta_{1})$ \n\n$\\beta_{1}$  0.9 \n\n## SGD with Nesterov Acceleration\n\nSGD \n\nNesterov  1 $g_{t} = \\nabla f(w_{t} - \\alpha \\cdot m_{t-1})$ \n\n SGDM  $m_{t}$ \n\n## AdaGrad\n\n\n\n\n\n$V_{t} = \\sum_{\\tau = 1}^{t} g_{\\tau}^{2}$ \n\n 3 $\\eta_{t} = \\alpha \\cdot m_{t} \\Big/ \\sqrt{V_{t}}$ Ada = Adaptive 0 \n\n 0 \n\n## AdaDelta / RMSProp\n\n AdaGrad  Delta \n\n$V_{t} = \\beta_{2} \\cdot V_{t-1} + (1 - \\beta_{2}) \\cdot g_{t}^{2}$ \n\n## Adam / Nadam\n\n Adam  Nadam \n\nAdam = Adaptive + Momentum\n\nNadam = Nesterov + Adam\n\n $\\beta_{1}$  $\\beta_{2}$ \n\n#  SGD\n\n Adam  Nadam  SGD \n\n\n\nSGD \n\n\n\n# \n\nNo free lunch theorem","slug":"GD-Series","published":1,"updated":"2021-07-19T13:03:31.695Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrancn9c0006g0vwr4t86q7p","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> SGDSGDMNAGAdaGradAdaDeltaAdamNadam Momentum</p>\n<p></p>\n<p></p>\n<ul>\n<li>$w$</li>\n<li>$f(w)$</li>\n<li>$\\alpha$</li>\n</ul>\n<p> epoch $t$ </p>\n<blockquote>\n<ol>\n<li>$g_{t} = \\nabla f(w_{t})$</li>\n<li>$m_{t} = \\phi(g_{1}, g_{2}, \\cdots , g_{t}); \\, V_{t} = \\psi(g_{1}, g_{2}, \\cdots , g_{t})$</li>\n<li>$\\eta_{t} = \\alpha \\cdot m_{t} \\Big/ \\sqrt{V_{t}}$</li>\n<li>$w_{t+1} = w_{t} - \\eta_{t}$</li>\n</ol>\n</blockquote>\n<h2 id=\"SGD\"><a href=\"#SGD\" class=\"headerlink\" title=\"SGD\"></a>SGD</h2><p>SGD $m_{t} = g_{t}; \\, V_{t} = I^{2}$</p>\n<p>$\\eta_{t} = \\alpha \\cdot g_{t}$</p>\n<p>SGD </p>\n<h2 id=\"SGD-with-Momentum\"><a href=\"#SGD-with-Momentum\" class=\"headerlink\" title=\"SGD with Momentum\"></a>SGD with Momentum</h2><p> SGD SGDM $m_{t} = \\beta_{1} \\cdot m_{t-1} + (1 - \\beta_{1}) \\cdot g_{t}$</p>\n<p> $1 / (1 - \\beta_{1})$ </p>\n<p>$\\beta_{1}$  0.9 </p>\n<h2 id=\"SGD-with-Nesterov-Acceleration\"><a href=\"#SGD-with-Nesterov-Acceleration\" class=\"headerlink\" title=\"SGD with Nesterov Acceleration\"></a>SGD with Nesterov Acceleration</h2><p>SGD </p>\n<p>Nesterov  1 $g_{t} = \\nabla f(w_{t} - \\alpha \\cdot m_{t-1})$ </p>\n<p> SGDM  $m_{t}$ </p>\n<h2 id=\"AdaGrad\"><a href=\"#AdaGrad\" class=\"headerlink\" title=\"AdaGrad\"></a>AdaGrad</h2><p></p>\n<p></p>\n<p>$V_{t} = \\sum_{\\tau = 1}^{t} g_{\\tau}^{2}$ </p>\n<p> 3 $\\eta_{t} = \\alpha \\cdot m_{t} \\Big/ \\sqrt{V_{t}}$ Ada = Adaptive 0 </p>\n<p> 0 </p>\n<h2 id=\"AdaDelta-RMSProp\"><a href=\"#AdaDelta-RMSProp\" class=\"headerlink\" title=\"AdaDelta / RMSProp\"></a>AdaDelta / RMSProp</h2><p> AdaGrad  Delta </p>\n<p>$V_{t} = \\beta_{2} \\cdot V_{t-1} + (1 - \\beta_{2}) \\cdot g_{t}^{2}$ </p>\n<h2 id=\"Adam-Nadam\"><a href=\"#Adam-Nadam\" class=\"headerlink\" title=\"Adam / Nadam\"></a>Adam / Nadam</h2><p> Adam  Nadam </p>\n<p>Adam = Adaptive + Momentum</p>\n<p>Nadam = Nesterov + Adam</p>\n<p> $\\beta_{1}$  $\\beta_{2}$ </p>\n<h1 id=\"-SGD\"><a href=\"#-SGD\" class=\"headerlink\" title=\" SGD\"></a> SGD</h1><p> Adam  Nadam  SGD </p>\n<p></p>\n<p>SGD </p>\n<p></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>No free lunch theorem</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> SGDSGDMNAGAdaGradAdaDeltaAdamNadam Momentum</p>\n<p></p>\n<p></p>\n<ul>\n<li>$w$</li>\n<li>$f(w)$</li>\n<li>$\\alpha$</li>\n</ul>\n<p> epoch $t$ </p>\n<blockquote>\n<ol>\n<li>$g_{t} = \\nabla f(w_{t})$</li>\n<li>$m_{t} = \\phi(g_{1}, g_{2}, \\cdots , g_{t}); \\, V_{t} = \\psi(g_{1}, g_{2}, \\cdots , g_{t})$</li>\n<li>$\\eta_{t} = \\alpha \\cdot m_{t} \\Big/ \\sqrt{V_{t}}$</li>\n<li>$w_{t+1} = w_{t} - \\eta_{t}$</li>\n</ol>\n</blockquote>\n<h2 id=\"SGD\"><a href=\"#SGD\" class=\"headerlink\" title=\"SGD\"></a>SGD</h2><p>SGD $m_{t} = g_{t}; \\, V_{t} = I^{2}$</p>\n<p>$\\eta_{t} = \\alpha \\cdot g_{t}$</p>\n<p>SGD </p>\n<h2 id=\"SGD-with-Momentum\"><a href=\"#SGD-with-Momentum\" class=\"headerlink\" title=\"SGD with Momentum\"></a>SGD with Momentum</h2><p> SGD SGDM $m_{t} = \\beta_{1} \\cdot m_{t-1} + (1 - \\beta_{1}) \\cdot g_{t}$</p>\n<p> $1 / (1 - \\beta_{1})$ </p>\n<p>$\\beta_{1}$  0.9 </p>\n<h2 id=\"SGD-with-Nesterov-Acceleration\"><a href=\"#SGD-with-Nesterov-Acceleration\" class=\"headerlink\" title=\"SGD with Nesterov Acceleration\"></a>SGD with Nesterov Acceleration</h2><p>SGD </p>\n<p>Nesterov  1 $g_{t} = \\nabla f(w_{t} - \\alpha \\cdot m_{t-1})$ </p>\n<p> SGDM  $m_{t}$ </p>\n<h2 id=\"AdaGrad\"><a href=\"#AdaGrad\" class=\"headerlink\" title=\"AdaGrad\"></a>AdaGrad</h2><p></p>\n<p></p>\n<p>$V_{t} = \\sum_{\\tau = 1}^{t} g_{\\tau}^{2}$ </p>\n<p> 3 $\\eta_{t} = \\alpha \\cdot m_{t} \\Big/ \\sqrt{V_{t}}$ Ada = Adaptive 0 </p>\n<p> 0 </p>\n<h2 id=\"AdaDelta-RMSProp\"><a href=\"#AdaDelta-RMSProp\" class=\"headerlink\" title=\"AdaDelta / RMSProp\"></a>AdaDelta / RMSProp</h2><p> AdaGrad  Delta </p>\n<p>$V_{t} = \\beta_{2} \\cdot V_{t-1} + (1 - \\beta_{2}) \\cdot g_{t}^{2}$ </p>\n<h2 id=\"Adam-Nadam\"><a href=\"#Adam-Nadam\" class=\"headerlink\" title=\"Adam / Nadam\"></a>Adam / Nadam</h2><p> Adam  Nadam </p>\n<p>Adam = Adaptive + Momentum</p>\n<p>Nadam = Nesterov + Adam</p>\n<p> $\\beta_{1}$  $\\beta_{2}$ </p>\n<h1 id=\"-SGD\"><a href=\"#-SGD\" class=\"headerlink\" title=\" SGD\"></a> SGD</h1><p> Adam  Nadam  SGD </p>\n<p></p>\n<p>SGD </p>\n<p></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>No free lunch theorem</p>\n"},{"title":"","date":"2017-08-13T08:38:35.000Z","description":"","_content":"\n~~~~\n\n\nkernel function\n\nkernel\n****\n\n\n\n\n1. kernel  SVM  kernel  SVM  SVM  kernel \n\n2. **kernel  SVM ** SVM reproducing kernel Hilbert spaceRKHSsignal detectiontime seriesrandom walkpatternRKHS likelihood ratio kernel correlation\n\n    logistic regressionleast squaredimension reduction SVM \n\n3. ** kernel **Gaussian kernelradial basis functionRBF  RKHS \n\n4.  $\\Phi : X \\to \\mathcal{H}, \\Phi(x) = K(x, \\cdot)$  $\\mathcal{H}$  RKHS  $x$  RKHS  $<\\Phi(x), \\Phi(y)>_{\\mathcal{H}} = K(x, y)$ ~~~~\n\n5. \n   \n\n   ","source":"_posts/about-kernel-01.md","raw":"---\ntitle: \ndate: 2017-08-13 16:38:35\ncategories: ML\ntags:\n     - Kernel\n     - SVM\n     - RKHS\ndescription: \n---\n\n~~~~\n\n\nkernel function\n\nkernel\n****\n\n\n\n\n1. kernel  SVM  kernel  SVM  SVM  kernel \n\n2. **kernel  SVM ** SVM reproducing kernel Hilbert spaceRKHSsignal detectiontime seriesrandom walkpatternRKHS likelihood ratio kernel correlation\n\n    logistic regressionleast squaredimension reduction SVM \n\n3. ** kernel **Gaussian kernelradial basis functionRBF  RKHS \n\n4.  $\\Phi : X \\to \\mathcal{H}, \\Phi(x) = K(x, \\cdot)$  $\\mathcal{H}$  RKHS  $x$  RKHS  $<\\Phi(x), \\Phi(y)>_{\\mathcal{H}} = K(x, y)$ ~~~~\n\n5. \n   \n\n   ","slug":"about-kernel-01","published":1,"updated":"2021-07-19T13:03:31.696Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrancn9c0007g0vwlhdivyyh","content":"<p><del></del></p>\n<p><br>kernel function</p>\n<p>kernel<br><strong></strong><br></p>\n<p></p>\n<ol>\n<li><p>kernel  SVM  kernel  SVM  SVM  kernel </p>\n</li>\n<li><p><strong>kernel  SVM </strong> SVM reproducing kernel Hilbert spaceRKHSsignal detectiontime seriesrandom walkpatternRKHS likelihood ratio kernel correlation</p>\n<p> logistic regressionleast squaredimension reduction SVM </p>\n</li>\n<li><p><strong> kernel </strong>Gaussian kernelradial basis functionRBF  RKHS </p>\n</li>\n<li><p> $\\Phi : X \\to \\mathcal{H}, \\Phi(x) = K(x, \\cdot)$  $\\mathcal{H}$  RKHS  $x$  RKHS  $&lt;\\Phi(x), \\Phi(y)&gt;_{\\mathcal{H}} = K(x, y)$ <del></del></p>\n</li>\n<li><p><br></p>\n<p></p>\n</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<p><del></del></p>\n<p><br>kernel function</p>\n<p>kernel<br><strong></strong><br></p>\n<p></p>\n<ol>\n<li><p>kernel  SVM  kernel  SVM  SVM  kernel </p>\n</li>\n<li><p><strong>kernel  SVM </strong> SVM reproducing kernel Hilbert spaceRKHSsignal detectiontime seriesrandom walkpatternRKHS likelihood ratio kernel correlation</p>\n<p> logistic regressionleast squaredimension reduction SVM </p>\n</li>\n<li><p><strong> kernel </strong>Gaussian kernelradial basis functionRBF  RKHS </p>\n</li>\n<li><p> $\\Phi : X \\to \\mathcal{H}, \\Phi(x) = K(x, \\cdot)$  $\\mathcal{H}$  RKHS  $x$  RKHS  $&lt;\\Phi(x), \\Phi(y)&gt;_{\\mathcal{H}} = K(x, y)$ <del></del></p>\n</li>\n<li><p><br></p>\n<p></p>\n</li>\n</ol>\n"},{"title":"","date":"2017-08-13T09:42:38.000Z","description":"","_content":"\n**functional analysis**function space\n\n\n\n![Linear space](about-kernel-02/linear_space.png)\n\n# \n\nlinear spacebasis\n $\\lbrace 1, x, x^{2} \\rbrace$  $f(x)$  $f(x) = \\alpha_{1} \\cdot 1 + \\alpha_{2} \\cdot x + \\alpha_{3} \\cdot x^{2}$  $f(x)$  $\\lbrace \\alpha_{1}, \\alpha_{2}, \\alpha_{3} \\rbrace$  $\\lbrace x, y, z \\rbrace$ \n $\\lbrace 1, x+1, (x+1)^{2} \\rbrace$  $n$  $n$ \northogonal basis 0\n\n# \n\nmetric linear space**metric**\n\n\n1.  $d(x, y) \\ge 0; d(x, y) = 0 \\Leftrightarrow x = y$ \n2.  $d(x, y) = d(y, x)$ \n3. $d(x, z) + d(z, y) \\ge d(x, y)$ \n\n# \n\nnormed linear space**norm**\n\n\n1. $\\lVert x \\rVert \\ge 0$ \n2. $ \\lVert \\alpha x \\rVert = |\\alpha| \\lVert x \\rVert $ \n3. $\\lVert x \\rVert + \\lVert y \\rVert \\ge \\lVert x+y \\rVert$ \n\n $d(x, y) = \\lVert x - y \\rVert$ \n\n# \n\nBanach space\ncompletenessCauchy sequenceconvergence\n\n# \n\ninner product linear space**inner product**\n**scalar product****dot product**\n\n\n\n1. $\\langle x, y \\rangle = \\langle y, x \\rangle$ \n2.  $\\langle x, y \\rangle + \\langle x, z \\rangle = \\langle x, y + z \\rangle$  $\\langle \\alpha x, y \\rangle = \\alpha \\langle x, y \\rangle$ \n3. $\\langle x, x \\rangle \\ge 0$ \n\n $\\lVert x \\rVert^{2} = \\langle x, x \\rangle$ \n\n# \n\nEuclidean space\n\n# \n\nHilbert space\n\n# \n\n1. Taylor series $\\lbrace x^{i} \\rbrace_{0}^{\\infty}$ \n2. Fourier series $\\lbrace 1, \\cos x, \\sin x, \\cos 2x, \\sin 2x, \\cdots \\rbrace$ \n\n# \n\n\n\n $n \\times n$ \n$$\nAx=\\lambda x\n$$\n $n$ \n\n\n\n $f(x)$  $K(x, y)$ \n\n1. $\\forall f \\rightarrow \\iint f(x)K(x, y)f(y) \\,\\mathrm{d}x\\,\\mathrm{d}y \\ge 0$ \n2. $K(x, y) = K(y, x)$ \n\n**kernel function**\n\n $\\lambda$  $\\psi(x)$ \n$$\n\\int K(x, y)\\psi(x) \\mathrm{d}x = \\lambda \\psi (y)\n$$\n $\\lambda_{1}$  $\\lambda_{2}$  $\\psi_{1}(x)$  $\\psi_{2}(x)$ \n$$\n\\begin{aligned}\n\\int \\lambda_1 \\psi_{1}(x) \\psi_{2}(x) \\mathrm{d}x &= \\iint K(y, x) \\psi_{1}(y) \\mathrm{d}y\\,\\psi_{2}(x)\\mathrm{d}x \\\\\n&= \\iint K(y, x) \\psi_{2}(x) \\mathrm{d}x\\,\\psi_{1}(y)\\mathrm{d}y \\\\\n&= \\int \\lambda_2 \\psi_{2}(y) \\psi_{1}(y) \\mathrm{d}y \\\\\n&= \\int \\lambda_2 \\psi_{2}(x) \\psi_{1}(x) \\mathrm{d}x\n\\end{aligned}\n$$\n\n$$\n\\langle \\psi_{1}, \\psi_{2} \\rangle = \\int \\psi_{1}(x) \\psi_{2}(x) \\mathrm{d}x = 0\n$$\n $K$ $\\lbrace \\lambda_{i} \\rbrace_{i=1}^{\\infty}$  $\\lbrace \\psi_{i} \\rbrace_{i=1}^{\\infty}$ \n\n# \n\n $\\lbrace \\sqrt{\\lambda_{i}}\\psi_{i} \\rbrace_{i=1}^{\\infty}$  $\\mathcal{H}$ \n$$\nf = \\sum_{i=1}^{\\infty} f_{i}\\sqrt{\\lambda_{i}}\\psi_{i}\n$$\n $f$  $\\mathcal{H}$  $f = (f_{1}, f_{2}, \\cdots)_{\\mathcal{H}}^{\\mathsf{T}}$ \n $g = (g_{1}, g_{2}, \\cdots)_{\\mathcal{H}}^{\\mathsf{T}}$ \n$$\n\\langle f, g \\rangle_{\\mathcal{H}} = \\sum_{i=1}^{\\infty}f_{i}g_{i}\n$$\n\n$$\nK(x, \\cdot) = \\sum_{i=0}^{\\infty} \\lambda_{i}\\psi_{i}(x)\\psi_{i}(\\cdot)\n$$\n $\\psi_{i}$ $K(x, \\cdot) = \\sum_{i=0}^{\\infty} \\lambda_{i}\\psi_{i}(x)\\psi_{i}$ \n $K$ \n$$\nK(x, \\cdot) = (\\sqrt{\\lambda_{1}}\\psi_{1}(x), \\sqrt{\\lambda_{2}}\\psi_{2}(x),\\cdots)_{\\mathcal{H}}^{\\mathsf{T}}\n$$\n\n$$\n\\langle K(x, \\cdot), K(y, \\cdot) \\rangle_{\\mathcal{H}} = \\sum_{i=0}^{\\infty} \\lambda_{i} \\psi_{i}(x) \\psi_{i}(y) = K(x, y)\n$$\nreproducing $\\mathcal{H}$ reproducing kernel Hilbert spaceRKHS\n\n\n\n\n$$\n\\Phi(x) = K(x, \\cdot) = (\\sqrt{\\lambda_{1}}\\psi_{1}(x), \\sqrt{\\lambda_{2}}\\psi_{2}(x),\\cdots)_{\\mathcal{H}}^{\\mathsf{T}}\n$$\n\n$$\n\\langle \\Phi(x), \\Phi(y) \\rangle_{\\mathcal{H}} = \\langle K(x, \\cdot), K(y, \\cdot) \\rangle_{\\mathcal{H}} = K(x, y)\n$$\n $K$  $\\Phi$  $\\mathcal{H}$ \n$$\n\\langle \\Phi(x), \\Phi(y) \\rangle_{\\mathcal{H}} = K(x, y)\n$$\nkernel trick\n\n** RKHS **\n\n#  SVM \n\n\n\n $x$  $\\Phi(x)$ \n\n $\\langle x_{i}, x_{j} \\rangle$  $x$  $\\Phi(x)$  $\\langle \\Phi(x_{i}), \\Phi(x_{j}) \\rangle$ \n $K$  $K$  $\\Phi$  $\\langle \\Phi(x_{i}), \\Phi(x_{j}) \\rangle$  $K(x_{i}, x_{j})$  $K$  $x$ \n\n $\\lVert x - y \\rVert^{2}$ ","source":"_posts/about-kernel-02.md","raw":"---\ntitle: \ndate: 2017-08-13 17:42:38\ncategories: ML\ntags:\n     - Kernel\n     - RKHS\n     - Functional analysis\ndescription: \n---\n\n**functional analysis**function space\n\n\n\n![Linear space](about-kernel-02/linear_space.png)\n\n# \n\nlinear spacebasis\n $\\lbrace 1, x, x^{2} \\rbrace$  $f(x)$  $f(x) = \\alpha_{1} \\cdot 1 + \\alpha_{2} \\cdot x + \\alpha_{3} \\cdot x^{2}$  $f(x)$  $\\lbrace \\alpha_{1}, \\alpha_{2}, \\alpha_{3} \\rbrace$  $\\lbrace x, y, z \\rbrace$ \n $\\lbrace 1, x+1, (x+1)^{2} \\rbrace$  $n$  $n$ \northogonal basis 0\n\n# \n\nmetric linear space**metric**\n\n\n1.  $d(x, y) \\ge 0; d(x, y) = 0 \\Leftrightarrow x = y$ \n2.  $d(x, y) = d(y, x)$ \n3. $d(x, z) + d(z, y) \\ge d(x, y)$ \n\n# \n\nnormed linear space**norm**\n\n\n1. $\\lVert x \\rVert \\ge 0$ \n2. $ \\lVert \\alpha x \\rVert = |\\alpha| \\lVert x \\rVert $ \n3. $\\lVert x \\rVert + \\lVert y \\rVert \\ge \\lVert x+y \\rVert$ \n\n $d(x, y) = \\lVert x - y \\rVert$ \n\n# \n\nBanach space\ncompletenessCauchy sequenceconvergence\n\n# \n\ninner product linear space**inner product**\n**scalar product****dot product**\n\n\n\n1. $\\langle x, y \\rangle = \\langle y, x \\rangle$ \n2.  $\\langle x, y \\rangle + \\langle x, z \\rangle = \\langle x, y + z \\rangle$  $\\langle \\alpha x, y \\rangle = \\alpha \\langle x, y \\rangle$ \n3. $\\langle x, x \\rangle \\ge 0$ \n\n $\\lVert x \\rVert^{2} = \\langle x, x \\rangle$ \n\n# \n\nEuclidean space\n\n# \n\nHilbert space\n\n# \n\n1. Taylor series $\\lbrace x^{i} \\rbrace_{0}^{\\infty}$ \n2. Fourier series $\\lbrace 1, \\cos x, \\sin x, \\cos 2x, \\sin 2x, \\cdots \\rbrace$ \n\n# \n\n\n\n $n \\times n$ \n$$\nAx=\\lambda x\n$$\n $n$ \n\n\n\n $f(x)$  $K(x, y)$ \n\n1. $\\forall f \\rightarrow \\iint f(x)K(x, y)f(y) \\,\\mathrm{d}x\\,\\mathrm{d}y \\ge 0$ \n2. $K(x, y) = K(y, x)$ \n\n**kernel function**\n\n $\\lambda$  $\\psi(x)$ \n$$\n\\int K(x, y)\\psi(x) \\mathrm{d}x = \\lambda \\psi (y)\n$$\n $\\lambda_{1}$  $\\lambda_{2}$  $\\psi_{1}(x)$  $\\psi_{2}(x)$ \n$$\n\\begin{aligned}\n\\int \\lambda_1 \\psi_{1}(x) \\psi_{2}(x) \\mathrm{d}x &= \\iint K(y, x) \\psi_{1}(y) \\mathrm{d}y\\,\\psi_{2}(x)\\mathrm{d}x \\\\\n&= \\iint K(y, x) \\psi_{2}(x) \\mathrm{d}x\\,\\psi_{1}(y)\\mathrm{d}y \\\\\n&= \\int \\lambda_2 \\psi_{2}(y) \\psi_{1}(y) \\mathrm{d}y \\\\\n&= \\int \\lambda_2 \\psi_{2}(x) \\psi_{1}(x) \\mathrm{d}x\n\\end{aligned}\n$$\n\n$$\n\\langle \\psi_{1}, \\psi_{2} \\rangle = \\int \\psi_{1}(x) \\psi_{2}(x) \\mathrm{d}x = 0\n$$\n $K$ $\\lbrace \\lambda_{i} \\rbrace_{i=1}^{\\infty}$  $\\lbrace \\psi_{i} \\rbrace_{i=1}^{\\infty}$ \n\n# \n\n $\\lbrace \\sqrt{\\lambda_{i}}\\psi_{i} \\rbrace_{i=1}^{\\infty}$  $\\mathcal{H}$ \n$$\nf = \\sum_{i=1}^{\\infty} f_{i}\\sqrt{\\lambda_{i}}\\psi_{i}\n$$\n $f$  $\\mathcal{H}$  $f = (f_{1}, f_{2}, \\cdots)_{\\mathcal{H}}^{\\mathsf{T}}$ \n $g = (g_{1}, g_{2}, \\cdots)_{\\mathcal{H}}^{\\mathsf{T}}$ \n$$\n\\langle f, g \\rangle_{\\mathcal{H}} = \\sum_{i=1}^{\\infty}f_{i}g_{i}\n$$\n\n$$\nK(x, \\cdot) = \\sum_{i=0}^{\\infty} \\lambda_{i}\\psi_{i}(x)\\psi_{i}(\\cdot)\n$$\n $\\psi_{i}$ $K(x, \\cdot) = \\sum_{i=0}^{\\infty} \\lambda_{i}\\psi_{i}(x)\\psi_{i}$ \n $K$ \n$$\nK(x, \\cdot) = (\\sqrt{\\lambda_{1}}\\psi_{1}(x), \\sqrt{\\lambda_{2}}\\psi_{2}(x),\\cdots)_{\\mathcal{H}}^{\\mathsf{T}}\n$$\n\n$$\n\\langle K(x, \\cdot), K(y, \\cdot) \\rangle_{\\mathcal{H}} = \\sum_{i=0}^{\\infty} \\lambda_{i} \\psi_{i}(x) \\psi_{i}(y) = K(x, y)\n$$\nreproducing $\\mathcal{H}$ reproducing kernel Hilbert spaceRKHS\n\n\n\n\n$$\n\\Phi(x) = K(x, \\cdot) = (\\sqrt{\\lambda_{1}}\\psi_{1}(x), \\sqrt{\\lambda_{2}}\\psi_{2}(x),\\cdots)_{\\mathcal{H}}^{\\mathsf{T}}\n$$\n\n$$\n\\langle \\Phi(x), \\Phi(y) \\rangle_{\\mathcal{H}} = \\langle K(x, \\cdot), K(y, \\cdot) \\rangle_{\\mathcal{H}} = K(x, y)\n$$\n $K$  $\\Phi$  $\\mathcal{H}$ \n$$\n\\langle \\Phi(x), \\Phi(y) \\rangle_{\\mathcal{H}} = K(x, y)\n$$\nkernel trick\n\n** RKHS **\n\n#  SVM \n\n\n\n $x$  $\\Phi(x)$ \n\n $\\langle x_{i}, x_{j} \\rangle$  $x$  $\\Phi(x)$  $\\langle \\Phi(x_{i}), \\Phi(x_{j}) \\rangle$ \n $K$  $K$  $\\Phi$  $\\langle \\Phi(x_{i}), \\Phi(x_{j}) \\rangle$  $K(x_{i}, x_{j})$  $K$  $x$ \n\n $\\lVert x - y \\rVert^{2}$ ","slug":"about-kernel-02","published":1,"updated":"2021-07-19T13:03:31.696Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrancn9e000bg0vw2nlae14l","content":"<p><strong>functional analysis</strong>function space</p>\n<p></p>\n<p><img src=\"/.me//linear_space.png\" alt=\"Linear space\"></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>linear spacebasis<br> $\\lbrace 1, x, x^{2} \\rbrace$  $f(x)$  $f(x) = \\alpha_{1} \\cdot 1 + \\alpha_{2} \\cdot x + \\alpha_{3} \\cdot x^{2}$  $f(x)$  $\\lbrace \\alpha_{1}, \\alpha_{2}, \\alpha_{3} \\rbrace$  $\\lbrace x, y, z \\rbrace$ <br> $\\lbrace 1, x+1, (x+1)^{2} \\rbrace$  $n$  $n$ <br>orthogonal basis 0</p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>metric linear space<strong>metric</strong><br></p>\n<ol>\n<li> $d(x, y) \\ge 0; d(x, y) = 0 \\Leftrightarrow x = y$ </li>\n<li> $d(x, y) = d(y, x)$ </li>\n<li>$d(x, z) + d(z, y) \\ge d(x, y)$ </li>\n</ol>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>normed linear space<strong>norm</strong><br></p>\n<ol>\n<li>$\\lVert x \\rVert \\ge 0$ </li>\n<li>$ \\lVert \\alpha x \\rVert = |\\alpha| \\lVert x \\rVert $ </li>\n<li>$\\lVert x \\rVert + \\lVert y \\rVert \\ge \\lVert x+y \\rVert$ </li>\n</ol>\n<p> $d(x, y) = \\lVert x - y \\rVert$ </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>Banach space<br>completenessCauchy sequenceconvergence</p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>inner product linear space<strong>inner product</strong><br><strong>scalar product</strong><strong>dot product</strong><br><br></p>\n<ol>\n<li>$\\langle x, y \\rangle = \\langle y, x \\rangle$ </li>\n<li> $\\langle x, y \\rangle + \\langle x, z \\rangle = \\langle x, y + z \\rangle$  $\\langle \\alpha x, y \\rangle = \\alpha \\langle x, y \\rangle$ </li>\n<li>$\\langle x, x \\rangle \\ge 0$ </li>\n</ol>\n<p> $\\lVert x \\rVert^{2} = \\langle x, x \\rangle$ </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>Euclidean space</p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>Hilbert space</p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><ol>\n<li>Taylor series $\\lbrace x^{i} \\rbrace_{0}^{\\infty}$ </li>\n<li>Fourier series $\\lbrace 1, \\cos x, \\sin x, \\cos 2x, \\sin 2x, \\cdots \\rbrace$ </li>\n</ol>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n<p> $n \\times n$ <br>$$<br>Ax=\\lambda x<br>$$<br> $n$ </p>\n<p></p>\n<p> $f(x)$  $K(x, y)$ </p>\n<ol>\n<li>$\\forall f \\rightarrow \\iint f(x)K(x, y)f(y) \\,\\mathrm{d}x\\,\\mathrm{d}y \\ge 0$ </li>\n<li>$K(x, y) = K(y, x)$ </li>\n</ol>\n<p><strong>kernel function</strong></p>\n<p> $\\lambda$  $\\psi(x)$ <br>$$<br>\\int K(x, y)\\psi(x) \\mathrm{d}x = \\lambda \\psi (y)<br>$$<br> $\\lambda_{1}$  $\\lambda_{2}$  $\\psi_{1}(x)$  $\\psi_{2}(x)$ <br>$$<br>\\begin{aligned}<br>\\int \\lambda_1 \\psi_{1}(x) \\psi_{2}(x) \\mathrm{d}x &amp;= \\iint K(y, x) \\psi_{1}(y) \\mathrm{d}y\\,\\psi_{2}(x)\\mathrm{d}x \\<br>&amp;= \\iint K(y, x) \\psi_{2}(x) \\mathrm{d}x\\,\\psi_{1}(y)\\mathrm{d}y \\<br>&amp;= \\int \\lambda_2 \\psi_{2}(y) \\psi_{1}(y) \\mathrm{d}y \\<br>&amp;= \\int \\lambda_2 \\psi_{2}(x) \\psi_{1}(x) \\mathrm{d}x<br>\\end{aligned}<br>$$<br><br>$$<br>\\langle \\psi_{1}, \\psi_{2} \\rangle = \\int \\psi_{1}(x) \\psi_{2}(x) \\mathrm{d}x = 0<br>$$<br> $K$ $\\lbrace \\lambda_{i} \\rbrace_{i=1}^{\\infty}$  $\\lbrace \\psi_{i} \\rbrace_{i=1}^{\\infty}$ </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> $\\lbrace \\sqrt{\\lambda_{i}}\\psi_{i} \\rbrace_{i=1}^{\\infty}$  $\\mathcal{H}$ <br>$$<br>f = \\sum_{i=1}^{\\infty} f_{i}\\sqrt{\\lambda_{i}}\\psi_{i}<br>$$<br> $f$  $\\mathcal{H}$  $f = (f_{1}, f_{2}, \\cdots)<em>{\\mathcal{H}}^{\\mathsf{T}}$ <br> $g = (g</em>{1}, g_{2}, \\cdots)<em>{\\mathcal{H}}^{\\mathsf{T}}$ <br>$$<br>\\langle f, g \\rangle</em>{\\mathcal{H}} = \\sum_{i=1}^{\\infty}f_{i}g_{i}<br>$$<br><br>$$<br>K(x, \\cdot) = \\sum_{i=0}^{\\infty} \\lambda_{i}\\psi_{i}(x)\\psi_{i}(\\cdot)<br>$$<br> $\\psi_{i}$ $K(x, \\cdot) = \\sum_{i=0}^{\\infty} \\lambda_{i}\\psi_{i}(x)\\psi_{i}$ <br> $K$ <br>$$<br>K(x, \\cdot) = (\\sqrt{\\lambda_{1}}\\psi_{1}(x), \\sqrt{\\lambda_{2}}\\psi_{2}(x),\\cdots)<em>{\\mathcal{H}}^{\\mathsf{T}}<br>$$<br><br>$$<br>\\langle K(x, \\cdot), K(y, \\cdot) \\rangle</em>{\\mathcal{H}} = \\sum_{i=0}^{\\infty} \\lambda_{i} \\psi_{i}(x) \\psi_{i}(y) = K(x, y)<br>$$<br>reproducing $\\mathcal{H}$ reproducing kernel Hilbert spaceRKHS</p>\n<p></p>\n<p><br>$$<br>\\Phi(x) = K(x, \\cdot) = (\\sqrt{\\lambda_{1}}\\psi_{1}(x), \\sqrt{\\lambda_{2}}\\psi_{2}(x),\\cdots)<em>{\\mathcal{H}}^{\\mathsf{T}}<br>$$<br><br>$$<br>\\langle \\Phi(x), \\Phi(y) \\rangle</em>{\\mathcal{H}} = \\langle K(x, \\cdot), K(y, \\cdot) \\rangle_{\\mathcal{H}} = K(x, y)<br>$$<br> $K$  $\\Phi$  $\\mathcal{H}$ <br>$$<br>\\langle \\Phi(x), \\Phi(y) \\rangle_{\\mathcal{H}} = K(x, y)<br>$$<br>kernel trick</p>\n<p><strong> RKHS </strong></p>\n<h1 id=\"-SVM-\"><a href=\"#-SVM-\" class=\"headerlink\" title=\" SVM \"></a> SVM </h1><p></p>\n<p> $x$  $\\Phi(x)$ </p>\n<p> $\\langle x_{i}, x_{j} \\rangle$  $x$  $\\Phi(x)$  $\\langle \\Phi(x_{i}), \\Phi(x_{j}) \\rangle$ <br> $K$  $K$  $\\Phi$  $\\langle \\Phi(x_{i}), \\Phi(x_{j}) \\rangle$  $K(x_{i}, x_{j})$  $K$  $x$ </p>\n<p> $\\lVert x - y \\rVert^{2}$ </p>\n","site":{"data":{}},"excerpt":"","more":"<p><strong>functional analysis</strong>function space</p>\n<p></p>\n<p><img src=\"/.me//linear_space.png\" alt=\"Linear space\"></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>linear spacebasis<br> $\\lbrace 1, x, x^{2} \\rbrace$  $f(x)$  $f(x) = \\alpha_{1} \\cdot 1 + \\alpha_{2} \\cdot x + \\alpha_{3} \\cdot x^{2}$  $f(x)$  $\\lbrace \\alpha_{1}, \\alpha_{2}, \\alpha_{3} \\rbrace$  $\\lbrace x, y, z \\rbrace$ <br> $\\lbrace 1, x+1, (x+1)^{2} \\rbrace$  $n$  $n$ <br>orthogonal basis 0</p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>metric linear space<strong>metric</strong><br></p>\n<ol>\n<li> $d(x, y) \\ge 0; d(x, y) = 0 \\Leftrightarrow x = y$ </li>\n<li> $d(x, y) = d(y, x)$ </li>\n<li>$d(x, z) + d(z, y) \\ge d(x, y)$ </li>\n</ol>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>normed linear space<strong>norm</strong><br></p>\n<ol>\n<li>$\\lVert x \\rVert \\ge 0$ </li>\n<li>$ \\lVert \\alpha x \\rVert = |\\alpha| \\lVert x \\rVert $ </li>\n<li>$\\lVert x \\rVert + \\lVert y \\rVert \\ge \\lVert x+y \\rVert$ </li>\n</ol>\n<p> $d(x, y) = \\lVert x - y \\rVert$ </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>Banach space<br>completenessCauchy sequenceconvergence</p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>inner product linear space<strong>inner product</strong><br><strong>scalar product</strong><strong>dot product</strong><br><br></p>\n<ol>\n<li>$\\langle x, y \\rangle = \\langle y, x \\rangle$ </li>\n<li> $\\langle x, y \\rangle + \\langle x, z \\rangle = \\langle x, y + z \\rangle$  $\\langle \\alpha x, y \\rangle = \\alpha \\langle x, y \\rangle$ </li>\n<li>$\\langle x, x \\rangle \\ge 0$ </li>\n</ol>\n<p> $\\lVert x \\rVert^{2} = \\langle x, x \\rangle$ </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>Euclidean space</p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>Hilbert space</p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><ol>\n<li>Taylor series $\\lbrace x^{i} \\rbrace_{0}^{\\infty}$ </li>\n<li>Fourier series $\\lbrace 1, \\cos x, \\sin x, \\cos 2x, \\sin 2x, \\cdots \\rbrace$ </li>\n</ol>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n<p> $n \\times n$ <br>$$<br>Ax=\\lambda x<br>$$<br> $n$ </p>\n<p></p>\n<p> $f(x)$  $K(x, y)$ </p>\n<ol>\n<li>$\\forall f \\rightarrow \\iint f(x)K(x, y)f(y) \\,\\mathrm{d}x\\,\\mathrm{d}y \\ge 0$ </li>\n<li>$K(x, y) = K(y, x)$ </li>\n</ol>\n<p><strong>kernel function</strong></p>\n<p> $\\lambda$  $\\psi(x)$ <br>$$<br>\\int K(x, y)\\psi(x) \\mathrm{d}x = \\lambda \\psi (y)<br>$$<br> $\\lambda_{1}$  $\\lambda_{2}$  $\\psi_{1}(x)$  $\\psi_{2}(x)$ <br>$$<br>\\begin{aligned}<br>\\int \\lambda_1 \\psi_{1}(x) \\psi_{2}(x) \\mathrm{d}x &amp;= \\iint K(y, x) \\psi_{1}(y) \\mathrm{d}y\\,\\psi_{2}(x)\\mathrm{d}x \\<br>&amp;= \\iint K(y, x) \\psi_{2}(x) \\mathrm{d}x\\,\\psi_{1}(y)\\mathrm{d}y \\<br>&amp;= \\int \\lambda_2 \\psi_{2}(y) \\psi_{1}(y) \\mathrm{d}y \\<br>&amp;= \\int \\lambda_2 \\psi_{2}(x) \\psi_{1}(x) \\mathrm{d}x<br>\\end{aligned}<br>$$<br><br>$$<br>\\langle \\psi_{1}, \\psi_{2} \\rangle = \\int \\psi_{1}(x) \\psi_{2}(x) \\mathrm{d}x = 0<br>$$<br> $K$ $\\lbrace \\lambda_{i} \\rbrace_{i=1}^{\\infty}$  $\\lbrace \\psi_{i} \\rbrace_{i=1}^{\\infty}$ </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> $\\lbrace \\sqrt{\\lambda_{i}}\\psi_{i} \\rbrace_{i=1}^{\\infty}$  $\\mathcal{H}$ <br>$$<br>f = \\sum_{i=1}^{\\infty} f_{i}\\sqrt{\\lambda_{i}}\\psi_{i}<br>$$<br> $f$  $\\mathcal{H}$  $f = (f_{1}, f_{2}, \\cdots)<em>{\\mathcal{H}}^{\\mathsf{T}}$ <br> $g = (g</em>{1}, g_{2}, \\cdots)<em>{\\mathcal{H}}^{\\mathsf{T}}$ <br>$$<br>\\langle f, g \\rangle</em>{\\mathcal{H}} = \\sum_{i=1}^{\\infty}f_{i}g_{i}<br>$$<br><br>$$<br>K(x, \\cdot) = \\sum_{i=0}^{\\infty} \\lambda_{i}\\psi_{i}(x)\\psi_{i}(\\cdot)<br>$$<br> $\\psi_{i}$ $K(x, \\cdot) = \\sum_{i=0}^{\\infty} \\lambda_{i}\\psi_{i}(x)\\psi_{i}$ <br> $K$ <br>$$<br>K(x, \\cdot) = (\\sqrt{\\lambda_{1}}\\psi_{1}(x), \\sqrt{\\lambda_{2}}\\psi_{2}(x),\\cdots)<em>{\\mathcal{H}}^{\\mathsf{T}}<br>$$<br><br>$$<br>\\langle K(x, \\cdot), K(y, \\cdot) \\rangle</em>{\\mathcal{H}} = \\sum_{i=0}^{\\infty} \\lambda_{i} \\psi_{i}(x) \\psi_{i}(y) = K(x, y)<br>$$<br>reproducing $\\mathcal{H}$ reproducing kernel Hilbert spaceRKHS</p>\n<p></p>\n<p><br>$$<br>\\Phi(x) = K(x, \\cdot) = (\\sqrt{\\lambda_{1}}\\psi_{1}(x), \\sqrt{\\lambda_{2}}\\psi_{2}(x),\\cdots)<em>{\\mathcal{H}}^{\\mathsf{T}}<br>$$<br><br>$$<br>\\langle \\Phi(x), \\Phi(y) \\rangle</em>{\\mathcal{H}} = \\langle K(x, \\cdot), K(y, \\cdot) \\rangle_{\\mathcal{H}} = K(x, y)<br>$$<br> $K$  $\\Phi$  $\\mathcal{H}$ <br>$$<br>\\langle \\Phi(x), \\Phi(y) \\rangle_{\\mathcal{H}} = K(x, y)<br>$$<br>kernel trick</p>\n<p><strong> RKHS </strong></p>\n<h1 id=\"-SVM-\"><a href=\"#-SVM-\" class=\"headerlink\" title=\" SVM \"></a> SVM </h1><p></p>\n<p> $x$  $\\Phi(x)$ </p>\n<p> $\\langle x_{i}, x_{j} \\rangle$  $x$  $\\Phi(x)$  $\\langle \\Phi(x_{i}), \\Phi(x_{j}) \\rangle$ <br> $K$  $K$  $\\Phi$  $\\langle \\Phi(x_{i}), \\Phi(x_{j}) \\rangle$  $K(x_{i}, x_{j})$  $K$  $x$ </p>\n<p> $\\lVert x - y \\rVert^{2}$ </p>\n"},{"title":"","date":"2020-01-12T02:35:19.000Z","description":"32","_content":"\n# \n\n<br /><br />\n\n3210\n\n#  \n\n****\n\n<br /><br />\n\n<br />\n\n<br /><br />\n\n\n\n- \n- \n\n\n\n****\n\n<br />\n\n****\n\n\n\n#  \n\n****\n\n<br /><br />\n\n****\n\n\n\n- \n- \n- \n\n<br />\n\n<br />\n\n#  \n\n****\n\n<br />\n\n<br />\n\n<br /><br />\n\n<br /><br /><br />\n\n<br /><br />\n\n# \n\n<br /><br />\n\n<br /><br />\n\n****\n\n","source":"_posts/alterable-vs-permanent.md","raw":"---\ntitle: \ndate: 2020-01-12 10:35:19\ncategories: Life\ntags: Tips\ndescription: 32\n---\n\n# \n\n<br /><br />\n\n3210\n\n#  \n\n****\n\n<br /><br />\n\n<br />\n\n<br /><br />\n\n\n\n- \n- \n\n\n\n****\n\n<br />\n\n****\n\n\n\n#  \n\n****\n\n<br /><br />\n\n****\n\n\n\n- \n- \n- \n\n<br />\n\n<br />\n\n#  \n\n****\n\n<br />\n\n<br />\n\n<br /><br />\n\n<br /><br /><br />\n\n<br /><br />\n\n# \n\n<br /><br />\n\n<br /><br />\n\n****\n\n","slug":"alterable-vs-permanent","published":1,"updated":"2021-07-19T13:03:31.696Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrancn9f000cg0vwz6oud3ca","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p><br><br></p>\n<p>3210</p>\n<h1 id=\"-\"><a href=\"#-\" class=\"headerlink\" title=\" \"></a> </h1><p><strong></strong></p>\n<p><br><br></p>\n<p><br></p>\n<p><br><br></p>\n<p></p>\n<ul>\n<li></li>\n<li></li>\n</ul>\n<p></p>\n<p><strong></strong></p>\n<p><br></p>\n<p><strong></strong></p>\n<p></p>\n<h1 id=\"-\"><a href=\"#-\" class=\"headerlink\" title=\" \"></a> </h1><p><strong></strong></p>\n<p><br><br></p>\n<p><strong></strong></p>\n<p></p>\n<ul>\n<li></li>\n<li></li>\n<li></li>\n</ul>\n<p><br></p>\n<p><br></p>\n<h1 id=\"-\"><a href=\"#-\" class=\"headerlink\" title=\" \"></a> </h1><p><strong></strong></p>\n<p><br></p>\n<p><br></p>\n<p><br><br></p>\n<p><br><br><br></p>\n<p><br><br></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p><br><br></p>\n<p><br><br></p>\n<p><strong></strong></p>\n<p></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p><br><br></p>\n<p>3210</p>\n<h1 id=\"-\"><a href=\"#-\" class=\"headerlink\" title=\" \"></a> </h1><p><strong></strong></p>\n<p><br><br></p>\n<p><br></p>\n<p><br><br></p>\n<p></p>\n<ul>\n<li></li>\n<li></li>\n</ul>\n<p></p>\n<p><strong></strong></p>\n<p><br></p>\n<p><strong></strong></p>\n<p></p>\n<h1 id=\"-\"><a href=\"#-\" class=\"headerlink\" title=\" \"></a> </h1><p><strong></strong></p>\n<p><br><br></p>\n<p><strong></strong></p>\n<p></p>\n<ul>\n<li></li>\n<li></li>\n<li></li>\n</ul>\n<p><br></p>\n<p><br></p>\n<h1 id=\"-\"><a href=\"#-\" class=\"headerlink\" title=\" \"></a> </h1><p><strong></strong></p>\n<p><br></p>\n<p><br></p>\n<p><br><br></p>\n<p><br><br><br></p>\n<p><br><br></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p><br><br></p>\n<p><br><br></p>\n<p><strong></strong></p>\n<p></p>\n"},{"title":"","date":"2017-12-03T04:39:14.000Z","description":"","_content":"\n# \n\n\n\n BP  $\\delta$  BP \n\n\n\n- $net$  $net_{i}$  $net = w^{\\mathsf{T}}x$ \n- $a$  $a_{i}$  $a=f(net)$ \n\n# \n\n identical $a=net=w^{\\mathsf{T}}x$ \n\n$$\nJ(w) = \\frac{1}{2} (a-y)^{2}=\\frac{1}{2} (net-y)^{2}\n$$\n\n$$\n\\delta = \\frac{\\partial J}{\\partial net} = net-y = a-y\n$$\n\n# \n\n\n\n## \n\n sigmoid  $a=\\sigma(net)=\\sigma(w^{\\mathsf{T}}x)=\\frac{1}{1+\\exp(-w^{\\mathsf{T}}x)}$ \n\n$$\nJ(w) = -L(w) = -y \\ln a - (1-y) \\ln (1-a)\n$$\n\n$$\nL(w) = y \\ln a + (1-y) \\ln (1-a)\n$$\n log-likelihood\n\n\n$$\n\\delta = \\frac{\\partial J}{\\partial net} = \\frac{\\partial J}{\\partial a} \\frac{\\partial a}{\\partial net} = \\frac{a-y}{(1-a)a}(1-a)a = a-y\n$$\n\n## \n\n softmax $a=\\mathrm{softmax}(net)=\\frac{\\exp(net_{i})}{\\sum_{j=1}^{N}\\exp(net_{j})}$ \n\n\n$$\nJ(w) = -\\sum_{j=1}^{N} y_j \\ln a_j = \\sum_{j=1}^{N} y_j (\\ln \\sum_{k=1}^{N} e^{net_{k}}-net_{j})\n$$\n $i$ \n$$\n\\delta_{i} = \\frac{\\partial J}{\\partial net_{i}} = \\sum_{j=1}^{N} y_j (\\frac{\\sum_{k=1}^{N} e^{net_{k}}\\frac{\\partial net_{k}}{\\partial net_{i}}}{\\sum_{k=1}^{N} e^{net_{k}}} - \\frac{\\partial net_{j}}{\\partial net_{i}}) = \\sum_{j=1}^{N} y_j  \\frac{e^{net_{i}}}{\\sum_{k=1}^{N} e^{net_{k}}} - \\sum_{j=1}^{N} y_j  \\frac{\\partial net_{j}}{\\partial net_{i}} = a_{i}-y_{i}\n$$\n","source":"_posts/common-ground-of-mse-and-cee-in-nn.md","raw":"---\ntitle: \ndate: 2017-12-03 12:39:14\ncategories: ML\ntags: \n     - Deep learning\n     - Loss function\ndescription: \n---\n\n# \n\n\n\n BP  $\\delta$  BP \n\n\n\n- $net$  $net_{i}$  $net = w^{\\mathsf{T}}x$ \n- $a$  $a_{i}$  $a=f(net)$ \n\n# \n\n identical $a=net=w^{\\mathsf{T}}x$ \n\n$$\nJ(w) = \\frac{1}{2} (a-y)^{2}=\\frac{1}{2} (net-y)^{2}\n$$\n\n$$\n\\delta = \\frac{\\partial J}{\\partial net} = net-y = a-y\n$$\n\n# \n\n\n\n## \n\n sigmoid  $a=\\sigma(net)=\\sigma(w^{\\mathsf{T}}x)=\\frac{1}{1+\\exp(-w^{\\mathsf{T}}x)}$ \n\n$$\nJ(w) = -L(w) = -y \\ln a - (1-y) \\ln (1-a)\n$$\n\n$$\nL(w) = y \\ln a + (1-y) \\ln (1-a)\n$$\n log-likelihood\n\n\n$$\n\\delta = \\frac{\\partial J}{\\partial net} = \\frac{\\partial J}{\\partial a} \\frac{\\partial a}{\\partial net} = \\frac{a-y}{(1-a)a}(1-a)a = a-y\n$$\n\n## \n\n softmax $a=\\mathrm{softmax}(net)=\\frac{\\exp(net_{i})}{\\sum_{j=1}^{N}\\exp(net_{j})}$ \n\n\n$$\nJ(w) = -\\sum_{j=1}^{N} y_j \\ln a_j = \\sum_{j=1}^{N} y_j (\\ln \\sum_{k=1}^{N} e^{net_{k}}-net_{j})\n$$\n $i$ \n$$\n\\delta_{i} = \\frac{\\partial J}{\\partial net_{i}} = \\sum_{j=1}^{N} y_j (\\frac{\\sum_{k=1}^{N} e^{net_{k}}\\frac{\\partial net_{k}}{\\partial net_{i}}}{\\sum_{k=1}^{N} e^{net_{k}}} - \\frac{\\partial net_{j}}{\\partial net_{i}}) = \\sum_{j=1}^{N} y_j  \\frac{e^{net_{i}}}{\\sum_{k=1}^{N} e^{net_{k}}} - \\sum_{j=1}^{N} y_j  \\frac{\\partial net_{j}}{\\partial net_{i}} = a_{i}-y_{i}\n$$\n","slug":"common-ground-of-mse-and-cee-in-nn","published":1,"updated":"2021-07-19T13:03:31.696Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrancn9j000gg0vwliaw3abq","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n<p> BP  $\\delta$  BP </p>\n<p></p>\n<ul>\n<li>$net$  $net_{i}$  $net = w^{\\mathsf{T}}x$ </li>\n<li>$a$  $a_{i}$  $a=f(net)$ </li>\n</ul>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> identical $a=net=w^{\\mathsf{T}}x$ <br><br>$$<br>J(w) = \\frac{1}{2} (a-y)^{2}=\\frac{1}{2} (net-y)^{2}<br>$$<br><br>$$<br>\\delta = \\frac{\\partial J}{\\partial net} = net-y = a-y<br>$$</p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> sigmoid  $a=\\sigma(net)=\\sigma(w^{\\mathsf{T}}x)=\\frac{1}{1+\\exp(-w^{\\mathsf{T}}x)}$ <br><br>$$<br>J(w) = -L(w) = -y \\ln a - (1-y) \\ln (1-a)<br>$$<br><br>$$<br>L(w) = y \\ln a + (1-y) \\ln (1-a)<br>$$<br> log-likelihood</p>\n<p><br>$$<br>\\delta = \\frac{\\partial J}{\\partial net} = \\frac{\\partial J}{\\partial a} \\frac{\\partial a}{\\partial net} = \\frac{a-y}{(1-a)a}(1-a)a = a-y<br>$$</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> softmax $a=\\mathrm{softmax}(net)=\\frac{\\exp(net_{i})}{\\sum_{j=1}^{N}\\exp(net_{j})}$ </p>\n<p><br>$$<br>J(w) = -\\sum_{j=1}^{N} y_j \\ln a_j = \\sum_{j=1}^{N} y_j (\\ln \\sum_{k=1}^{N} e^{net_{k}}-net_{j})<br>$$<br> $i$ <br>$$<br>\\delta_{i} = \\frac{\\partial J}{\\partial net_{i}} = \\sum_{j=1}^{N} y_j (\\frac{\\sum_{k=1}^{N} e^{net_{k}}\\frac{\\partial net_{k}}{\\partial net_{i}}}{\\sum_{k=1}^{N} e^{net_{k}}} - \\frac{\\partial net_{j}}{\\partial net_{i}}) = \\sum_{j=1}^{N} y_j  \\frac{e^{net_{i}}}{\\sum_{k=1}^{N} e^{net_{k}}} - \\sum_{j=1}^{N} y_j  \\frac{\\partial net_{j}}{\\partial net_{i}} = a_{i}-y_{i}<br>$$</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n<p> BP  $\\delta$  BP </p>\n<p></p>\n<ul>\n<li>$net$  $net_{i}$  $net = w^{\\mathsf{T}}x$ </li>\n<li>$a$  $a_{i}$  $a=f(net)$ </li>\n</ul>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> identical $a=net=w^{\\mathsf{T}}x$ <br><br>$$<br>J(w) = \\frac{1}{2} (a-y)^{2}=\\frac{1}{2} (net-y)^{2}<br>$$<br><br>$$<br>\\delta = \\frac{\\partial J}{\\partial net} = net-y = a-y<br>$$</p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> sigmoid  $a=\\sigma(net)=\\sigma(w^{\\mathsf{T}}x)=\\frac{1}{1+\\exp(-w^{\\mathsf{T}}x)}$ <br><br>$$<br>J(w) = -L(w) = -y \\ln a - (1-y) \\ln (1-a)<br>$$<br><br>$$<br>L(w) = y \\ln a + (1-y) \\ln (1-a)<br>$$<br> log-likelihood</p>\n<p><br>$$<br>\\delta = \\frac{\\partial J}{\\partial net} = \\frac{\\partial J}{\\partial a} \\frac{\\partial a}{\\partial net} = \\frac{a-y}{(1-a)a}(1-a)a = a-y<br>$$</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> softmax $a=\\mathrm{softmax}(net)=\\frac{\\exp(net_{i})}{\\sum_{j=1}^{N}\\exp(net_{j})}$ </p>\n<p><br>$$<br>J(w) = -\\sum_{j=1}^{N} y_j \\ln a_j = \\sum_{j=1}^{N} y_j (\\ln \\sum_{k=1}^{N} e^{net_{k}}-net_{j})<br>$$<br> $i$ <br>$$<br>\\delta_{i} = \\frac{\\partial J}{\\partial net_{i}} = \\sum_{j=1}^{N} y_j (\\frac{\\sum_{k=1}^{N} e^{net_{k}}\\frac{\\partial net_{k}}{\\partial net_{i}}}{\\sum_{k=1}^{N} e^{net_{k}}} - \\frac{\\partial net_{j}}{\\partial net_{i}}) = \\sum_{j=1}^{N} y_j  \\frac{e^{net_{i}}}{\\sum_{k=1}^{N} e^{net_{k}}} - \\sum_{j=1}^{N} y_j  \\frac{\\partial net_{j}}{\\partial net_{i}} = a_{i}-y_{i}<br>$$</p>\n"},{"title":" PCA  LFM ","date":"2017-08-08T12:25:59.000Z","_content":"\n\nPCAPrincipal Component AnalysisLFMLatent Factor Model\nPCA  LFM \n\n PCA  LFM  user-item \n\n\n\n<!-- more -->\n\n1. PCA\n\n   PCA $p$  $q$ \n    $\\lbrace X_{1}, X_{2}, \\cdots, X_{p} \\rbrace$  $\\lbrace Y_{1}, Y_{2}, \\cdots, Y_{q} \\rbrace$ \n   $$\n   Y_{i} = \\sum_{j=1}^{p} \\alpha_{j}^{(i)} X_{j}\n   $$\n   PCAcorrelation  correlation \n\n    $Y_{i}$  $X_{j}$  $\\alpha_{j}^{(i)}$  $\\alpha_{j}^{(i)}$  $0$  $0$  $Y_{i}$  $X_{j}$  $\\alpha_{j}^{(i)} $  $0$  $p$  $q$ \n\n   PCA  $q$ eigenvector $q$  $q$ eigenvalue $q$ \n\n   PCA \n\n2. LFM\n\n    $\\lbrace X_{1}, X_{2}, \\cdots, X_{p} \\rbrace$  $Z_{k}$\n   $$\n   \\forall X_{i}, \\text{  } \\beta_{k}^{(i)}, \\text{  } X_{i} = \\sum_{k=1}^{r} \\beta_{k}^{(i)} Z_{k}\n   $$\n    $\\lbrace Z_{1}, Z_{2}, \\cdots, Z_{r} \\rbrace$ \n    $\\beta^{\\mathsf{T}}$  user  $Z$  item  user-item LFM\n\n    $X$  $\\beta$  $X$ \n   \n\n   \n\n****\nPCA\nLFM\n","source":"_posts/compare-between-PCA-and-LFM.md","raw":"---\ntitle:  PCA  LFM \ndate: 2017-08-08 20:25:59\ncategories: ML\ntags: \n      - PCA\n      - LFM\n---\n\n\nPCAPrincipal Component AnalysisLFMLatent Factor Model\nPCA  LFM \n\n PCA  LFM  user-item \n\n\n\n<!-- more -->\n\n1. PCA\n\n   PCA $p$  $q$ \n    $\\lbrace X_{1}, X_{2}, \\cdots, X_{p} \\rbrace$  $\\lbrace Y_{1}, Y_{2}, \\cdots, Y_{q} \\rbrace$ \n   $$\n   Y_{i} = \\sum_{j=1}^{p} \\alpha_{j}^{(i)} X_{j}\n   $$\n   PCAcorrelation  correlation \n\n    $Y_{i}$  $X_{j}$  $\\alpha_{j}^{(i)}$  $\\alpha_{j}^{(i)}$  $0$  $0$  $Y_{i}$  $X_{j}$  $\\alpha_{j}^{(i)} $  $0$  $p$  $q$ \n\n   PCA  $q$ eigenvector $q$  $q$ eigenvalue $q$ \n\n   PCA \n\n2. LFM\n\n    $\\lbrace X_{1}, X_{2}, \\cdots, X_{p} \\rbrace$  $Z_{k}$\n   $$\n   \\forall X_{i}, \\text{  } \\beta_{k}^{(i)}, \\text{  } X_{i} = \\sum_{k=1}^{r} \\beta_{k}^{(i)} Z_{k}\n   $$\n    $\\lbrace Z_{1}, Z_{2}, \\cdots, Z_{r} \\rbrace$ \n    $\\beta^{\\mathsf{T}}$  user  $Z$  item  user-item LFM\n\n    $X$  $\\beta$  $X$ \n   \n\n   \n\n****\nPCA\nLFM\n","slug":"compare-between-PCA-and-LFM","published":1,"updated":"2021-07-19T13:03:31.696Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrancn9k000ig0vwnlj7hffx","content":"<p><br>PCAPrincipal Component AnalysisLFMLatent Factor Model<br>PCA  LFM <br><br> PCA  LFM  user-item </p>\n<p></p>\n<a id=\"more\"></a>\n<ol>\n<li><p>PCA</p>\n<p>PCA $p$  $q$ <br> $\\lbrace X_{1}, X_{2}, \\cdots, X_{p} \\rbrace$  $\\lbrace Y_{1}, Y_{2}, \\cdots, Y_{q} \\rbrace$ <br>$$<br>Y_{i} = \\sum_{j=1}^{p} \\alpha_{j}^{(i)} X_{j}<br>$$<br>PCAcorrelation  correlation </p>\n<p> $Y_{i}$  $X_{j}$  $\\alpha_{j}^{(i)}$  $\\alpha_{j}^{(i)}$  $0$  $0$  $Y_{i}$  $X_{j}$  $\\alpha_{j}^{(i)} $  $0$  $p$  $q$ </p>\n<p>PCA  $q$ eigenvector $q$  $q$ eigenvalue $q$ </p>\n<p>PCA </p>\n</li>\n<li><p>LFM</p>\n<p> $\\lbrace X_{1}, X_{2}, \\cdots, X_{p} \\rbrace$  $Z_{k}$<br>$$<br>\\forall X_{i}, \\text{  } \\beta_{k}^{(i)}, \\text{  } X_{i} = \\sum_{k=1}^{r} \\beta_{k}^{(i)} Z_{k}<br>$$<br> $\\lbrace Z_{1}, Z_{2}, \\cdots, Z_{r} \\rbrace$ <br> $\\beta^{\\mathsf{T}}$  user  $Z$  item  user-item LFM</p>\n<p> $X$  $\\beta$  $X$ <br></p>\n<p></p>\n</li>\n</ol>\n<p><strong></strong><br>PCA<br>LFM</p>\n","site":{"data":{}},"excerpt":"<p><br>PCAPrincipal Component AnalysisLFMLatent Factor Model<br>PCA  LFM <br><br> PCA  LFM  user-item </p>\n<p></p>","more":"<ol>\n<li><p>PCA</p>\n<p>PCA $p$  $q$ <br> $\\lbrace X_{1}, X_{2}, \\cdots, X_{p} \\rbrace$  $\\lbrace Y_{1}, Y_{2}, \\cdots, Y_{q} \\rbrace$ <br>$$<br>Y_{i} = \\sum_{j=1}^{p} \\alpha_{j}^{(i)} X_{j}<br>$$<br>PCAcorrelation  correlation </p>\n<p> $Y_{i}$  $X_{j}$  $\\alpha_{j}^{(i)}$  $\\alpha_{j}^{(i)}$  $0$  $0$  $Y_{i}$  $X_{j}$  $\\alpha_{j}^{(i)} $  $0$  $p$  $q$ </p>\n<p>PCA  $q$ eigenvector $q$  $q$ eigenvalue $q$ </p>\n<p>PCA </p>\n</li>\n<li><p>LFM</p>\n<p> $\\lbrace X_{1}, X_{2}, \\cdots, X_{p} \\rbrace$  $Z_{k}$<br>$$<br>\\forall X_{i}, \\text{  } \\beta_{k}^{(i)}, \\text{  } X_{i} = \\sum_{k=1}^{r} \\beta_{k}^{(i)} Z_{k}<br>$$<br> $\\lbrace Z_{1}, Z_{2}, \\cdots, Z_{r} \\rbrace$ <br> $\\beta^{\\mathsf{T}}$  user  $Z$  item  user-item LFM</p>\n<p> $X$  $\\beta$  $X$ <br></p>\n<p></p>\n</li>\n</ol>\n<p><strong></strong><br>PCA<br>LFM</p>"},{"title":"","date":"2018-02-04T14:17:38.000Z","description":"","_content":"\n# \n\n**Discriminative model** $+1$  $-1$ \n\nLinear RegressionLogistic RegressionSVMTraditional Neural Networks)Linear Discriminative AnalysisConditional Random Field\n\n**Generative model**\n\nNaive BayesHidden Markov ModelBayes NetworksLatent Dirichlet Allocation\n\n# \n\n\n\n-  $(C, X)$ $C=\\{c_1, c_2, \\cdots, c_n\\}$ n$X=\\{x_1, x_2, \\cdots, x_n\\}$ n\n-  $(\\tilde{c}, \\tilde{x})$ $\\tilde{c}$ $\\tilde{x}$ \n\n## \n\n $P(\\tilde{c} | \\tilde{x}) = P(\\tilde{c} | \\tilde{x}, C, X)$  $\\theta$  $P(\\tilde{c} | \\tilde{x},\\theta)$ \n$$\n\\begin{aligned}\n&P(\\tilde{c} | \\tilde{x}) \\\\\n=&P(\\tilde{c} | \\tilde{x}, C, X) \\\\\n=&\\int P(\\tilde{c}, \\theta | \\tilde{x}, C, X) \\, d \\theta \\\\\n=&\\int P(\\tilde{c} | \\tilde{x}, \\theta) \\cdot P(\\theta | C, X) \\, d \\theta \\\\\n=&\\int P(\\tilde{c} | \\tilde{x}, \\theta) \\cdot \\frac{P(C | X, \\theta) \\cdot P(\\theta)}{\\int P(C | X, \\theta) \\cdot P(\\theta) \\, d \\theta} \\, d \\theta\n\\end{aligned}\n$$\n\n\n variational inference  $\\theta$  $\\theta_{MAP}$  $\\theta$ point estimate $P(\\tilde{c} | \\tilde{x}) = P(\\tilde{c} | \\tilde{x}, C, X) = P(\\tilde{c} | \\tilde{x},\\theta_{MAP})$ \n\n $P(C|X) = \\int P(C | X, \\theta) \\cdot P(\\theta) \\, d \\theta$  $P(C | X, \\theta) \\cdot P(\\theta)$ \n\n\n\n/\n\n## \n\n $P(\\tilde{x}, \\tilde{c})$ \n\n$P(\\tilde{x}, \\tilde{c}) = P(\\tilde{x} | \\tilde{c}) \\cdot P(\\tilde{c})$ \n\n\n\n# \n\n## \n\n\n\n1. \n2. \n3. \n\n\n\n\n\n## \n\n\n\n1. \n2. \n3. Gaussian Mixture Model\n\n\n\n1. ","source":"_posts/discriminative-or-generative-model.md","raw":"---\ntitle: \ndate: 2018-02-04 22:17:38\ncategories: ML\ntags:\n     - Discriminative model\n     - Generative model\ndescription: \n---\n\n# \n\n**Discriminative model** $+1$  $-1$ \n\nLinear RegressionLogistic RegressionSVMTraditional Neural Networks)Linear Discriminative AnalysisConditional Random Field\n\n**Generative model**\n\nNaive BayesHidden Markov ModelBayes NetworksLatent Dirichlet Allocation\n\n# \n\n\n\n-  $(C, X)$ $C=\\{c_1, c_2, \\cdots, c_n\\}$ n$X=\\{x_1, x_2, \\cdots, x_n\\}$ n\n-  $(\\tilde{c}, \\tilde{x})$ $\\tilde{c}$ $\\tilde{x}$ \n\n## \n\n $P(\\tilde{c} | \\tilde{x}) = P(\\tilde{c} | \\tilde{x}, C, X)$  $\\theta$  $P(\\tilde{c} | \\tilde{x},\\theta)$ \n$$\n\\begin{aligned}\n&P(\\tilde{c} | \\tilde{x}) \\\\\n=&P(\\tilde{c} | \\tilde{x}, C, X) \\\\\n=&\\int P(\\tilde{c}, \\theta | \\tilde{x}, C, X) \\, d \\theta \\\\\n=&\\int P(\\tilde{c} | \\tilde{x}, \\theta) \\cdot P(\\theta | C, X) \\, d \\theta \\\\\n=&\\int P(\\tilde{c} | \\tilde{x}, \\theta) \\cdot \\frac{P(C | X, \\theta) \\cdot P(\\theta)}{\\int P(C | X, \\theta) \\cdot P(\\theta) \\, d \\theta} \\, d \\theta\n\\end{aligned}\n$$\n\n\n variational inference  $\\theta$  $\\theta_{MAP}$  $\\theta$ point estimate $P(\\tilde{c} | \\tilde{x}) = P(\\tilde{c} | \\tilde{x}, C, X) = P(\\tilde{c} | \\tilde{x},\\theta_{MAP})$ \n\n $P(C|X) = \\int P(C | X, \\theta) \\cdot P(\\theta) \\, d \\theta$  $P(C | X, \\theta) \\cdot P(\\theta)$ \n\n\n\n/\n\n## \n\n $P(\\tilde{x}, \\tilde{c})$ \n\n$P(\\tilde{x}, \\tilde{c}) = P(\\tilde{x} | \\tilde{c}) \\cdot P(\\tilde{c})$ \n\n\n\n# \n\n## \n\n\n\n1. \n2. \n3. \n\n\n\n\n\n## \n\n\n\n1. \n2. \n3. Gaussian Mixture Model\n\n\n\n1. ","slug":"discriminative-or-generative-model","published":1,"updated":"2021-07-19T13:03:31.696Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrancn9m000mg0vwiujvh1ml","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p><strong>Discriminative model</strong> $+1$  $-1$ </p>\n<p>Linear RegressionLogistic RegressionSVMTraditional Neural Networks)Linear Discriminative AnalysisConditional Random Field</p>\n<p><strong>Generative model</strong></p>\n<p>Naive BayesHidden Markov ModelBayes NetworksLatent Dirichlet Allocation</p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n<ul>\n<li> $(C, X)$ $C={c_1, c_2, \\cdots, c_n}$ n$X={x_1, x_2, \\cdots, x_n}$ n</li>\n<li> $(\\tilde{c}, \\tilde{x})$ $\\tilde{c}$ $\\tilde{x}$ </li>\n</ul>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> $P(\\tilde{c} | \\tilde{x}) = P(\\tilde{c} | \\tilde{x}, C, X)$  $\\theta$  $P(\\tilde{c} | \\tilde{x},\\theta)$ <br>$$<br>\\begin{aligned}<br>&amp;P(\\tilde{c} | \\tilde{x}) \\<br>=&amp;P(\\tilde{c} | \\tilde{x}, C, X) \\<br>=&amp;\\int P(\\tilde{c}, \\theta | \\tilde{x}, C, X) \\, d \\theta \\<br>=&amp;\\int P(\\tilde{c} | \\tilde{x}, \\theta) \\cdot P(\\theta | C, X) \\, d \\theta \\<br>=&amp;\\int P(\\tilde{c} | \\tilde{x}, \\theta) \\cdot \\frac{P(C | X, \\theta) \\cdot P(\\theta)}{\\int P(C | X, \\theta) \\cdot P(\\theta) \\, d \\theta} \\, d \\theta<br>\\end{aligned}<br>$$<br></p>\n<p> variational inference  $\\theta$  $\\theta_{MAP}$  $\\theta$ point estimate $P(\\tilde{c} | \\tilde{x}) = P(\\tilde{c} | \\tilde{x}, C, X) = P(\\tilde{c} | \\tilde{x},\\theta_{MAP})$ </p>\n<p> $P(C|X) = \\int P(C | X, \\theta) \\cdot P(\\theta) \\, d \\theta$  $P(C | X, \\theta) \\cdot P(\\theta)$ </p>\n<p></p>\n<p>/</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> $P(\\tilde{x}, \\tilde{c})$ </p>\n<p>$P(\\tilde{x}, \\tilde{c}) = P(\\tilde{x} | \\tilde{c}) \\cdot P(\\tilde{c})$ </p>\n<p></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><h2 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"\"></a></h2><p></p>\n<ol>\n<li></li>\n<li></li>\n<li></li>\n</ol>\n<p></p>\n<p></p>\n<h2 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"\"></a></h2><p></p>\n<ol>\n<li></li>\n<li></li>\n<li>Gaussian Mixture Model</li>\n</ol>\n<p></p>\n<ol>\n<li></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p><strong>Discriminative model</strong> $+1$  $-1$ </p>\n<p>Linear RegressionLogistic RegressionSVMTraditional Neural Networks)Linear Discriminative AnalysisConditional Random Field</p>\n<p><strong>Generative model</strong></p>\n<p>Naive BayesHidden Markov ModelBayes NetworksLatent Dirichlet Allocation</p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n<ul>\n<li> $(C, X)$ $C={c_1, c_2, \\cdots, c_n}$ n$X={x_1, x_2, \\cdots, x_n}$ n</li>\n<li> $(\\tilde{c}, \\tilde{x})$ $\\tilde{c}$ $\\tilde{x}$ </li>\n</ul>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> $P(\\tilde{c} | \\tilde{x}) = P(\\tilde{c} | \\tilde{x}, C, X)$  $\\theta$  $P(\\tilde{c} | \\tilde{x},\\theta)$ <br>$$<br>\\begin{aligned}<br>&amp;P(\\tilde{c} | \\tilde{x}) \\<br>=&amp;P(\\tilde{c} | \\tilde{x}, C, X) \\<br>=&amp;\\int P(\\tilde{c}, \\theta | \\tilde{x}, C, X) \\, d \\theta \\<br>=&amp;\\int P(\\tilde{c} | \\tilde{x}, \\theta) \\cdot P(\\theta | C, X) \\, d \\theta \\<br>=&amp;\\int P(\\tilde{c} | \\tilde{x}, \\theta) \\cdot \\frac{P(C | X, \\theta) \\cdot P(\\theta)}{\\int P(C | X, \\theta) \\cdot P(\\theta) \\, d \\theta} \\, d \\theta<br>\\end{aligned}<br>$$<br></p>\n<p> variational inference  $\\theta$  $\\theta_{MAP}$  $\\theta$ point estimate $P(\\tilde{c} | \\tilde{x}) = P(\\tilde{c} | \\tilde{x}, C, X) = P(\\tilde{c} | \\tilde{x},\\theta_{MAP})$ </p>\n<p> $P(C|X) = \\int P(C | X, \\theta) \\cdot P(\\theta) \\, d \\theta$  $P(C | X, \\theta) \\cdot P(\\theta)$ </p>\n<p></p>\n<p>/</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> $P(\\tilde{x}, \\tilde{c})$ </p>\n<p>$P(\\tilde{x}, \\tilde{c}) = P(\\tilde{x} | \\tilde{c}) \\cdot P(\\tilde{c})$ </p>\n<p></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><h2 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"\"></a></h2><p></p>\n<ol>\n<li></li>\n<li></li>\n<li></li>\n</ol>\n<p></p>\n<p></p>\n<h2 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"\"></a></h2><p></p>\n<ol>\n<li></li>\n<li></li>\n<li>Gaussian Mixture Model</li>\n</ol>\n<p></p>\n<ol>\n<li></li>\n</ol>\n"},{"title":"","date":"2017-08-07T14:28:18.000Z","_content":"\n\n\n<!-- more -->\n\n[](http://txshi-mt.com)\n\n\n\n\n\n\n\n\n\n\nAI\n","source":"_posts/init-blog.md","raw":"---\ntitle: \ndate: 2017-08-07 22:28:18\ncategories: Life\ntags:\n---\n\n\n\n<!-- more -->\n\n[](http://txshi-mt.com)\n\n\n\n\n\n\n\n\n\n\nAI\n","slug":"init-blog","published":1,"updated":"2021-07-19T13:03:31.697Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrancn9n000pg0vwrhf1e5th","content":"<p></p>\n<a id=\"more\"></a>\n<p><a href=\"http://txshi-mt.com\" target=\"_blank\" rel=\"noopener\"></a><br><br><br></p>\n<p><br><br><br></p>\n<p><br>AI</p>\n","site":{"data":{}},"excerpt":"<p></p>","more":"<p><a href=\"http://txshi-mt.com\" target=\"_blank\" rel=\"noopener\"></a><br><br><br></p>\n<p><br><br><br></p>\n<p><br>AI</p>"},{"title":" boostAdaboost  ","date":"2017-10-15T03:31:35.000Z","description":" boost  Adaboost ","_content":"\n# \n\n1. boost =  + \n2. Adaboost = boost + \n3.  = boost + \n\n\n\n1. boost \n2. Adaboost  boost  boost \n\n# \n\n\n\n\n\n# \n\n Adaboost \n\n BRT \n\n GBDT/GBRT \n\n# \n\nBDT +  Adaboost \n\nBRT + \n\nGBDT + \n\nGBRT + \n\n GBRT  BRT","source":"_posts/from-boost-to-Adaboost-to-GBT.md","raw":"---\ntitle:  boostAdaboost  \ndate: 2017-10-15 11:31:35\ncategories: ML\ntags:\n     - Ensemble\n     - Boost\n     - GBDT/GBRT\ndescription:  boost  Adaboost \n---\n\n# \n\n1. boost =  + \n2. Adaboost = boost + \n3.  = boost + \n\n\n\n1. boost \n2. Adaboost  boost  boost \n\n# \n\n\n\n\n\n# \n\n Adaboost \n\n BRT \n\n GBDT/GBRT \n\n# \n\nBDT +  Adaboost \n\nBRT + \n\nGBDT + \n\nGBRT + \n\n GBRT  BRT","slug":"from-boost-to-Adaboost-to-GBT","published":1,"updated":"2021-07-19T13:03:31.697Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrancn9p000ug0vwafj50q6v","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><ol>\n<li>boost =  + </li>\n<li>Adaboost = boost + </li>\n<li> = boost + </li>\n</ol>\n<p></p>\n<ol>\n<li>boost </li>\n<li>Adaboost  boost  boost </li>\n</ol>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n<p></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> Adaboost </p>\n<p> BRT </p>\n<p> GBDT/GBRT </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>BDT +  Adaboost </p>\n<p>BRT + </p>\n<p>GBDT + </p>\n<p>GBRT + </p>\n<p> GBRT  BRT</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><ol>\n<li>boost =  + </li>\n<li>Adaboost = boost + </li>\n<li> = boost + </li>\n</ol>\n<p></p>\n<ol>\n<li>boost </li>\n<li>Adaboost  boost  boost </li>\n</ol>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n<p></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> Adaboost </p>\n<p> BRT </p>\n<p> GBDT/GBRT </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>BDT +  Adaboost </p>\n<p>BRT + </p>\n<p>GBDT + </p>\n<p>GBRT + </p>\n<p> GBRT  BRT</p>\n"},{"title":" L0 L1 L2 ","date":"2017-09-07T06:43:10.000Z","description":" L0 L1 L2 ","_content":"\n# \n\nMinimize your error while regularizing your parameters\nminimize error\"\nregularizing parameters Occam's Razor\n\n\n$$\nw^{\\ast} = \\mathop{\\arg\\min}_{w} \\sum_{i} L(y_{i}, f(x_{i};w)) + \\lambda\\Omega(w)\n$$\n\n\n- Square loss  \n- Hinge loss  SVM\n- Exp loss  Boosting\n- Log loss  Logistic regression\n\n\n loss \n\n $ \\Omega(w) $ \n\n# \n\n $w$ L0 L1 L2 Frobenius \n\n L0 L1   L2 \n\n\n\n## L0  L1 \n\n\n\nL0  0 \n NP-hard \n L0  L1 [^1]\n\n\n\n-  0 \n-  10  10 \n\n## L2 \n\n L1  L2 \n\n L2 \n\nill-condition\n\n $AX = b$  $A$  $b$  $X$ ill-conditionwell-condition\n\n\n\n $A$  $A$ condition number\n$$\n\\kappa(A) = \\lVert A \\rVert \\lVert A^{-1} \\rVert\n$$\n A\n$$\n\\frac{\\lVert \\Delta x \\rVert}{\\lVert x \\rVert} \\le \\kappa(A) \\cdot \\frac{\\lVert \\Delta b \\rVert}{\\lVert b \\rVert} \\\\\n\\frac{\\lVert \\Delta x \\rVert}{\\lVert x + \\Delta x \\rVert} \\le \\kappa(A) \\cdot \\frac{\\lVert \\Delta A \\rVert}{\\lVert A \\rVert}\n$$\ncondition number \n condition number  1  well-condition  ill-condition \n\n\n$$\nw^{\\ast} = (X^{\\mathsf{T}}X)^{-1}X^{\\mathsf{T}}y\n$$\n $ X^{\\mathsf{T}}X $ \n L2 \n$$\nw^{\\ast} = (X^{\\mathsf{T}}X + \\lambda I)^{-1}X^{\\mathsf{T}}y\n$$\n\n\n\n $ \\lambda $ $\\lambda$ -strongly convex\n\n $f$ \n$$\nf(y) \\ge f(x) + \\langle \\nabla f(x), y-x \\rangle + \\frac{\\lambda}{2} \\lVert y-x \\rVert ^{2}\n$$\n $f$  $ \\lambda $ $\\lambda$ -strongly convex\n\n\n$$\nf(y) \\ge f(x) + \\langle \\nabla f(x), y-x \\rangle + o (\\lVert y-x \\rVert)\n$$\n\n\n\n[^1]: [Ramirez, C. & V. Kreinovich & M. Argaez. Why l1 is a good approximation to l0: A geometric explanation [J]. *Engineering*, 2013, 7 (3): 203-207](https://www.researchgate.net/publication/290729378_Why_l1_is_a_good_approximation_to_l0_A_geometric_explanation)\n\n","source":"_posts/norm-regularization-01.md","raw":"---\ntitle:  L0 L1 L2 \ndate: 2017-09-07 14:43:10\ncategories: ML\ntags:\n      - Norm regularization\n      - Convex optimization\ndescription:  L0 L1 L2 \n---\n\n# \n\nMinimize your error while regularizing your parameters\nminimize error\"\nregularizing parameters Occam's Razor\n\n\n$$\nw^{\\ast} = \\mathop{\\arg\\min}_{w} \\sum_{i} L(y_{i}, f(x_{i};w)) + \\lambda\\Omega(w)\n$$\n\n\n- Square loss  \n- Hinge loss  SVM\n- Exp loss  Boosting\n- Log loss  Logistic regression\n\n\n loss \n\n $ \\Omega(w) $ \n\n# \n\n $w$ L0 L1 L2 Frobenius \n\n L0 L1   L2 \n\n\n\n## L0  L1 \n\n\n\nL0  0 \n NP-hard \n L0  L1 [^1]\n\n\n\n-  0 \n-  10  10 \n\n## L2 \n\n L1  L2 \n\n L2 \n\nill-condition\n\n $AX = b$  $A$  $b$  $X$ ill-conditionwell-condition\n\n\n\n $A$  $A$ condition number\n$$\n\\kappa(A) = \\lVert A \\rVert \\lVert A^{-1} \\rVert\n$$\n A\n$$\n\\frac{\\lVert \\Delta x \\rVert}{\\lVert x \\rVert} \\le \\kappa(A) \\cdot \\frac{\\lVert \\Delta b \\rVert}{\\lVert b \\rVert} \\\\\n\\frac{\\lVert \\Delta x \\rVert}{\\lVert x + \\Delta x \\rVert} \\le \\kappa(A) \\cdot \\frac{\\lVert \\Delta A \\rVert}{\\lVert A \\rVert}\n$$\ncondition number \n condition number  1  well-condition  ill-condition \n\n\n$$\nw^{\\ast} = (X^{\\mathsf{T}}X)^{-1}X^{\\mathsf{T}}y\n$$\n $ X^{\\mathsf{T}}X $ \n L2 \n$$\nw^{\\ast} = (X^{\\mathsf{T}}X + \\lambda I)^{-1}X^{\\mathsf{T}}y\n$$\n\n\n\n $ \\lambda $ $\\lambda$ -strongly convex\n\n $f$ \n$$\nf(y) \\ge f(x) + \\langle \\nabla f(x), y-x \\rangle + \\frac{\\lambda}{2} \\lVert y-x \\rVert ^{2}\n$$\n $f$  $ \\lambda $ $\\lambda$ -strongly convex\n\n\n$$\nf(y) \\ge f(x) + \\langle \\nabla f(x), y-x \\rangle + o (\\lVert y-x \\rVert)\n$$\n\n\n\n[^1]: [Ramirez, C. & V. Kreinovich & M. Argaez. Why l1 is a good approximation to l0: A geometric explanation [J]. *Engineering*, 2013, 7 (3): 203-207](https://www.researchgate.net/publication/290729378_Why_l1_is_a_good_approximation_to_l0_A_geometric_explanation)\n\n","slug":"norm-regularization-01","published":1,"updated":"2021-07-19T13:03:31.697Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrancn9p000wg0vw1sxmglt5","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>Minimize your error while regularizing your parameters<br>minimize error<br>regularizing parameters Occams Razor</p>\n<p><br>$$<br>w^{\\ast} = \\mathop{\\arg\\min}<em>{w} \\sum</em>{i} L(y_{i}, f(x_{i};w)) + \\lambda\\Omega(w)<br>$$<br></p>\n<ul>\n<li>Square loss  </li>\n<li>Hinge loss  SVM</li>\n<li>Exp loss  Boosting</li>\n<li>Log loss  Logistic regression</li>\n</ul>\n<p><br> loss </p>\n<p> $ \\Omega(w) $ </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> $w$ L0 L1 L2 Frobenius </p>\n<p> L0 L1   L2 </p>\n<p></p>\n<h2 id=\"L0--L1-\"><a href=\"#L0--L1-\" class=\"headerlink\" title=\"L0  L1 \"></a>L0  L1 </h2><p></p>\n<p>L0  0 <br> NP-hard <br> L0  L1 <sup id=\"fnref:1\"><a href=\"#fn:1\" rel=\"footnote\">1</a></sup></p>\n<p></p>\n<ul>\n<li> 0 </li>\n<li> 10  10 </li>\n</ul>\n<h2 id=\"L2-\"><a href=\"#L2-\" class=\"headerlink\" title=\"L2 \"></a>L2 </h2><p> L1  L2 </p>\n<p> L2 </p>\n<p>ill-condition<br><br> $AX = b$  $A$  $b$  $X$ ill-conditionwell-condition</p>\n<p></p>\n<p> $A$  $A$ condition number<br>$$<br>\\kappa(A) = \\lVert A \\rVert \\lVert A^{-1} \\rVert<br>$$<br> A<br>$$<br>\\frac{\\lVert \\Delta x \\rVert}{\\lVert x \\rVert} \\le \\kappa(A) \\cdot \\frac{\\lVert \\Delta b \\rVert}{\\lVert b \\rVert} \\<br>\\frac{\\lVert \\Delta x \\rVert}{\\lVert x + \\Delta x \\rVert} \\le \\kappa(A) \\cdot \\frac{\\lVert \\Delta A \\rVert}{\\lVert A \\rVert}<br>$$<br>condition number <br> condition number  1  well-condition  ill-condition </p>\n<p><br>$$<br>w^{\\ast} = (X^{\\mathsf{T}}X)^{-1}X^{\\mathsf{T}}y<br>$$<br> $ X^{\\mathsf{T}}X $ <br> L2 <br>$$<br>w^{\\ast} = (X^{\\mathsf{T}}X + \\lambda I)^{-1}X^{\\mathsf{T}}y<br>$$<br></p>\n<p><br> $ \\lambda $ $\\lambda$ -strongly convex</p>\n<p> $f$ <br>$$<br>f(y) \\ge f(x) + \\langle \\nabla f(x), y-x \\rangle + \\frac{\\lambda}{2} \\lVert y-x \\rVert ^{2}<br>$$<br> $f$  $ \\lambda $ $\\lambda$ -strongly convex</p>\n<p><br>$$<br>f(y) \\ge f(x) + \\langle \\nabla f(x), y-x \\rangle + o (\\lVert y-x \\rVert)<br>$$<br><br></p>\n<div id=\"footnotes\"><hr><div id=\"footnotelist\"><ol style=\"list-style:none; padding-left: 0;\"><li id=\"fn:1\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px;\">1.</span><span style=\"display: inline-block; vertical-align: top;\"><a href=\"https://www.researchgate.net/publication/290729378_Why_l1_is_a_good_approximation_to_l0_A_geometric_explanation\" target=\"_blank\" rel=\"noopener\">Ramirez, C. &amp; V. Kreinovich &amp; M. Argaez. Why l1 is a good approximation to l0: A geometric explanation [J]. <em>Engineering</em>, 2013, 7 (3): 203-207</a></span><a href=\"#fnref:1\" rev=\"footnote\"> </a></li></ol></div></div>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>Minimize your error while regularizing your parameters<br>minimize error<br>regularizing parameters Occams Razor</p>\n<p><br>$$<br>w^{\\ast} = \\mathop{\\arg\\min}<em>{w} \\sum</em>{i} L(y_{i}, f(x_{i};w)) + \\lambda\\Omega(w)<br>$$<br></p>\n<ul>\n<li>Square loss  </li>\n<li>Hinge loss  SVM</li>\n<li>Exp loss  Boosting</li>\n<li>Log loss  Logistic regression</li>\n</ul>\n<p><br> loss </p>\n<p> $ \\Omega(w) $ </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> $w$ L0 L1 L2 Frobenius </p>\n<p> L0 L1   L2 </p>\n<p></p>\n<h2 id=\"L0--L1-\"><a href=\"#L0--L1-\" class=\"headerlink\" title=\"L0  L1 \"></a>L0  L1 </h2><p></p>\n<p>L0  0 <br> NP-hard <br> L0  L1 <sup id=\"fnref:1\"><a href=\"#fn:1\" rel=\"footnote\">1</a></sup></p>\n<p></p>\n<ul>\n<li> 0 </li>\n<li> 10  10 </li>\n</ul>\n<h2 id=\"L2-\"><a href=\"#L2-\" class=\"headerlink\" title=\"L2 \"></a>L2 </h2><p> L1  L2 </p>\n<p> L2 </p>\n<p>ill-condition<br><br> $AX = b$  $A$  $b$  $X$ ill-conditionwell-condition</p>\n<p></p>\n<p> $A$  $A$ condition number<br>$$<br>\\kappa(A) = \\lVert A \\rVert \\lVert A^{-1} \\rVert<br>$$<br> A<br>$$<br>\\frac{\\lVert \\Delta x \\rVert}{\\lVert x \\rVert} \\le \\kappa(A) \\cdot \\frac{\\lVert \\Delta b \\rVert}{\\lVert b \\rVert} \\<br>\\frac{\\lVert \\Delta x \\rVert}{\\lVert x + \\Delta x \\rVert} \\le \\kappa(A) \\cdot \\frac{\\lVert \\Delta A \\rVert}{\\lVert A \\rVert}<br>$$<br>condition number <br> condition number  1  well-condition  ill-condition </p>\n<p><br>$$<br>w^{\\ast} = (X^{\\mathsf{T}}X)^{-1}X^{\\mathsf{T}}y<br>$$<br> $ X^{\\mathsf{T}}X $ <br> L2 <br>$$<br>w^{\\ast} = (X^{\\mathsf{T}}X + \\lambda I)^{-1}X^{\\mathsf{T}}y<br>$$<br></p>\n<p><br> $ \\lambda $ $\\lambda$ -strongly convex</p>\n<p> $f$ <br>$$<br>f(y) \\ge f(x) + \\langle \\nabla f(x), y-x \\rangle + \\frac{\\lambda}{2} \\lVert y-x \\rVert ^{2}<br>$$<br> $f$  $ \\lambda $ $\\lambda$ -strongly convex</p>\n<p><br>$$<br>f(y) \\ge f(x) + \\langle \\nabla f(x), y-x \\rangle + o (\\lVert y-x \\rVert)<br>$$<br><br></p>\n<div id=\"footnotes\"><hr><div id=\"footnotelist\"><ol style=\"list-style:none; padding-left: 0;\"><li id=\"fn:1\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px;\">1.</span><span style=\"display: inline-block; vertical-align: top;\"><a href=\"https://www.researchgate.net/publication/290729378_Why_l1_is_a_good_approximation_to_l0_A_geometric_explanation\" target=\"_blank\" rel=\"noopener\">Ramirez, C. &amp; V. Kreinovich &amp; M. Argaez. Why l1 is a good approximation to l0: A geometric explanation [J]. <em>Engineering</em>, 2013, 7 (3): 203-207</a></span><a href=\"#fnref:1\" rev=\"footnote\"> </a></li></ol></div></div>"},{"title":"K-means  GMM ","date":"2017-10-23T15:23:56.000Z","description":" GMM  K-means  EM ","_content":"\n#  GMM \n\n K-means \n GMMGaussian Mixture Model K-means  EM  K-means  EM \n\n\n\n**** $x_{1}, x_{2}, \\cdots, x_{N} \\in \\mathbb{R}^{m}$  $K$  $K$  $\\mu_{1}, \\mu_{2}, \\cdots, \\mu_{K}$  $\\sum\\limits_{i=1}^{N} \\min\\limits_{1 \\le k \\le K} \\lVert x_{i} - \\mu_{k} \\rVert_{2}^{2}$ \n\n**** $x_{1}, x_{2}, \\cdots, x_{N} \\in \\mathbb{R}^{m}$  $K$ \n$$\np(x, z \\mid \\mu_{1}, \\mu_{2}, \\cdots, \\mu_{K}) \\propto \n\\left \\{\n\\begin{aligned}\n&\\exp \\left ( -\\lVert x-\\mu_{z} \\rVert_{2}^{2} \\right ) & \\qquad \\lVert x-\\mu_{z} \\rVert_{2} = \\min_{1 \\le k \\le K} \\lVert x-\\mu_{k} \\rVert_{2} \\\\\n&0 & \\qquad \\lVert x-\\mu_{z} \\rVert_{2} > \\min_{1 \\le k \\le K} \\lVert x-\\mu_{k} \\rVert_{2}\n\\end{aligned}\n\\right.\n$$\n $z \\in \\{1, 2, \\cdots, K\\}$ \n\n $K$  $K$ \n\n EM  K-means \n\n**E **\n\n1. \n   $$\n   p(z_{i} \\mid x_{i}, \\{\\mu_{k}\\}^{(t)}) \\propto \n   \\left \\{\n   \\begin{aligned}\n   &1 & \\qquad \\lVert x_{i}-\\mu_{z_{i}}^{(t)} \\rVert_{2} = \\min_{1 \\le k \\le K} \\lVert x_{i}-\\mu_{k}^{(t)} \\rVert_{2} \\\\\n   &0 & \\qquad \\lVert x_{i}-\\mu_{z_{i}}^{(t)} \\rVert_{2} > \\min_{1 \\le k \\le K} \\lVert x_{i}-\\mu_{k}^{(t)} \\rVert_{2}\n   \\end{aligned}\n   \\right.\n   $$\n   \n   $$\n   p(Z \\mid X, \\{ \\mu_{k} \\}^{(t)}) = \\prod_{i=1}^{N} p(z_{i} \\mid x_{i}, \\{ \\mu_{k} \\}^{(t)})\n   $$\n   \n\n    $y_{i}$ \n\n2. \n   $$\n   \\begin{aligned}\n   Q(\\{ \\mu_{k} \\} \\mid \\{ \\mu_{k} \\}^{(t)}) & = E_{Z \\mid X, \\{ \\mu_{k} \\}^{(t)}} [\\log p(X, Z \\mid \\{ \\mu_{k} \\})] \\\\\n   & = \\sum_{i=1}^{N} E_{z_{i} \\mid x_{i}, \\{ \\mu_{k} \\}^{(t)}} [\\log p(x_{i}, z_{i} \\mid \\{ \\mu_{k} \\})] \\\\\n   & = \\sum_{i=1}^{N} \\log p(x_{i}, z_{i}=y_{i} \\mid \\{ \\mu_{k} \\}) \\\\\n   & = \\mathrm{const} - \\sum_{i=1}^{N} \\lVert x_{i} - \\mu_{y_{i}} \\rVert_{2}^{2}\n   \\end{aligned}\n   $$\n    $Q'(\\{ \\mu_{k} \\} \\mid \\{ \\mu_{k} \\}^{(t)}) = \\sum\\limits_{i=1}^{N} \\lVert x_{i} - \\mu_{y_{i}} \\rVert_{2}^{2}$ \n\n**M **\n\n $\\{ \\mu_{k} \\}$  $Q'$ \n\n $\\{1, 2, \\cdots, K \\}$  $K$ $I_{k} = \\{ i \\mid y_{i}=k \\}$ \n$$\nQ'(\\{ \\mu_{k} \\} \\mid \\{ \\mu_{k} \\}^{(t)}) = \\sum\\limits_{i=1}^{K} \\sum_{i \\in I_{k}} \\lVert x_{i} - \\mu_{k} \\rVert_{2}^{2}\n$$\n $\\sum\\limits_{i \\in I_{k}} \\lVert x_{i} - \\mu_{k} \\rVert_{2}^{2}$ \n\n K-means \n\n# \n\n GMM K-means  GMM  $\\Sigma = \\sigma I$  $\\sigma \\to 0$  [](http://icml.cc/2012/papers/291.pdf)  2.1 ","source":"_posts/kmeans-is-a-gmm.md","raw":"---\ntitle: K-means  GMM \ndate: 2017-10-23 23:23:56\ncategories: ML\ntags:\n     - K-means\n     - GMM\n     - EM Algorithm\ndescription:  GMM  K-means  EM \n---\n\n#  GMM \n\n K-means \n GMMGaussian Mixture Model K-means  EM  K-means  EM \n\n\n\n**** $x_{1}, x_{2}, \\cdots, x_{N} \\in \\mathbb{R}^{m}$  $K$  $K$  $\\mu_{1}, \\mu_{2}, \\cdots, \\mu_{K}$  $\\sum\\limits_{i=1}^{N} \\min\\limits_{1 \\le k \\le K} \\lVert x_{i} - \\mu_{k} \\rVert_{2}^{2}$ \n\n**** $x_{1}, x_{2}, \\cdots, x_{N} \\in \\mathbb{R}^{m}$  $K$ \n$$\np(x, z \\mid \\mu_{1}, \\mu_{2}, \\cdots, \\mu_{K}) \\propto \n\\left \\{\n\\begin{aligned}\n&\\exp \\left ( -\\lVert x-\\mu_{z} \\rVert_{2}^{2} \\right ) & \\qquad \\lVert x-\\mu_{z} \\rVert_{2} = \\min_{1 \\le k \\le K} \\lVert x-\\mu_{k} \\rVert_{2} \\\\\n&0 & \\qquad \\lVert x-\\mu_{z} \\rVert_{2} > \\min_{1 \\le k \\le K} \\lVert x-\\mu_{k} \\rVert_{2}\n\\end{aligned}\n\\right.\n$$\n $z \\in \\{1, 2, \\cdots, K\\}$ \n\n $K$  $K$ \n\n EM  K-means \n\n**E **\n\n1. \n   $$\n   p(z_{i} \\mid x_{i}, \\{\\mu_{k}\\}^{(t)}) \\propto \n   \\left \\{\n   \\begin{aligned}\n   &1 & \\qquad \\lVert x_{i}-\\mu_{z_{i}}^{(t)} \\rVert_{2} = \\min_{1 \\le k \\le K} \\lVert x_{i}-\\mu_{k}^{(t)} \\rVert_{2} \\\\\n   &0 & \\qquad \\lVert x_{i}-\\mu_{z_{i}}^{(t)} \\rVert_{2} > \\min_{1 \\le k \\le K} \\lVert x_{i}-\\mu_{k}^{(t)} \\rVert_{2}\n   \\end{aligned}\n   \\right.\n   $$\n   \n   $$\n   p(Z \\mid X, \\{ \\mu_{k} \\}^{(t)}) = \\prod_{i=1}^{N} p(z_{i} \\mid x_{i}, \\{ \\mu_{k} \\}^{(t)})\n   $$\n   \n\n    $y_{i}$ \n\n2. \n   $$\n   \\begin{aligned}\n   Q(\\{ \\mu_{k} \\} \\mid \\{ \\mu_{k} \\}^{(t)}) & = E_{Z \\mid X, \\{ \\mu_{k} \\}^{(t)}} [\\log p(X, Z \\mid \\{ \\mu_{k} \\})] \\\\\n   & = \\sum_{i=1}^{N} E_{z_{i} \\mid x_{i}, \\{ \\mu_{k} \\}^{(t)}} [\\log p(x_{i}, z_{i} \\mid \\{ \\mu_{k} \\})] \\\\\n   & = \\sum_{i=1}^{N} \\log p(x_{i}, z_{i}=y_{i} \\mid \\{ \\mu_{k} \\}) \\\\\n   & = \\mathrm{const} - \\sum_{i=1}^{N} \\lVert x_{i} - \\mu_{y_{i}} \\rVert_{2}^{2}\n   \\end{aligned}\n   $$\n    $Q'(\\{ \\mu_{k} \\} \\mid \\{ \\mu_{k} \\}^{(t)}) = \\sum\\limits_{i=1}^{N} \\lVert x_{i} - \\mu_{y_{i}} \\rVert_{2}^{2}$ \n\n**M **\n\n $\\{ \\mu_{k} \\}$  $Q'$ \n\n $\\{1, 2, \\cdots, K \\}$  $K$ $I_{k} = \\{ i \\mid y_{i}=k \\}$ \n$$\nQ'(\\{ \\mu_{k} \\} \\mid \\{ \\mu_{k} \\}^{(t)}) = \\sum\\limits_{i=1}^{K} \\sum_{i \\in I_{k}} \\lVert x_{i} - \\mu_{k} \\rVert_{2}^{2}\n$$\n $\\sum\\limits_{i \\in I_{k}} \\lVert x_{i} - \\mu_{k} \\rVert_{2}^{2}$ \n\n K-means \n\n# \n\n GMM K-means  GMM  $\\Sigma = \\sigma I$  $\\sigma \\to 0$  [](http://icml.cc/2012/papers/291.pdf)  2.1 ","slug":"kmeans-is-a-gmm","published":1,"updated":"2021-07-19T13:03:31.697Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrancn9t000zg0vwtlnykhka","content":"<h1 id=\"-GMM-\"><a href=\"#-GMM-\" class=\"headerlink\" title=\" GMM \"></a> GMM </h1><p> K-means <br> GMMGaussian Mixture Model K-means  EM  K-means  EM </p>\n<p></p>\n<p><strong></strong> $x_{1}, x_{2}, \\cdots, x_{N} \\in \\mathbb{R}^{m}$  $K$  $K$  $\\mu_{1}, \\mu_{2}, \\cdots, \\mu_{K}$  $\\sum\\limits_{i=1}^{N} \\min\\limits_{1 \\le k \\le K} \\lVert x_{i} - \\mu_{k} \\rVert_{2}^{2}$ </p>\n<p><strong></strong> $x_{1}, x_{2}, \\cdots, x_{N} \\in \\mathbb{R}^{m}$  $K$ <br>$$<br>p(x, z \\mid \\mu_{1}, \\mu_{2}, \\cdots, \\mu_{K}) \\propto<br>\\left {<br>\\begin{aligned}<br>&amp;\\exp \\left ( -\\lVert x-\\mu_{z} \\rVert_{2}^{2} \\right ) &amp; \\qquad \\lVert x-\\mu_{z} \\rVert_{2} = \\min_{1 \\le k \\le K} \\lVert x-\\mu_{k} \\rVert_{2} \\<br>&amp;0 &amp; \\qquad \\lVert x-\\mu_{z} \\rVert_{2} &gt; \\min_{1 \\le k \\le K} \\lVert x-\\mu_{k} \\rVert_{2}<br>\\end{aligned}<br>\\right.<br>$$<br> $z \\in {1, 2, \\cdots, K}$ </p>\n<p> $K$  $K$ </p>\n<p> EM  K-means </p>\n<p><strong>E </strong></p>\n<ol>\n<li><p><br>$$<br>p(z_{i} \\mid x_{i}, {\\mu_{k}}^{(t)}) \\propto<br>\\left {<br>\\begin{aligned}<br>&amp;1 &amp; \\qquad \\lVert x_{i}-\\mu_{z_{i}}^{(t)} \\rVert_{2} = \\min_{1 \\le k \\le K} \\lVert x_{i}-\\mu_{k}^{(t)} \\rVert_{2} \\<br>&amp;0 &amp; \\qquad \\lVert x_{i}-\\mu_{z_{i}}^{(t)} \\rVert_{2} &gt; \\min_{1 \\le k \\le K} \\lVert x_{i}-\\mu_{k}^{(t)} \\rVert_{2}<br>\\end{aligned}<br>\\right.<br>$$<br><br>$$<br>p(Z \\mid X, { \\mu_{k} }^{(t)}) = \\prod_{i=1}^{N} p(z_{i} \\mid x_{i}, { \\mu_{k} }^{(t)})<br>$$<br></p>\n<p> $y_{i}$ </p>\n</li>\n<li><p><br>$$<br>\\begin{aligned}<br>Q({ \\mu_{k} } \\mid { \\mu_{k} }^{(t)}) &amp; = E_{Z \\mid X, { \\mu_{k} }^{(t)}} [\\log p(X, Z \\mid { \\mu_{k} })] \\<br>&amp; = \\sum_{i=1}^{N} E_{z_{i} \\mid x_{i}, { \\mu_{k} }^{(t)}} [\\log p(x_{i}, z_{i} \\mid { \\mu_{k} })] \\<br>&amp; = \\sum_{i=1}^{N} \\log p(x_{i}, z_{i}=y_{i} \\mid { \\mu_{k} }) \\<br>&amp; = \\mathrm{const} - \\sum_{i=1}^{N} \\lVert x_{i} - \\mu_{y_{i}} \\rVert_{2}^{2}<br>\\end{aligned}<br>$$<br> $Q({ \\mu_{k} } \\mid { \\mu_{k} }^{(t)}) = \\sum\\limits_{i=1}^{N} \\lVert x_{i} - \\mu_{y_{i}} \\rVert_{2}^{2}$ </p>\n</li>\n</ol>\n<p><strong>M </strong></p>\n<p> ${ \\mu_{k} }$  $Q$ </p>\n<p> ${1, 2, \\cdots, K }$  $K$ $I_{k} = { i \\mid y_{i}=k }$ <br>$$<br>Q({ \\mu_{k} } \\mid { \\mu_{k} }^{(t)}) = \\sum\\limits_{i=1}^{K} \\sum_{i \\in I_{k}} \\lVert x_{i} - \\mu_{k} \\rVert_{2}^{2}<br>$$<br> $\\sum\\limits_{i \\in I_{k}} \\lVert x_{i} - \\mu_{k} \\rVert_{2}^{2}$ </p>\n<p> K-means </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> GMM K-means  GMM  $\\Sigma = \\sigma I$  $\\sigma \\to 0$  <a href=\"http://icml.cc/2012/papers/291.pdf\" target=\"_blank\" rel=\"noopener\"></a>  2.1 </p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"-GMM-\"><a href=\"#-GMM-\" class=\"headerlink\" title=\" GMM \"></a> GMM </h1><p> K-means <br> GMMGaussian Mixture Model K-means  EM  K-means  EM </p>\n<p></p>\n<p><strong></strong> $x_{1}, x_{2}, \\cdots, x_{N} \\in \\mathbb{R}^{m}$  $K$  $K$  $\\mu_{1}, \\mu_{2}, \\cdots, \\mu_{K}$  $\\sum\\limits_{i=1}^{N} \\min\\limits_{1 \\le k \\le K} \\lVert x_{i} - \\mu_{k} \\rVert_{2}^{2}$ </p>\n<p><strong></strong> $x_{1}, x_{2}, \\cdots, x_{N} \\in \\mathbb{R}^{m}$  $K$ <br>$$<br>p(x, z \\mid \\mu_{1}, \\mu_{2}, \\cdots, \\mu_{K}) \\propto<br>\\left {<br>\\begin{aligned}<br>&amp;\\exp \\left ( -\\lVert x-\\mu_{z} \\rVert_{2}^{2} \\right ) &amp; \\qquad \\lVert x-\\mu_{z} \\rVert_{2} = \\min_{1 \\le k \\le K} \\lVert x-\\mu_{k} \\rVert_{2} \\<br>&amp;0 &amp; \\qquad \\lVert x-\\mu_{z} \\rVert_{2} &gt; \\min_{1 \\le k \\le K} \\lVert x-\\mu_{k} \\rVert_{2}<br>\\end{aligned}<br>\\right.<br>$$<br> $z \\in {1, 2, \\cdots, K}$ </p>\n<p> $K$  $K$ </p>\n<p> EM  K-means </p>\n<p><strong>E </strong></p>\n<ol>\n<li><p><br>$$<br>p(z_{i} \\mid x_{i}, {\\mu_{k}}^{(t)}) \\propto<br>\\left {<br>\\begin{aligned}<br>&amp;1 &amp; \\qquad \\lVert x_{i}-\\mu_{z_{i}}^{(t)} \\rVert_{2} = \\min_{1 \\le k \\le K} \\lVert x_{i}-\\mu_{k}^{(t)} \\rVert_{2} \\<br>&amp;0 &amp; \\qquad \\lVert x_{i}-\\mu_{z_{i}}^{(t)} \\rVert_{2} &gt; \\min_{1 \\le k \\le K} \\lVert x_{i}-\\mu_{k}^{(t)} \\rVert_{2}<br>\\end{aligned}<br>\\right.<br>$$<br><br>$$<br>p(Z \\mid X, { \\mu_{k} }^{(t)}) = \\prod_{i=1}^{N} p(z_{i} \\mid x_{i}, { \\mu_{k} }^{(t)})<br>$$<br></p>\n<p> $y_{i}$ </p>\n</li>\n<li><p><br>$$<br>\\begin{aligned}<br>Q({ \\mu_{k} } \\mid { \\mu_{k} }^{(t)}) &amp; = E_{Z \\mid X, { \\mu_{k} }^{(t)}} [\\log p(X, Z \\mid { \\mu_{k} })] \\<br>&amp; = \\sum_{i=1}^{N} E_{z_{i} \\mid x_{i}, { \\mu_{k} }^{(t)}} [\\log p(x_{i}, z_{i} \\mid { \\mu_{k} })] \\<br>&amp; = \\sum_{i=1}^{N} \\log p(x_{i}, z_{i}=y_{i} \\mid { \\mu_{k} }) \\<br>&amp; = \\mathrm{const} - \\sum_{i=1}^{N} \\lVert x_{i} - \\mu_{y_{i}} \\rVert_{2}^{2}<br>\\end{aligned}<br>$$<br> $Q({ \\mu_{k} } \\mid { \\mu_{k} }^{(t)}) = \\sum\\limits_{i=1}^{N} \\lVert x_{i} - \\mu_{y_{i}} \\rVert_{2}^{2}$ </p>\n</li>\n</ol>\n<p><strong>M </strong></p>\n<p> ${ \\mu_{k} }$  $Q$ </p>\n<p> ${1, 2, \\cdots, K }$  $K$ $I_{k} = { i \\mid y_{i}=k }$ <br>$$<br>Q({ \\mu_{k} } \\mid { \\mu_{k} }^{(t)}) = \\sum\\limits_{i=1}^{K} \\sum_{i \\in I_{k}} \\lVert x_{i} - \\mu_{k} \\rVert_{2}^{2}<br>$$<br> $\\sum\\limits_{i \\in I_{k}} \\lVert x_{i} - \\mu_{k} \\rVert_{2}^{2}$ </p>\n<p> K-means </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> GMM K-means  GMM  $\\Sigma = \\sigma I$  $\\sigma \\to 0$  <a href=\"http://icml.cc/2012/papers/291.pdf\" target=\"_blank\" rel=\"noopener\"></a>  2.1 </p>\n"},{"title":" ","date":"2017-09-09T01:42:10.000Z","description":"","_content":"\n# matrix completion\n\n Netflix  100  10%  2009  9  BellKors Pragmatic \n\n Netflix \n rate \n\n\nlow rank\n1cliques2genres\ncollaborative filtering\n\n\n\n$$\n\\begin{aligned}\n&\\mathrm{minimize} \\quad  \\mathrm{rank}(Z) \\\\\n&\\text{subject to} \\quad  \\sum_{(i,j):\\text{Observed}} (Z_{ij} - X_{ij})^{2} \\le \\delta\n\\end{aligned} \\\\\n\\text{Impute missing } X_{ij} \\text{ with } Z_{ij}\n$$\n NP-hard \n\n\n\n L1  L0 \n\n\n# \n\n## \n\n $X$ \n$$\n\\lVert X \\rVert _{\\ast} = \\mathrm{tr}\\left( \\sqrt{X^{\\mathsf{T}}X} \\right)\n$$\n $X$  $X=U \\Sigma V^{\\mathsf{T}}$ \n$$\n\\begin{aligned}\n\\mathrm{tr}\\left( \\sqrt{X^{\\mathsf{T}}X} \\right) & =  \\mathrm{tr} \\left( \\sqrt{(U \\Sigma V^{\\mathsf{T}})^{\\mathsf{T}}U \\Sigma V^{\\mathsf{T}}} \\right) \\\\\n& = \\mathrm{tr} \\left( \\sqrt{V \\Sigma^{\\mathsf{T}} U^{\\mathsf{T}} U \\Sigma V^{\\mathsf{T}}} \\right) \\\\\n& = \\mathrm{tr} \\left( \\sqrt{V \\Sigma^{2} V^{\\mathsf{T}}} \\right)  \\quad \\left( \\Sigma^{\\mathsf{T}} = \\Sigma \\right) \\\\\n& = \\mathrm{tr} \\left( \\sqrt{V^{\\mathsf{T}} V \\Sigma^{2}} \\right) \\\\\n& = \\mathrm{tr} (\\Sigma)\n\\end{aligned}\n$$\n\n## \n\n $ f_x (A) = \\lVert Ax \\rVert _{p} \\quad (p \\ge 1) $  $f_x$  $\\lVert A \\rVert _{p} = \\sup\\limits_{\\Vert x \\rVert _{p}=1} f_x (A)$ $\\lVert A \\rVert_{2}$ \n $\\lVert A \\rVert_{\\ast}$  $\\lVert A \\rVert_{2}$  $\\lVert A \\rVert_{\\ast}$ $\\lVert A \\rVert_{\\ast}=\\sup \\limits_{\\lVert X \\rVert_{2}=1} \\mathrm{tr} \\left( A^{\\mathsf{T}}X \\right)$\n\n## \n\n S.V.D \n$$\n\\frac{\\partial \\lVert X \\rVert_{\\ast}}{\\partial X} = \\frac{\\partial \\mathrm{tr} (\\Sigma)}{\\partial X} = \\frac{\\mathrm{tr} (\\partial \\Sigma)}{\\partial X}\n$$\n $\\partial \\Sigma$ \n $X=U \\Sigma V^{\\mathsf{T}}$ \n$$\n\\begin{aligned}\n& & \\partial X & = (\\partial U )\\Sigma V^{\\mathsf{T}} + U (\\partial \\Sigma) V^{\\mathsf{T}} + U \\Sigma (\\partial V^{\\mathsf{T}}) \\\\\n&\\Rightarrow \\, & \\partial \\Sigma & = U^{\\mathsf{T}} (\\partial X) V - U^{\\mathsf{T}} (\\partial U) \\Sigma - \\Sigma (\\partial V^{\\mathsf{T}}) V \\\\\n& & & = U^{\\mathsf{T}} (\\partial X) V \\qquad (- U^{\\mathsf{T}} (\\partial U) \\Sigma - \\Sigma (\\partial V^{\\mathsf{T}}) V = 0)\n\\end{aligned}\n$$\n\n$$\n\\frac{\\partial \\lVert X \\rVert_{\\ast}}{\\partial X} = \\frac{\\mathrm{tr} (\\partial \\Sigma)}{\\partial X} = \\frac{\\mathrm{tr} (U^{\\mathsf{T}} (\\partial X)V)}{\\partial X} = \\frac{\\mathrm{tr} (V U^{\\mathsf{T}} (\\partial X))}{\\partial X} = (V U^{\\mathsf{T}})^{\\mathsf{T}} = U V^{\\mathsf{T}}\n$$\n\n# \n\n\n\n$$\n\\begin{aligned}\n&\\mathrm{minimize} \\quad  \\lVert Z \\rVert_{\\ast} \\\\\n&\\text{subject to} \\quad  \\sum_{(i,j):\\text{Observed}} (Z_{ij} - X_{ij})^{2} \\le \\delta\n\\end{aligned} \\\\\n\\text{Impute missing } X_{ij} \\text{ with } Z_{ij}\n$$\n","source":"_posts/norm-regularization-02.md","raw":"---\ntitle:  \ndate: 2017-09-09 09:42:10\ncategories: ML\ntags:\n      - Norm regularization\n      - Convex optimization\n      - Matrix theory\ndescription: \n---\n\n# matrix completion\n\n Netflix  100  10%  2009  9  BellKors Pragmatic \n\n Netflix \n rate \n\n\nlow rank\n1cliques2genres\ncollaborative filtering\n\n\n\n$$\n\\begin{aligned}\n&\\mathrm{minimize} \\quad  \\mathrm{rank}(Z) \\\\\n&\\text{subject to} \\quad  \\sum_{(i,j):\\text{Observed}} (Z_{ij} - X_{ij})^{2} \\le \\delta\n\\end{aligned} \\\\\n\\text{Impute missing } X_{ij} \\text{ with } Z_{ij}\n$$\n NP-hard \n\n\n\n L1  L0 \n\n\n# \n\n## \n\n $X$ \n$$\n\\lVert X \\rVert _{\\ast} = \\mathrm{tr}\\left( \\sqrt{X^{\\mathsf{T}}X} \\right)\n$$\n $X$  $X=U \\Sigma V^{\\mathsf{T}}$ \n$$\n\\begin{aligned}\n\\mathrm{tr}\\left( \\sqrt{X^{\\mathsf{T}}X} \\right) & =  \\mathrm{tr} \\left( \\sqrt{(U \\Sigma V^{\\mathsf{T}})^{\\mathsf{T}}U \\Sigma V^{\\mathsf{T}}} \\right) \\\\\n& = \\mathrm{tr} \\left( \\sqrt{V \\Sigma^{\\mathsf{T}} U^{\\mathsf{T}} U \\Sigma V^{\\mathsf{T}}} \\right) \\\\\n& = \\mathrm{tr} \\left( \\sqrt{V \\Sigma^{2} V^{\\mathsf{T}}} \\right)  \\quad \\left( \\Sigma^{\\mathsf{T}} = \\Sigma \\right) \\\\\n& = \\mathrm{tr} \\left( \\sqrt{V^{\\mathsf{T}} V \\Sigma^{2}} \\right) \\\\\n& = \\mathrm{tr} (\\Sigma)\n\\end{aligned}\n$$\n\n## \n\n $ f_x (A) = \\lVert Ax \\rVert _{p} \\quad (p \\ge 1) $  $f_x$  $\\lVert A \\rVert _{p} = \\sup\\limits_{\\Vert x \\rVert _{p}=1} f_x (A)$ $\\lVert A \\rVert_{2}$ \n $\\lVert A \\rVert_{\\ast}$  $\\lVert A \\rVert_{2}$  $\\lVert A \\rVert_{\\ast}$ $\\lVert A \\rVert_{\\ast}=\\sup \\limits_{\\lVert X \\rVert_{2}=1} \\mathrm{tr} \\left( A^{\\mathsf{T}}X \\right)$\n\n## \n\n S.V.D \n$$\n\\frac{\\partial \\lVert X \\rVert_{\\ast}}{\\partial X} = \\frac{\\partial \\mathrm{tr} (\\Sigma)}{\\partial X} = \\frac{\\mathrm{tr} (\\partial \\Sigma)}{\\partial X}\n$$\n $\\partial \\Sigma$ \n $X=U \\Sigma V^{\\mathsf{T}}$ \n$$\n\\begin{aligned}\n& & \\partial X & = (\\partial U )\\Sigma V^{\\mathsf{T}} + U (\\partial \\Sigma) V^{\\mathsf{T}} + U \\Sigma (\\partial V^{\\mathsf{T}}) \\\\\n&\\Rightarrow \\, & \\partial \\Sigma & = U^{\\mathsf{T}} (\\partial X) V - U^{\\mathsf{T}} (\\partial U) \\Sigma - \\Sigma (\\partial V^{\\mathsf{T}}) V \\\\\n& & & = U^{\\mathsf{T}} (\\partial X) V \\qquad (- U^{\\mathsf{T}} (\\partial U) \\Sigma - \\Sigma (\\partial V^{\\mathsf{T}}) V = 0)\n\\end{aligned}\n$$\n\n$$\n\\frac{\\partial \\lVert X \\rVert_{\\ast}}{\\partial X} = \\frac{\\mathrm{tr} (\\partial \\Sigma)}{\\partial X} = \\frac{\\mathrm{tr} (U^{\\mathsf{T}} (\\partial X)V)}{\\partial X} = \\frac{\\mathrm{tr} (V U^{\\mathsf{T}} (\\partial X))}{\\partial X} = (V U^{\\mathsf{T}})^{\\mathsf{T}} = U V^{\\mathsf{T}}\n$$\n\n# \n\n\n\n$$\n\\begin{aligned}\n&\\mathrm{minimize} \\quad  \\lVert Z \\rVert_{\\ast} \\\\\n&\\text{subject to} \\quad  \\sum_{(i,j):\\text{Observed}} (Z_{ij} - X_{ij})^{2} \\le \\delta\n\\end{aligned} \\\\\n\\text{Impute missing } X_{ij} \\text{ with } Z_{ij}\n$$\n","slug":"norm-regularization-02","published":1,"updated":"2021-07-19T13:03:31.697Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrancn9u0012g0vw5ff1wqyh","content":"<h1 id=\"matrix-completion\"><a href=\"#matrix-completion\" class=\"headerlink\" title=\"matrix completion\"></a>matrix completion</h1><p> Netflix  100  10%  2009  9  BellKors Pragmatic </p>\n<p> Netflix <br> rate </p>\n<p><br>low rank<br>1cliques2genres<br>collaborative filtering<br></p>\n<p><br>$$<br>\\begin{aligned}<br>&amp;\\mathrm{minimize} \\quad  \\mathrm{rank}(Z) \\<br>&amp;\\text{subject to} \\quad  \\sum_{(i,j):\\text{Observed}} (Z_{ij} - X_{ij})^{2} \\le \\delta<br>\\end{aligned} \\<br>\\text{Impute missing } X_{ij} \\text{ with } Z_{ij}<br>$$<br> NP-hard </p>\n<p><br><br> L1  L0 <br></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> $X$ <br>$$<br>\\lVert X \\rVert _{\\ast} = \\mathrm{tr}\\left( \\sqrt{X^{\\mathsf{T}}X} \\right)<br>$$<br> $X$  $X=U \\Sigma V^{\\mathsf{T}}$ <br>$$<br>\\begin{aligned}<br>\\mathrm{tr}\\left( \\sqrt{X^{\\mathsf{T}}X} \\right) &amp; =  \\mathrm{tr} \\left( \\sqrt{(U \\Sigma V^{\\mathsf{T}})^{\\mathsf{T}}U \\Sigma V^{\\mathsf{T}}} \\right) \\<br>&amp; = \\mathrm{tr} \\left( \\sqrt{V \\Sigma^{\\mathsf{T}} U^{\\mathsf{T}} U \\Sigma V^{\\mathsf{T}}} \\right) \\<br>&amp; = \\mathrm{tr} \\left( \\sqrt{V \\Sigma^{2} V^{\\mathsf{T}}} \\right)  \\quad \\left( \\Sigma^{\\mathsf{T}} = \\Sigma \\right) \\<br>&amp; = \\mathrm{tr} \\left( \\sqrt{V^{\\mathsf{T}} V \\Sigma^{2}} \\right) \\<br>&amp; = \\mathrm{tr} (\\Sigma)<br>\\end{aligned}<br>$$</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> $ f_x (A) = \\lVert Ax \\rVert _{p} \\quad (p \\ge 1) $  $f_x$  $\\lVert A \\rVert <em>{p} = \\sup\\limits</em>{\\Vert x \\rVert _{p}=1} f_x (A)$ $\\lVert A \\rVert_{2}$ <br> $\\lVert A \\rVert_{\\ast}$  $\\lVert A \\rVert_{2}$  $\\lVert A \\rVert_{\\ast}$ $\\lVert A \\rVert_{\\ast}=\\sup \\limits_{\\lVert X \\rVert_{2}=1} \\mathrm{tr} \\left( A^{\\mathsf{T}}X \\right)$</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> S.V.D <br>$$<br>\\frac{\\partial \\lVert X \\rVert_{\\ast}}{\\partial X} = \\frac{\\partial \\mathrm{tr} (\\Sigma)}{\\partial X} = \\frac{\\mathrm{tr} (\\partial \\Sigma)}{\\partial X}<br>$$<br> $\\partial \\Sigma$ <br> $X=U \\Sigma V^{\\mathsf{T}}$ <br>$$<br>\\begin{aligned}<br>&amp; &amp; \\partial X &amp; = (\\partial U )\\Sigma V^{\\mathsf{T}} + U (\\partial \\Sigma) V^{\\mathsf{T}} + U \\Sigma (\\partial V^{\\mathsf{T}}) \\<br>&amp;\\Rightarrow \\, &amp; \\partial \\Sigma &amp; = U^{\\mathsf{T}} (\\partial X) V - U^{\\mathsf{T}} (\\partial U) \\Sigma - \\Sigma (\\partial V^{\\mathsf{T}}) V \\<br>&amp; &amp; &amp; = U^{\\mathsf{T}} (\\partial X) V \\qquad (- U^{\\mathsf{T}} (\\partial U) \\Sigma - \\Sigma (\\partial V^{\\mathsf{T}}) V = 0)<br>\\end{aligned}<br>$$<br><br>$$<br>\\frac{\\partial \\lVert X \\rVert_{\\ast}}{\\partial X} = \\frac{\\mathrm{tr} (\\partial \\Sigma)}{\\partial X} = \\frac{\\mathrm{tr} (U^{\\mathsf{T}} (\\partial X)V)}{\\partial X} = \\frac{\\mathrm{tr} (V U^{\\mathsf{T}} (\\partial X))}{\\partial X} = (V U^{\\mathsf{T}})^{\\mathsf{T}} = U V^{\\mathsf{T}}<br>$$</p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p><br><br>$$<br>\\begin{aligned}<br>&amp;\\mathrm{minimize} \\quad  \\lVert Z \\rVert_{\\ast} \\<br>&amp;\\text{subject to} \\quad  \\sum_{(i,j):\\text{Observed}} (Z_{ij} - X_{ij})^{2} \\le \\delta<br>\\end{aligned} \\<br>\\text{Impute missing } X_{ij} \\text{ with } Z_{ij}<br>$$</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"matrix-completion\"><a href=\"#matrix-completion\" class=\"headerlink\" title=\"matrix completion\"></a>matrix completion</h1><p> Netflix  100  10%  2009  9  BellKors Pragmatic </p>\n<p> Netflix <br> rate </p>\n<p><br>low rank<br>1cliques2genres<br>collaborative filtering<br></p>\n<p><br>$$<br>\\begin{aligned}<br>&amp;\\mathrm{minimize} \\quad  \\mathrm{rank}(Z) \\<br>&amp;\\text{subject to} \\quad  \\sum_{(i,j):\\text{Observed}} (Z_{ij} - X_{ij})^{2} \\le \\delta<br>\\end{aligned} \\<br>\\text{Impute missing } X_{ij} \\text{ with } Z_{ij}<br>$$<br> NP-hard </p>\n<p><br><br> L1  L0 <br></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> $X$ <br>$$<br>\\lVert X \\rVert _{\\ast} = \\mathrm{tr}\\left( \\sqrt{X^{\\mathsf{T}}X} \\right)<br>$$<br> $X$  $X=U \\Sigma V^{\\mathsf{T}}$ <br>$$<br>\\begin{aligned}<br>\\mathrm{tr}\\left( \\sqrt{X^{\\mathsf{T}}X} \\right) &amp; =  \\mathrm{tr} \\left( \\sqrt{(U \\Sigma V^{\\mathsf{T}})^{\\mathsf{T}}U \\Sigma V^{\\mathsf{T}}} \\right) \\<br>&amp; = \\mathrm{tr} \\left( \\sqrt{V \\Sigma^{\\mathsf{T}} U^{\\mathsf{T}} U \\Sigma V^{\\mathsf{T}}} \\right) \\<br>&amp; = \\mathrm{tr} \\left( \\sqrt{V \\Sigma^{2} V^{\\mathsf{T}}} \\right)  \\quad \\left( \\Sigma^{\\mathsf{T}} = \\Sigma \\right) \\<br>&amp; = \\mathrm{tr} \\left( \\sqrt{V^{\\mathsf{T}} V \\Sigma^{2}} \\right) \\<br>&amp; = \\mathrm{tr} (\\Sigma)<br>\\end{aligned}<br>$$</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> $ f_x (A) = \\lVert Ax \\rVert _{p} \\quad (p \\ge 1) $  $f_x$  $\\lVert A \\rVert <em>{p} = \\sup\\limits</em>{\\Vert x \\rVert _{p}=1} f_x (A)$ $\\lVert A \\rVert_{2}$ <br> $\\lVert A \\rVert_{\\ast}$  $\\lVert A \\rVert_{2}$  $\\lVert A \\rVert_{\\ast}$ $\\lVert A \\rVert_{\\ast}=\\sup \\limits_{\\lVert X \\rVert_{2}=1} \\mathrm{tr} \\left( A^{\\mathsf{T}}X \\right)$</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> S.V.D <br>$$<br>\\frac{\\partial \\lVert X \\rVert_{\\ast}}{\\partial X} = \\frac{\\partial \\mathrm{tr} (\\Sigma)}{\\partial X} = \\frac{\\mathrm{tr} (\\partial \\Sigma)}{\\partial X}<br>$$<br> $\\partial \\Sigma$ <br> $X=U \\Sigma V^{\\mathsf{T}}$ <br>$$<br>\\begin{aligned}<br>&amp; &amp; \\partial X &amp; = (\\partial U )\\Sigma V^{\\mathsf{T}} + U (\\partial \\Sigma) V^{\\mathsf{T}} + U \\Sigma (\\partial V^{\\mathsf{T}}) \\<br>&amp;\\Rightarrow \\, &amp; \\partial \\Sigma &amp; = U^{\\mathsf{T}} (\\partial X) V - U^{\\mathsf{T}} (\\partial U) \\Sigma - \\Sigma (\\partial V^{\\mathsf{T}}) V \\<br>&amp; &amp; &amp; = U^{\\mathsf{T}} (\\partial X) V \\qquad (- U^{\\mathsf{T}} (\\partial U) \\Sigma - \\Sigma (\\partial V^{\\mathsf{T}}) V = 0)<br>\\end{aligned}<br>$$<br><br>$$<br>\\frac{\\partial \\lVert X \\rVert_{\\ast}}{\\partial X} = \\frac{\\mathrm{tr} (\\partial \\Sigma)}{\\partial X} = \\frac{\\mathrm{tr} (U^{\\mathsf{T}} (\\partial X)V)}{\\partial X} = \\frac{\\mathrm{tr} (V U^{\\mathsf{T}} (\\partial X))}{\\partial X} = (V U^{\\mathsf{T}})^{\\mathsf{T}} = U V^{\\mathsf{T}}<br>$$</p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p><br><br>$$<br>\\begin{aligned}<br>&amp;\\mathrm{minimize} \\quad  \\lVert Z \\rVert_{\\ast} \\<br>&amp;\\text{subject to} \\quad  \\sum_{(i,j):\\text{Observed}} (Z_{ij} - X_{ij})^{2} \\le \\delta<br>\\end{aligned} \\<br>\\text{Impute missing } X_{ij} \\text{ with } Z_{ij}<br>$$</p>\n"},{"title":"","date":"2017-09-13T03:14:14.000Z","description":"","_content":"\n#  A $\\kappa({A})$\n\n\n\n $AX = b$  $x$ \n$$\n\\begin{aligned}\n& \\quad (A+\\Delta A)(x + \\Delta x) = b+\\Delta b \\\\\n& \\Rightarrow \\Delta x = A^{-1}[\\Delta b - (\\Delta A) x - (\\Delta A)(\\Delta x)] \\\\\n& \\Rightarrow \\lVert \\Delta x \\rVert \\le \\lVert A^{-1} \\rVert (\\lVert \\Delta b \\rVert + \\lVert \\Delta A \\rVert \\lVert x \\rVert + \\lVert \\Delta A\\rVert \\lVert \\Delta x\\rVert ) \\\\\n& \\Rightarrow (1-\\lVert A^{-1} \\rVert \\lVert \\Delta A \\rVert) \\lVert \\Delta x \\rVert \\le \\lVert A^{-1} \\rVert (\\lVert \\Delta b \\rVert + \\lVert \\Delta A \\rVert \\lVert x \\rVert) \\\\\n& \\Rightarrow \\lVert \\Delta x \\rVert \\le \\frac{\\lVert A^{-1} \\rVert}{1-\\lVert A^{-1} \\rVert \\lVert \\Delta A \\rVert} \\left( \\lVert \\Delta b \\rVert + \\lVert \\Delta A \\rVert \\lVert x \\rVert \\right) \\qquad (\\because \\, \\lVert A^{-1} \\rVert \\lVert \\Delta A \\rVert \\le 1) \\\\\n& \\Rightarrow \\frac {\\lVert \\Delta x \\rVert}{\\lVert x \\rVert} \\le \\frac{\\lVert A^{-1} \\rVert \\lVert A \\rVert}{1-\\lVert A^{-1} \\rVert \\lVert \\Delta A \\rVert} \\left( \\frac{\\lVert \\Delta b \\rVert}{\\lVert b \\rVert} + \\frac{\\lVert \\Delta A \\rVert}{\\lVert A \\rVert} \\right) \\qquad (\\because \\, \\lVert b \\rVert \\le \\lVert A \\rVert \\lVert x \\rVert)\n\\end{aligned}\n$$\n $\\Delta A = 0$ \n$$\n\\frac{\\lVert \\Delta x \\rVert}{\\lVert x \\rVert} \\le \\kappa(A) \\frac{\\lVert \\Delta b \\rVert}{\\lVert b \\rVert}\n$$\n $\\Delta b = 0$ \n$$\n\\frac{\\lVert \\Delta x \\rVert}{\\lVert x \\rVert} \\le \\kappa(A) \\frac{\\lVert \\Delta A \\rVert}{\\lVert A \\rVert}(1+o(\\lVert A^{-1} \\rVert))\n$$\n 2-\n$$\n\\kappa_{2}(A) = \\lVert A^{-1} \\rVert _{2} \\lVert A \\rVert _{2} = \\sqrt{\\frac{\\lambda _{max}}{\\lambda _{min}}}\n$$\n$\\lambda_{max}$  $\\lambda_{min}$  $A^{\\mathsf{H}}A$ \n\n\n","source":"_posts/norm-regularization-appendix.md","raw":"---\ntitle: \ndate: 2017-09-13 11:14:14\ncategories: ML\ntags:\n      - Norm regularization\n      - Matrix theory\ndescription: \n---\n\n#  A $\\kappa({A})$\n\n\n\n $AX = b$  $x$ \n$$\n\\begin{aligned}\n& \\quad (A+\\Delta A)(x + \\Delta x) = b+\\Delta b \\\\\n& \\Rightarrow \\Delta x = A^{-1}[\\Delta b - (\\Delta A) x - (\\Delta A)(\\Delta x)] \\\\\n& \\Rightarrow \\lVert \\Delta x \\rVert \\le \\lVert A^{-1} \\rVert (\\lVert \\Delta b \\rVert + \\lVert \\Delta A \\rVert \\lVert x \\rVert + \\lVert \\Delta A\\rVert \\lVert \\Delta x\\rVert ) \\\\\n& \\Rightarrow (1-\\lVert A^{-1} \\rVert \\lVert \\Delta A \\rVert) \\lVert \\Delta x \\rVert \\le \\lVert A^{-1} \\rVert (\\lVert \\Delta b \\rVert + \\lVert \\Delta A \\rVert \\lVert x \\rVert) \\\\\n& \\Rightarrow \\lVert \\Delta x \\rVert \\le \\frac{\\lVert A^{-1} \\rVert}{1-\\lVert A^{-1} \\rVert \\lVert \\Delta A \\rVert} \\left( \\lVert \\Delta b \\rVert + \\lVert \\Delta A \\rVert \\lVert x \\rVert \\right) \\qquad (\\because \\, \\lVert A^{-1} \\rVert \\lVert \\Delta A \\rVert \\le 1) \\\\\n& \\Rightarrow \\frac {\\lVert \\Delta x \\rVert}{\\lVert x \\rVert} \\le \\frac{\\lVert A^{-1} \\rVert \\lVert A \\rVert}{1-\\lVert A^{-1} \\rVert \\lVert \\Delta A \\rVert} \\left( \\frac{\\lVert \\Delta b \\rVert}{\\lVert b \\rVert} + \\frac{\\lVert \\Delta A \\rVert}{\\lVert A \\rVert} \\right) \\qquad (\\because \\, \\lVert b \\rVert \\le \\lVert A \\rVert \\lVert x \\rVert)\n\\end{aligned}\n$$\n $\\Delta A = 0$ \n$$\n\\frac{\\lVert \\Delta x \\rVert}{\\lVert x \\rVert} \\le \\kappa(A) \\frac{\\lVert \\Delta b \\rVert}{\\lVert b \\rVert}\n$$\n $\\Delta b = 0$ \n$$\n\\frac{\\lVert \\Delta x \\rVert}{\\lVert x \\rVert} \\le \\kappa(A) \\frac{\\lVert \\Delta A \\rVert}{\\lVert A \\rVert}(1+o(\\lVert A^{-1} \\rVert))\n$$\n 2-\n$$\n\\kappa_{2}(A) = \\lVert A^{-1} \\rVert _{2} \\lVert A \\rVert _{2} = \\sqrt{\\frac{\\lambda _{max}}{\\lambda _{min}}}\n$$\n$\\lambda_{max}$  $\\lambda_{min}$  $A^{\\mathsf{H}}A$ \n\n\n","slug":"norm-regularization-appendix","published":1,"updated":"2021-07-19T13:03:31.697Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrancn9v0016g0vwzeh0hl1v","content":"<h1 id=\"-A-kappa-A\"><a href=\"#-A-kappa-A\" class=\"headerlink\" title=\" A $\\kappa({A})$\"></a> A $\\kappa({A})$</h1><p></p>\n<p> $AX = b$  $x$ <br>$$<br>\\begin{aligned}<br>&amp; \\quad (A+\\Delta A)(x + \\Delta x) = b+\\Delta b \\<br>&amp; \\Rightarrow \\Delta x = A^{-1}[\\Delta b - (\\Delta A) x - (\\Delta A)(\\Delta x)] \\<br>&amp; \\Rightarrow \\lVert \\Delta x \\rVert \\le \\lVert A^{-1} \\rVert (\\lVert \\Delta b \\rVert + \\lVert \\Delta A \\rVert \\lVert x \\rVert + \\lVert \\Delta A\\rVert \\lVert \\Delta x\\rVert ) \\<br>&amp; \\Rightarrow (1-\\lVert A^{-1} \\rVert \\lVert \\Delta A \\rVert) \\lVert \\Delta x \\rVert \\le \\lVert A^{-1} \\rVert (\\lVert \\Delta b \\rVert + \\lVert \\Delta A \\rVert \\lVert x \\rVert) \\<br>&amp; \\Rightarrow \\lVert \\Delta x \\rVert \\le \\frac{\\lVert A^{-1} \\rVert}{1-\\lVert A^{-1} \\rVert \\lVert \\Delta A \\rVert} \\left( \\lVert \\Delta b \\rVert + \\lVert \\Delta A \\rVert \\lVert x \\rVert \\right) \\qquad (\\because \\, \\lVert A^{-1} \\rVert \\lVert \\Delta A \\rVert \\le 1) \\<br>&amp; \\Rightarrow \\frac {\\lVert \\Delta x \\rVert}{\\lVert x \\rVert} \\le \\frac{\\lVert A^{-1} \\rVert \\lVert A \\rVert}{1-\\lVert A^{-1} \\rVert \\lVert \\Delta A \\rVert} \\left( \\frac{\\lVert \\Delta b \\rVert}{\\lVert b \\rVert} + \\frac{\\lVert \\Delta A \\rVert}{\\lVert A \\rVert} \\right) \\qquad (\\because \\, \\lVert b \\rVert \\le \\lVert A \\rVert \\lVert x \\rVert)<br>\\end{aligned}<br>$$<br> $\\Delta A = 0$ <br>$$<br>\\frac{\\lVert \\Delta x \\rVert}{\\lVert x \\rVert} \\le \\kappa(A) \\frac{\\lVert \\Delta b \\rVert}{\\lVert b \\rVert}<br>$$<br> $\\Delta b = 0$ <br>$$<br>\\frac{\\lVert \\Delta x \\rVert}{\\lVert x \\rVert} \\le \\kappa(A) \\frac{\\lVert \\Delta A \\rVert}{\\lVert A \\rVert}(1+o(\\lVert A^{-1} \\rVert))<br>$$<br> 2-<br>$$<br>\\kappa_{2}(A) = \\lVert A^{-1} \\rVert _{2} \\lVert A \\rVert _{2} = \\sqrt{\\frac{\\lambda _{max}}{\\lambda <em>{min}}}<br>$$<br>$\\lambda</em>{max}$  $\\lambda_{min}$  $A^{\\mathsf{H}}A$ <br></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"-A-kappa-A\"><a href=\"#-A-kappa-A\" class=\"headerlink\" title=\" A $\\kappa({A})$\"></a> A $\\kappa({A})$</h1><p></p>\n<p> $AX = b$  $x$ <br>$$<br>\\begin{aligned}<br>&amp; \\quad (A+\\Delta A)(x + \\Delta x) = b+\\Delta b \\<br>&amp; \\Rightarrow \\Delta x = A^{-1}[\\Delta b - (\\Delta A) x - (\\Delta A)(\\Delta x)] \\<br>&amp; \\Rightarrow \\lVert \\Delta x \\rVert \\le \\lVert A^{-1} \\rVert (\\lVert \\Delta b \\rVert + \\lVert \\Delta A \\rVert \\lVert x \\rVert + \\lVert \\Delta A\\rVert \\lVert \\Delta x\\rVert ) \\<br>&amp; \\Rightarrow (1-\\lVert A^{-1} \\rVert \\lVert \\Delta A \\rVert) \\lVert \\Delta x \\rVert \\le \\lVert A^{-1} \\rVert (\\lVert \\Delta b \\rVert + \\lVert \\Delta A \\rVert \\lVert x \\rVert) \\<br>&amp; \\Rightarrow \\lVert \\Delta x \\rVert \\le \\frac{\\lVert A^{-1} \\rVert}{1-\\lVert A^{-1} \\rVert \\lVert \\Delta A \\rVert} \\left( \\lVert \\Delta b \\rVert + \\lVert \\Delta A \\rVert \\lVert x \\rVert \\right) \\qquad (\\because \\, \\lVert A^{-1} \\rVert \\lVert \\Delta A \\rVert \\le 1) \\<br>&amp; \\Rightarrow \\frac {\\lVert \\Delta x \\rVert}{\\lVert x \\rVert} \\le \\frac{\\lVert A^{-1} \\rVert \\lVert A \\rVert}{1-\\lVert A^{-1} \\rVert \\lVert \\Delta A \\rVert} \\left( \\frac{\\lVert \\Delta b \\rVert}{\\lVert b \\rVert} + \\frac{\\lVert \\Delta A \\rVert}{\\lVert A \\rVert} \\right) \\qquad (\\because \\, \\lVert b \\rVert \\le \\lVert A \\rVert \\lVert x \\rVert)<br>\\end{aligned}<br>$$<br> $\\Delta A = 0$ <br>$$<br>\\frac{\\lVert \\Delta x \\rVert}{\\lVert x \\rVert} \\le \\kappa(A) \\frac{\\lVert \\Delta b \\rVert}{\\lVert b \\rVert}<br>$$<br> $\\Delta b = 0$ <br>$$<br>\\frac{\\lVert \\Delta x \\rVert}{\\lVert x \\rVert} \\le \\kappa(A) \\frac{\\lVert \\Delta A \\rVert}{\\lVert A \\rVert}(1+o(\\lVert A^{-1} \\rVert))<br>$$<br> 2-<br>$$<br>\\kappa_{2}(A) = \\lVert A^{-1} \\rVert _{2} \\lVert A \\rVert _{2} = \\sqrt{\\frac{\\lambda _{max}}{\\lambda <em>{min}}}<br>$$<br>$\\lambda</em>{max}$  $\\lambda_{min}$  $A^{\\mathsf{H}}A$ <br></p>\n"},{"title":"","date":"2017-12-27T12:57:14.000Z","description":"","_content":"\n# \n\nYouTube StackOverflow  item  50000  95  5  95  item 95 50000  90  40000  92 \n\n# \n\n 100  85  100  85%  15%  item  85%  $\\alpha$  $\\alpha = 95\\%$  item \n\n\n\n- $S_{\\mathrm{max}}$ \n-  item $p$\n- $n$\n- $K = z_{1-\\frac{\\alpha}{2}}$\n\n\n$$\ns = p_{\\mathrm{min}} \\cdot S_{\\mathrm{max}}, \\, p_{\\mathrm{min}} = \\frac{p + \\frac{K^{2}}{2n} - K \\sqrt{\\frac{p(1 - p)}{n} + \\frac{K^{2}}{4n^{2}}}}{1 + \\frac{K^{2}}{n}}\n$$\n [](http://sakigami-yang.me/2017/10/12/statistical-formulars-for-programmers/) \n\n# \n\n item  $C$ \n\n\n\n- $C$\n- $M$\n-  item $n$\n-  item $s$\n\n\n$$\n\\hat{s} = \\frac{CM + ns}{C + n}\n$$\n\n# Python \n\n```python\ndef Wilson(p, n):\n    \"\"\"\n    \n    \"\"\"\n    p = float(p)\n    K = 1.96  # 95% confidence level\n    _K2_div_n = (K ** 2) / n\n    pmin = (p + _K2_div_n / 2.0 -\n            K * ((p * (1 - p) / n + _K2_div_n / n / 4.0) ** 0.5)) / (1 + _K2_div_n)\n    return pmin\n\n\ndef WilsonAvgP(n):\n    \"\"\"\n     Bayesian  M \n     0.01~1  100 \n    \n    \"\"\"\n    totalP = 0.0\n    totalN = 0\n    p = 0.01\n\n    while True:\n        totalP += Wilson(p, n, 1);\n        totalN += 1\n        p += 0.01\n        if p >= 1:\n            break\n\n    return totalP / totalN\n\n\ndef Bayesian(C, M, n, s):\n    \"\"\"\n    \n     C  M \n    \n    \"\"\"\n    return (C * M + n * s) / (n + C)\n```\n\n\n\n\n\n","source":"_posts/rating-model-considering-user-count.md","raw":"---\ntitle: \ndate: 2017-12-27 20:57:14\ncategories: ML\ntags:\n     - Rating\ndescription: \n---\n\n# \n\nYouTube StackOverflow  item  50000  95  5  95  item 95 50000  90  40000  92 \n\n# \n\n 100  85  100  85%  15%  item  85%  $\\alpha$  $\\alpha = 95\\%$  item \n\n\n\n- $S_{\\mathrm{max}}$ \n-  item $p$\n- $n$\n- $K = z_{1-\\frac{\\alpha}{2}}$\n\n\n$$\ns = p_{\\mathrm{min}} \\cdot S_{\\mathrm{max}}, \\, p_{\\mathrm{min}} = \\frac{p + \\frac{K^{2}}{2n} - K \\sqrt{\\frac{p(1 - p)}{n} + \\frac{K^{2}}{4n^{2}}}}{1 + \\frac{K^{2}}{n}}\n$$\n [](http://sakigami-yang.me/2017/10/12/statistical-formulars-for-programmers/) \n\n# \n\n item  $C$ \n\n\n\n- $C$\n- $M$\n-  item $n$\n-  item $s$\n\n\n$$\n\\hat{s} = \\frac{CM + ns}{C + n}\n$$\n\n# Python \n\n```python\ndef Wilson(p, n):\n    \"\"\"\n    \n    \"\"\"\n    p = float(p)\n    K = 1.96  # 95% confidence level\n    _K2_div_n = (K ** 2) / n\n    pmin = (p + _K2_div_n / 2.0 -\n            K * ((p * (1 - p) / n + _K2_div_n / n / 4.0) ** 0.5)) / (1 + _K2_div_n)\n    return pmin\n\n\ndef WilsonAvgP(n):\n    \"\"\"\n     Bayesian  M \n     0.01~1  100 \n    \n    \"\"\"\n    totalP = 0.0\n    totalN = 0\n    p = 0.01\n\n    while True:\n        totalP += Wilson(p, n, 1);\n        totalN += 1\n        p += 0.01\n        if p >= 1:\n            break\n\n    return totalP / totalN\n\n\ndef Bayesian(C, M, n, s):\n    \"\"\"\n    \n     C  M \n    \n    \"\"\"\n    return (C * M + n * s) / (n + C)\n```\n\n\n\n\n\n","slug":"rating-model-considering-user-count","published":1,"updated":"2021-07-19T13:03:31.698Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrancn9x0018g0vw5uzijjsg","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>YouTube StackOverflow  item  50000  95  5  95  item 95 50000  90  40000  92 </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> 100  85  100  85%  15%  item  85%  $\\alpha$  $\\alpha = 95\\%$  item </p>\n<p></p>\n<ul>\n<li>$S_{\\mathrm{max}}$ </li>\n<li> item $p$</li>\n<li>$n$</li>\n<li>$K = z_{1-\\frac{\\alpha}{2}}$</li>\n</ul>\n<p><br>$$<br>s = p_{\\mathrm{min}} \\cdot S_{\\mathrm{max}}, \\, p_{\\mathrm{min}} = \\frac{p + \\frac{K^{2}}{2n} - K \\sqrt{\\frac{p(1 - p)}{n} + \\frac{K^{2}}{4n^{2}}}}{1 + \\frac{K^{2}}{n}}<br>$$<br> <a href=\"http://sakigami-yang.me/2017/10/12/statistical-formulars-for-programmers/\"></a> </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> item  $C$ </p>\n<p></p>\n<ul>\n<li>$C$</li>\n<li>$M$</li>\n<li> item $n$</li>\n<li> item $s$</li>\n</ul>\n<p><br>$$<br>\\hat{s} = \\frac{CM + ns}{C + n}<br>$$</p>\n<h1 id=\"Python-\"><a href=\"#Python-\" class=\"headerlink\" title=\"Python \"></a>Python </h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Wilson</span><span class=\"params\">(p, n)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    p = float(p)</span><br><span class=\"line\">    K = <span class=\"number\">1.96</span>  <span class=\"comment\"># 95% confidence level</span></span><br><span class=\"line\">    _K2_div_n = (K ** <span class=\"number\">2</span>) / n</span><br><span class=\"line\">    pmin = (p + _K2_div_n / <span class=\"number\">2.0</span> -</span><br><span class=\"line\">            K * ((p * (<span class=\"number\">1</span> - p) / n + _K2_div_n / n / <span class=\"number\">4.0</span>) ** <span class=\"number\">0.5</span>)) / (<span class=\"number\">1</span> + _K2_div_n)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> pmin</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">WilsonAvgP</span><span class=\"params\">(n)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">     Bayesian  M </span></span><br><span class=\"line\"><span class=\"string\">     0.01~1  100 </span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    totalP = <span class=\"number\">0.0</span></span><br><span class=\"line\">    totalN = <span class=\"number\">0</span></span><br><span class=\"line\">    p = <span class=\"number\">0.01</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">while</span> <span class=\"literal\">True</span>:</span><br><span class=\"line\">        totalP += Wilson(p, n, <span class=\"number\">1</span>);</span><br><span class=\"line\">        totalN += <span class=\"number\">1</span></span><br><span class=\"line\">        p += <span class=\"number\">0.01</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> p &gt;= <span class=\"number\">1</span>:</span><br><span class=\"line\">            <span class=\"keyword\">break</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> totalP / totalN</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Bayesian</span><span class=\"params\">(C, M, n, s)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">     C  M </span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> (C * M + n * s) / (n + C)</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>YouTube StackOverflow  item  50000  95  5  95  item 95 50000  90  40000  92 </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> 100  85  100  85%  15%  item  85%  $\\alpha$  $\\alpha = 95\\%$  item </p>\n<p></p>\n<ul>\n<li>$S_{\\mathrm{max}}$ </li>\n<li> item $p$</li>\n<li>$n$</li>\n<li>$K = z_{1-\\frac{\\alpha}{2}}$</li>\n</ul>\n<p><br>$$<br>s = p_{\\mathrm{min}} \\cdot S_{\\mathrm{max}}, \\, p_{\\mathrm{min}} = \\frac{p + \\frac{K^{2}}{2n} - K \\sqrt{\\frac{p(1 - p)}{n} + \\frac{K^{2}}{4n^{2}}}}{1 + \\frac{K^{2}}{n}}<br>$$<br> <a href=\"http://sakigami-yang.me/2017/10/12/statistical-formulars-for-programmers/\"></a> </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p> item  $C$ </p>\n<p></p>\n<ul>\n<li>$C$</li>\n<li>$M$</li>\n<li> item $n$</li>\n<li> item $s$</li>\n</ul>\n<p><br>$$<br>\\hat{s} = \\frac{CM + ns}{C + n}<br>$$</p>\n<h1 id=\"Python-\"><a href=\"#Python-\" class=\"headerlink\" title=\"Python \"></a>Python </h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Wilson</span><span class=\"params\">(p, n)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    p = float(p)</span><br><span class=\"line\">    K = <span class=\"number\">1.96</span>  <span class=\"comment\"># 95% confidence level</span></span><br><span class=\"line\">    _K2_div_n = (K ** <span class=\"number\">2</span>) / n</span><br><span class=\"line\">    pmin = (p + _K2_div_n / <span class=\"number\">2.0</span> -</span><br><span class=\"line\">            K * ((p * (<span class=\"number\">1</span> - p) / n + _K2_div_n / n / <span class=\"number\">4.0</span>) ** <span class=\"number\">0.5</span>)) / (<span class=\"number\">1</span> + _K2_div_n)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> pmin</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">WilsonAvgP</span><span class=\"params\">(n)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">     Bayesian  M </span></span><br><span class=\"line\"><span class=\"string\">     0.01~1  100 </span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    totalP = <span class=\"number\">0.0</span></span><br><span class=\"line\">    totalN = <span class=\"number\">0</span></span><br><span class=\"line\">    p = <span class=\"number\">0.01</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">while</span> <span class=\"literal\">True</span>:</span><br><span class=\"line\">        totalP += Wilson(p, n, <span class=\"number\">1</span>);</span><br><span class=\"line\">        totalN += <span class=\"number\">1</span></span><br><span class=\"line\">        p += <span class=\"number\">0.01</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> p &gt;= <span class=\"number\">1</span>:</span><br><span class=\"line\">            <span class=\"keyword\">break</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> totalP / totalN</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Bayesian</span><span class=\"params\">(C, M, n, s)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">     C  M </span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> (C * M + n * s) / (n + C)</span><br></pre></td></tr></table></figure>\n"},{"title":"","date":"2017-10-12T01:56:30.000Z","description":"","_content":"\n# \n\n## \n\n\n$$\ns = \\sqrt{\\frac{1}{N-1}\\sum_{i=1}^{N}(x_{i}-\\bar{x})^{2}}\n$$\n\n\n- $N$ \n- $x_{i}$  $i$ \n- $\\bar{x}$ \n\n## \n\n\n$$\nSE = \\frac{s}{\\sqrt{N}}\n$$\n\n## \n\n\n$$\nCI = \\bar{x} \\pm t_{\\alpha/2} SE\n$$\n\n\n- $\\alpha$  5%  1 \n- $t_{\\alpha/2}$  $N-1$  t \n\n##  t \n\n t \n\n\n$$\nt = \\frac{\\bar{x_{1}} - \\bar{x_{2}}}{\\sqrt{s_{1}^{2}/n_{1}+s_{2}^{2}/n_{2}}}\n$$\n $\\lvert t \\rvert$ \n$$\ndf = \\frac{(s_{1}^{2}/n_{1}+s_{2}^{2}/n_{2})^2}{(s_{1}^{2}/n_{1})^{2}/(n_{1}-1)+(s_{2}^{2}/n_{2})^{2}/(n_{2}-1)}\n$$\n t  $1-\\alpha /2$ \n\n# \n\n## \n\n\n$$\nCI = \\left( p + \\frac{z_{\\alpha / 2}^{2}}{2N} \\pm z_{\\alpha / 2}\\sqrt{[p(1-p)+z_{\\alpha / 2}/4N] / N} \\right) / (1+z_{\\alpha / 2}^{2}/N)\n$$\n\n\n- $p$ \n- $z_{\\alpha / 2}$  $1-\\alpha / 2$ \n\n## \n\n$$\nCI = \\left( p_{j} + \\frac{z_{\\alpha / 2}^{2}}{2N} \\pm z_{\\alpha / 2}\\sqrt{[p_{j}(1-p_{j})+z_{\\alpha / 2}/4N] / N} \\right) / (1+z_{\\alpha / 2}^{2}/N)\n$$\n\n## \n\n\n\n\n$$\nX^{2} = \\sum_{i=1}^{n} \\sum_{j=1}^{m} \\frac{(O_{i,j}-E_{i,j})^{2}}{E_{i,j}}\n$$\n\n\n- $n$ \n- $m$ \n- $O_{i,j}$  $i$  $j$ \n- $E_{i,j}$  $i$  $j$ \n\n\n$$\nE_{i,j} = \\frac{\\sum_{k=1}^{n} O_{k,j} \\sum_{l=1}^{m} O_{i,l}}{N}\n$$\n\n\n- $N$ \n\n$X^{2}$  $(m-1) \\times (n-1)$  $\\chi^{2}$  $1-\\alpha$ \n\n# \n\n## \n\n$$\n\\sigma = \\sqrt{\\lambda}\n$$\n\n## \n\n$$\nCI = \\left( \\frac{\\gamma^{-1}(\\alpha / 2,c)}{t},\\frac{\\gamma^{-1}(1 - \\alpha / 2,c+1)}{t} \\right)\n$$\n\n\n\n- $c$  $t$ \n- $\\gamma^{-1}(p, c)$ \n\n## \n\n 5  -2  28.57%\n\n5  7 \n\np \n$$\np = 2 \\times \\frac{c!}{t^{c}} \\times \\min \\left\\{ \\sum_{i=0}^{c_{1}}\\frac{t_{1}^{i}t_{2}^{c-i}}{i!(c-i!)}, \\sum_{i=c_{1}}^{c}\\frac{t_{1}^{i}t_{2}^{c-i}}{i!(c-i!)} \\right\\}\n$$\n\n\n-  1  $t_{1}$  $c_{1}$ \n-  2  $t_{2}$  $c_{2}$ \n- $c = c_{1} + c_{2}$ $t = t_{1}+t_{2}$ \n\n","source":"_posts/statistical-formulars-for-programmers.md","raw":"---\ntitle: \ndate: 2017-10-12 09:56:30\ncategories: Mathematics\ntags: \n     - Statistics\ndescription: \n---\n\n# \n\n## \n\n\n$$\ns = \\sqrt{\\frac{1}{N-1}\\sum_{i=1}^{N}(x_{i}-\\bar{x})^{2}}\n$$\n\n\n- $N$ \n- $x_{i}$  $i$ \n- $\\bar{x}$ \n\n## \n\n\n$$\nSE = \\frac{s}{\\sqrt{N}}\n$$\n\n## \n\n\n$$\nCI = \\bar{x} \\pm t_{\\alpha/2} SE\n$$\n\n\n- $\\alpha$  5%  1 \n- $t_{\\alpha/2}$  $N-1$  t \n\n##  t \n\n t \n\n\n$$\nt = \\frac{\\bar{x_{1}} - \\bar{x_{2}}}{\\sqrt{s_{1}^{2}/n_{1}+s_{2}^{2}/n_{2}}}\n$$\n $\\lvert t \\rvert$ \n$$\ndf = \\frac{(s_{1}^{2}/n_{1}+s_{2}^{2}/n_{2})^2}{(s_{1}^{2}/n_{1})^{2}/(n_{1}-1)+(s_{2}^{2}/n_{2})^{2}/(n_{2}-1)}\n$$\n t  $1-\\alpha /2$ \n\n# \n\n## \n\n\n$$\nCI = \\left( p + \\frac{z_{\\alpha / 2}^{2}}{2N} \\pm z_{\\alpha / 2}\\sqrt{[p(1-p)+z_{\\alpha / 2}/4N] / N} \\right) / (1+z_{\\alpha / 2}^{2}/N)\n$$\n\n\n- $p$ \n- $z_{\\alpha / 2}$  $1-\\alpha / 2$ \n\n## \n\n$$\nCI = \\left( p_{j} + \\frac{z_{\\alpha / 2}^{2}}{2N} \\pm z_{\\alpha / 2}\\sqrt{[p_{j}(1-p_{j})+z_{\\alpha / 2}/4N] / N} \\right) / (1+z_{\\alpha / 2}^{2}/N)\n$$\n\n## \n\n\n\n\n$$\nX^{2} = \\sum_{i=1}^{n} \\sum_{j=1}^{m} \\frac{(O_{i,j}-E_{i,j})^{2}}{E_{i,j}}\n$$\n\n\n- $n$ \n- $m$ \n- $O_{i,j}$  $i$  $j$ \n- $E_{i,j}$  $i$  $j$ \n\n\n$$\nE_{i,j} = \\frac{\\sum_{k=1}^{n} O_{k,j} \\sum_{l=1}^{m} O_{i,l}}{N}\n$$\n\n\n- $N$ \n\n$X^{2}$  $(m-1) \\times (n-1)$  $\\chi^{2}$  $1-\\alpha$ \n\n# \n\n## \n\n$$\n\\sigma = \\sqrt{\\lambda}\n$$\n\n## \n\n$$\nCI = \\left( \\frac{\\gamma^{-1}(\\alpha / 2,c)}{t},\\frac{\\gamma^{-1}(1 - \\alpha / 2,c+1)}{t} \\right)\n$$\n\n\n\n- $c$  $t$ \n- $\\gamma^{-1}(p, c)$ \n\n## \n\n 5  -2  28.57%\n\n5  7 \n\np \n$$\np = 2 \\times \\frac{c!}{t^{c}} \\times \\min \\left\\{ \\sum_{i=0}^{c_{1}}\\frac{t_{1}^{i}t_{2}^{c-i}}{i!(c-i!)}, \\sum_{i=c_{1}}^{c}\\frac{t_{1}^{i}t_{2}^{c-i}}{i!(c-i!)} \\right\\}\n$$\n\n\n-  1  $t_{1}$  $c_{1}$ \n-  2  $t_{2}$  $c_{2}$ \n- $c = c_{1} + c_{2}$ $t = t_{1}+t_{2}$ \n\n","slug":"statistical-formulars-for-programmers","published":1,"updated":"2021-07-19T13:03:31.698Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrancn9y001ag0vwa6z7y3xd","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p><br>$$<br>s = \\sqrt{\\frac{1}{N-1}\\sum_{i=1}^{N}(x_{i}-\\bar{x})^{2}}<br>$$<br></p>\n<ul>\n<li>$N$ </li>\n<li>$x_{i}$  $i$ </li>\n<li>$\\bar{x}$ </li>\n</ul>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p><br>$$<br>SE = \\frac{s}{\\sqrt{N}}<br>$$</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p><br>$$<br>CI = \\bar{x} \\pm t_{\\alpha/2} SE<br>$$<br></p>\n<ul>\n<li>$\\alpha$  5%  1 </li>\n<li>$t_{\\alpha/2}$  $N-1$  t </li>\n</ul>\n<h2 id=\"-t-\"><a href=\"#-t-\" class=\"headerlink\" title=\" t \"></a> t </h2><p> t </p>\n<p><br>$$<br>t = \\frac{\\bar{x_{1}} - \\bar{x_{2}}}{\\sqrt{s_{1}^{2}/n_{1}+s_{2}^{2}/n_{2}}}<br>$$<br> $\\lvert t \\rvert$ <br>$$<br>df = \\frac{(s_{1}^{2}/n_{1}+s_{2}^{2}/n_{2})^2}{(s_{1}^{2}/n_{1})^{2}/(n_{1}-1)+(s_{2}^{2}/n_{2})^{2}/(n_{2}-1)}<br>$$<br> t  $1-\\alpha /2$ </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p><br>$$<br>CI = \\left( p + \\frac{z_{\\alpha / 2}^{2}}{2N} \\pm z_{\\alpha / 2}\\sqrt{[p(1-p)+z_{\\alpha / 2}/4N] / N} \\right) / (1+z_{\\alpha / 2}^{2}/N)<br>$$<br></p>\n<ul>\n<li>$p$ </li>\n<li>$z_{\\alpha / 2}$  $1-\\alpha / 2$ </li>\n</ul>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p>$$<br>CI = \\left( p_{j} + \\frac{z_{\\alpha / 2}^{2}}{2N} \\pm z_{\\alpha / 2}\\sqrt{[p_{j}(1-p_{j})+z_{\\alpha / 2}/4N] / N} \\right) / (1+z_{\\alpha / 2}^{2}/N)<br>$$</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p></p>\n<p><br>$$<br>X^{2} = \\sum_{i=1}^{n} \\sum_{j=1}^{m} \\frac{(O_{i,j}-E_{i,j})^{2}}{E_{i,j}}<br>$$<br></p>\n<ul>\n<li>$n$ </li>\n<li>$m$ </li>\n<li>$O_{i,j}$  $i$  $j$ </li>\n<li>$E_{i,j}$  $i$  $j$ </li>\n</ul>\n<p><br>$$<br>E_{i,j} = \\frac{\\sum_{k=1}^{n} O_{k,j} \\sum_{l=1}^{m} O_{i,l}}{N}<br>$$<br></p>\n<ul>\n<li>$N$ </li>\n</ul>\n<p>$X^{2}$  $(m-1) \\times (n-1)$  $\\chi^{2}$  $1-\\alpha$ </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p>$$<br>\\sigma = \\sqrt{\\lambda}<br>$$</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p>$$<br>CI = \\left( \\frac{\\gamma^{-1}(\\alpha / 2,c)}{t},\\frac{\\gamma^{-1}(1 - \\alpha / 2,c+1)}{t} \\right)<br>$$</p>\n<p></p>\n<ul>\n<li>$c$  $t$ </li>\n<li>$\\gamma^{-1}(p, c)$ </li>\n</ul>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> 5  -2  28.57%</p>\n<p>5  7 </p>\n<p>p <br>$$<br>p = 2 \\times \\frac{c!}{t^{c}} \\times \\min \\left{ \\sum_{i=0}^{c_{1}}\\frac{t_{1}^{i}t_{2}^{c-i}}{i!(c-i!)}, \\sum_{i=c_{1}}^{c}\\frac{t_{1}^{i}t_{2}^{c-i}}{i!(c-i!)} \\right}<br>$$<br></p>\n<ul>\n<li> 1  $t_{1}$  $c_{1}$ </li>\n<li> 2  $t_{2}$  $c_{2}$ </li>\n<li>$c = c_{1} + c_{2}$ $t = t_{1}+t_{2}$ </li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p><br>$$<br>s = \\sqrt{\\frac{1}{N-1}\\sum_{i=1}^{N}(x_{i}-\\bar{x})^{2}}<br>$$<br></p>\n<ul>\n<li>$N$ </li>\n<li>$x_{i}$  $i$ </li>\n<li>$\\bar{x}$ </li>\n</ul>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p><br>$$<br>SE = \\frac{s}{\\sqrt{N}}<br>$$</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p><br>$$<br>CI = \\bar{x} \\pm t_{\\alpha/2} SE<br>$$<br></p>\n<ul>\n<li>$\\alpha$  5%  1 </li>\n<li>$t_{\\alpha/2}$  $N-1$  t </li>\n</ul>\n<h2 id=\"-t-\"><a href=\"#-t-\" class=\"headerlink\" title=\" t \"></a> t </h2><p> t </p>\n<p><br>$$<br>t = \\frac{\\bar{x_{1}} - \\bar{x_{2}}}{\\sqrt{s_{1}^{2}/n_{1}+s_{2}^{2}/n_{2}}}<br>$$<br> $\\lvert t \\rvert$ <br>$$<br>df = \\frac{(s_{1}^{2}/n_{1}+s_{2}^{2}/n_{2})^2}{(s_{1}^{2}/n_{1})^{2}/(n_{1}-1)+(s_{2}^{2}/n_{2})^{2}/(n_{2}-1)}<br>$$<br> t  $1-\\alpha /2$ </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p><br>$$<br>CI = \\left( p + \\frac{z_{\\alpha / 2}^{2}}{2N} \\pm z_{\\alpha / 2}\\sqrt{[p(1-p)+z_{\\alpha / 2}/4N] / N} \\right) / (1+z_{\\alpha / 2}^{2}/N)<br>$$<br></p>\n<ul>\n<li>$p$ </li>\n<li>$z_{\\alpha / 2}$  $1-\\alpha / 2$ </li>\n</ul>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p>$$<br>CI = \\left( p_{j} + \\frac{z_{\\alpha / 2}^{2}}{2N} \\pm z_{\\alpha / 2}\\sqrt{[p_{j}(1-p_{j})+z_{\\alpha / 2}/4N] / N} \\right) / (1+z_{\\alpha / 2}^{2}/N)<br>$$</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p></p>\n<p><br>$$<br>X^{2} = \\sum_{i=1}^{n} \\sum_{j=1}^{m} \\frac{(O_{i,j}-E_{i,j})^{2}}{E_{i,j}}<br>$$<br></p>\n<ul>\n<li>$n$ </li>\n<li>$m$ </li>\n<li>$O_{i,j}$  $i$  $j$ </li>\n<li>$E_{i,j}$  $i$  $j$ </li>\n</ul>\n<p><br>$$<br>E_{i,j} = \\frac{\\sum_{k=1}^{n} O_{k,j} \\sum_{l=1}^{m} O_{i,l}}{N}<br>$$<br></p>\n<ul>\n<li>$N$ </li>\n</ul>\n<p>$X^{2}$  $(m-1) \\times (n-1)$  $\\chi^{2}$  $1-\\alpha$ </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p>$$<br>\\sigma = \\sqrt{\\lambda}<br>$$</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p>$$<br>CI = \\left( \\frac{\\gamma^{-1}(\\alpha / 2,c)}{t},\\frac{\\gamma^{-1}(1 - \\alpha / 2,c+1)}{t} \\right)<br>$$</p>\n<p></p>\n<ul>\n<li>$c$  $t$ </li>\n<li>$\\gamma^{-1}(p, c)$ </li>\n</ul>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p> 5  -2  28.57%</p>\n<p>5  7 </p>\n<p>p <br>$$<br>p = 2 \\times \\frac{c!}{t^{c}} \\times \\min \\left{ \\sum_{i=0}^{c_{1}}\\frac{t_{1}^{i}t_{2}^{c-i}}{i!(c-i!)}, \\sum_{i=c_{1}}^{c}\\frac{t_{1}^{i}t_{2}^{c-i}}{i!(c-i!)} \\right}<br>$$<br></p>\n<ul>\n<li> 1  $t_{1}$  $c_{1}$ </li>\n<li> 2  $t_{2}$  $c_{2}$ </li>\n<li>$c = c_{1} + c_{2}$ $t = t_{1}+t_{2}$ </li>\n</ul>\n"},{"title":" Softmax ","date":"2017-08-10T13:12:46.000Z","description":"   Softmax  Logistic  Softmax ","_content":"\n SoftmaxSoftmax\n\n\n#   \n\nLogistic  Softmax \nGeneralized Linear Model\n\n\n- \n-  Logistic \n-  Softmax \n\n $y$ exponential family distribution $h(x)$ \n\n\n\n\n\n1.  $x$  $\\theta$ $y$  $P(y \\mid x; \\, \\theta)$ \n2.  $T(y)$  $E[T(y) \\mid x]$ \n3. $\\eta$  $x$  $\\eta=\\theta^{\\mathsf{T}}x$ \n\n\n$$\nP(y; \\, \\eta) = b(y) \\ast \\exp(\\eta^{\\mathsf{T}}T(y)-a(\\eta))\n$$\n\n#  Logistic \n\nBernoulli distribution 0-1 binomial distribution\n $n$  $n$ \n$P(y; \\, \\phi) = \\phi^{y}(1-\\phi)^{1-y}$ \n$P(y; \\, \\phi) = \\exp(y\\ln \\frac{\\phi}{1-\\phi}+\\ln(1-\\phi))$ \n$\\eta=\\ln \\frac{\\phi}{1-\\phi} \\Rightarrow \\phi=\\frac{1}{1+e^{-\\eta}}$ \n\n$$\n\\phi = \\frac{1}{1+e^{-\\theta^{\\mathsf{T}}x}}\n$$\n Logistic \n 0  1\n\n#  Softmax \n\nmultinomial distribution $n$  $k$  Logistic \n$$\nP(y^{(i)}=j \\mid x^{(i)}; \\, \\theta)=\\frac{e^{\\theta_{j}^{\\mathsf{T}}x^{(i)}}}{\\sum_k e^{\\theta_{k}^{\\mathsf{T}}x^{(i)}}}\n$$\n Softmax  $k$  $k-1$  $k$ \n\n $k=2$ Softmax  Logistic ","source":"_posts/thinking-from-softmax-01.md","raw":"---\ntitle:  Softmax \ndate: 2017-08-10 21:12:46\ncategories: ML\ntags: \n     - Generalized Linear Model\n     - Exponential family distribution\n     - Logistic\n     - Softmax\ndescription:    Softmax  Logistic  Softmax \n---\n\n SoftmaxSoftmax\n\n\n#   \n\nLogistic  Softmax \nGeneralized Linear Model\n\n\n- \n-  Logistic \n-  Softmax \n\n $y$ exponential family distribution $h(x)$ \n\n\n\n\n\n1.  $x$  $\\theta$ $y$  $P(y \\mid x; \\, \\theta)$ \n2.  $T(y)$  $E[T(y) \\mid x]$ \n3. $\\eta$  $x$  $\\eta=\\theta^{\\mathsf{T}}x$ \n\n\n$$\nP(y; \\, \\eta) = b(y) \\ast \\exp(\\eta^{\\mathsf{T}}T(y)-a(\\eta))\n$$\n\n#  Logistic \n\nBernoulli distribution 0-1 binomial distribution\n $n$  $n$ \n$P(y; \\, \\phi) = \\phi^{y}(1-\\phi)^{1-y}$ \n$P(y; \\, \\phi) = \\exp(y\\ln \\frac{\\phi}{1-\\phi}+\\ln(1-\\phi))$ \n$\\eta=\\ln \\frac{\\phi}{1-\\phi} \\Rightarrow \\phi=\\frac{1}{1+e^{-\\eta}}$ \n\n$$\n\\phi = \\frac{1}{1+e^{-\\theta^{\\mathsf{T}}x}}\n$$\n Logistic \n 0  1\n\n#  Softmax \n\nmultinomial distribution $n$  $k$  Logistic \n$$\nP(y^{(i)}=j \\mid x^{(i)}; \\, \\theta)=\\frac{e^{\\theta_{j}^{\\mathsf{T}}x^{(i)}}}{\\sum_k e^{\\theta_{k}^{\\mathsf{T}}x^{(i)}}}\n$$\n Softmax  $k$  $k-1$  $k$ \n\n $k=2$ Softmax  Logistic ","slug":"thinking-from-softmax-01","published":1,"updated":"2021-07-19T13:03:31.698Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrancn9z001eg0vw65av20cz","content":"<p> SoftmaxSoftmax<br></p>\n<h1 id=\"--\"><a href=\"#--\" class=\"headerlink\" title=\"  \"></a>  </h1><p>Logistic  Softmax <br>Generalized Linear Model<br></p>\n<ul>\n<li></li>\n<li> Logistic </li>\n<li> Softmax </li>\n</ul>\n<p> $y$ exponential family distribution $h(x)$ </p>\n<p></p>\n<ol>\n<li> $x$  $\\theta$ $y$  $P(y \\mid x; \\, \\theta)$ </li>\n<li> $T(y)$  $E[T(y) \\mid x]$ </li>\n<li>$\\eta$  $x$  $\\eta=\\theta^{\\mathsf{T}}x$ </li>\n</ol>\n<p><br>$$<br>P(y; \\, \\eta) = b(y) \\ast \\exp(\\eta^{\\mathsf{T}}T(y)-a(\\eta))<br>$$</p>\n<h1 id=\"-Logistic-\"><a href=\"#-Logistic-\" class=\"headerlink\" title=\" Logistic \"></a> Logistic </h1><p>Bernoulli distribution 0-1 binomial distribution<br> $n$  $n$ <br>$P(y; \\, \\phi) = \\phi^{y}(1-\\phi)^{1-y}$ <br>$P(y; \\, \\phi) = \\exp(y\\ln \\frac{\\phi}{1-\\phi}+\\ln(1-\\phi))$ <br>$\\eta=\\ln \\frac{\\phi}{1-\\phi} \\Rightarrow \\phi=\\frac{1}{1+e^{-\\eta}}$ <br><br>$$<br>\\phi = \\frac{1}{1+e^{-\\theta^{\\mathsf{T}}x}}<br>$$<br> Logistic <br> 0  1</p>\n<h1 id=\"-Softmax-\"><a href=\"#-Softmax-\" class=\"headerlink\" title=\" Softmax \"></a> Softmax </h1><p>multinomial distribution $n$  $k$  Logistic <br>$$<br>P(y^{(i)}=j \\mid x^{(i)}; \\, \\theta)=\\frac{e^{\\theta_{j}^{\\mathsf{T}}x^{(i)}}}{\\sum_k e^{\\theta_{k}^{\\mathsf{T}}x^{(i)}}}<br>$$<br> Softmax  $k$  $k-1$  $k$ </p>\n<p> $k=2$ Softmax  Logistic </p>\n","site":{"data":{}},"excerpt":"","more":"<p> SoftmaxSoftmax<br></p>\n<h1 id=\"--\"><a href=\"#--\" class=\"headerlink\" title=\"  \"></a>  </h1><p>Logistic  Softmax <br>Generalized Linear Model<br></p>\n<ul>\n<li></li>\n<li> Logistic </li>\n<li> Softmax </li>\n</ul>\n<p> $y$ exponential family distribution $h(x)$ </p>\n<p></p>\n<ol>\n<li> $x$  $\\theta$ $y$  $P(y \\mid x; \\, \\theta)$ </li>\n<li> $T(y)$  $E[T(y) \\mid x]$ </li>\n<li>$\\eta$  $x$  $\\eta=\\theta^{\\mathsf{T}}x$ </li>\n</ol>\n<p><br>$$<br>P(y; \\, \\eta) = b(y) \\ast \\exp(\\eta^{\\mathsf{T}}T(y)-a(\\eta))<br>$$</p>\n<h1 id=\"-Logistic-\"><a href=\"#-Logistic-\" class=\"headerlink\" title=\" Logistic \"></a> Logistic </h1><p>Bernoulli distribution 0-1 binomial distribution<br> $n$  $n$ <br>$P(y; \\, \\phi) = \\phi^{y}(1-\\phi)^{1-y}$ <br>$P(y; \\, \\phi) = \\exp(y\\ln \\frac{\\phi}{1-\\phi}+\\ln(1-\\phi))$ <br>$\\eta=\\ln \\frac{\\phi}{1-\\phi} \\Rightarrow \\phi=\\frac{1}{1+e^{-\\eta}}$ <br><br>$$<br>\\phi = \\frac{1}{1+e^{-\\theta^{\\mathsf{T}}x}}<br>$$<br> Logistic <br> 0  1</p>\n<h1 id=\"-Softmax-\"><a href=\"#-Softmax-\" class=\"headerlink\" title=\" Softmax \"></a> Softmax </h1><p>multinomial distribution $n$  $k$  Logistic <br>$$<br>P(y^{(i)}=j \\mid x^{(i)}; \\, \\theta)=\\frac{e^{\\theta_{j}^{\\mathsf{T}}x^{(i)}}}{\\sum_k e^{\\theta_{k}^{\\mathsf{T}}x^{(i)}}}<br>$$<br> Softmax  $k$  $k-1$  $k$ </p>\n<p> $k=2$ Softmax  Logistic </p>\n"},{"title":" Softmax  Logistic  Loss ","date":"2017-08-10T14:18:09.000Z","description":"   Softmax  Logistic  Loss ","_content":"\n# Logistic  Loss \n\nLogistic  Loss \n\n$$\n\\begin{aligned}\nLoss(h_{\\theta}, \\, y) &=\\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{2}[h_{\\theta}(x_{i})-y_{i}]^{2} \\\\\n& =\\frac{1}{2}\\left(\\frac{1}{1+e^{-\\theta^{\\mathsf{T}}x}}-y\\right)^{2}\n\\end{aligned}\n$$\n $1/2$ \n$n$ \n\n\n Loss \n$$\nLoss = \n\\left \\{ \\begin{aligned}\n& -\\log (h_{\\theta}(x)) & \\qquad \\text{if }y=1 \\\\\n& -\\log (1-h_{\\theta}(x)) & \\qquad \\text{if }y=0\n\\end{aligned} \\right.\n$$\n\n\n****\n Loss  $-\\log$ \n\n#   \n\n\nconvex optimization\n\nSVM  Perceptron\n\n\n\ngradient descentGD\n$f(y) \\ge f(x) + \\nabla f(x)^{\\mathsf{T}}(y-x)$  $f$ \n$\\nabla^{2}f(x) \\ge 0$ \n\n\n $y=|x|$ \nneural netNNactivation FunctionReLU\n\nsubgradient\n $g$ $f(y) \\ge f(x) + g^{\\mathsf{T}}(y-x)$ \n\n $y=|x|$  0ReLU $1/2$ \n\n![Subgradient](thinking-from-softmax-02/subgradient.png)\n\n [](https://baike.baidu.com/item//13882223?fr=aladdin)\n\n# Softmax  Loss  Log-Sum-Exp \n\n wiki\n Logistic  Loss  $\\log$ $\\exp$ \n\n\n\n $f(x) = \\log (\\sum \\alpha_{i} e^{x_{i}})$  Affine composition  Log-Sum-Exp \nAffine composition $f(x)$  $\\Leftrightarrow f(Ax+b)$ \n\n\n$$\n\\begin{aligned}\n\\nabla_{i} f & = e^{x_{i}}\\Big/\\sum_{k} e^{x_{k}}  & \\\\\n\\nabla_{i}^{2} f & = \\mathrm{diag}(z)-zz^{\\mathsf{T}}, & \\quad z_{i} = e^{x_{i}}\\Big/\\sum_{k} e^{x_{k}}\n\\end{aligned}\n$$\n$\\nabla_{i}^{2} f$ diagonally dominant matrix\n $A$  $A_{ii} \\ge \\sum_{i \\ne j} |A_{ij}|$ \n$\\nabla_{i}^{2} f$ positive semidefinite matrixp.s.d\n Log-Sum-Exp ","source":"_posts/thinking-from-softmax-02.md","raw":"---\ntitle:  Softmax  Logistic  Loss \ndate: 2017-08-10 22:18:09\ncategories: ML\ntags: \n     - Logistic\n     - Softmax\n     - Convex optimization\n     - Gradient\n     - Subgradient\ndescription:    Softmax  Logistic  Loss \n---\n\n# Logistic  Loss \n\nLogistic  Loss \n\n$$\n\\begin{aligned}\nLoss(h_{\\theta}, \\, y) &=\\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{2}[h_{\\theta}(x_{i})-y_{i}]^{2} \\\\\n& =\\frac{1}{2}\\left(\\frac{1}{1+e^{-\\theta^{\\mathsf{T}}x}}-y\\right)^{2}\n\\end{aligned}\n$$\n $1/2$ \n$n$ \n\n\n Loss \n$$\nLoss = \n\\left \\{ \\begin{aligned}\n& -\\log (h_{\\theta}(x)) & \\qquad \\text{if }y=1 \\\\\n& -\\log (1-h_{\\theta}(x)) & \\qquad \\text{if }y=0\n\\end{aligned} \\right.\n$$\n\n\n****\n Loss  $-\\log$ \n\n#   \n\n\nconvex optimization\n\nSVM  Perceptron\n\n\n\ngradient descentGD\n$f(y) \\ge f(x) + \\nabla f(x)^{\\mathsf{T}}(y-x)$  $f$ \n$\\nabla^{2}f(x) \\ge 0$ \n\n\n $y=|x|$ \nneural netNNactivation FunctionReLU\n\nsubgradient\n $g$ $f(y) \\ge f(x) + g^{\\mathsf{T}}(y-x)$ \n\n $y=|x|$  0ReLU $1/2$ \n\n![Subgradient](thinking-from-softmax-02/subgradient.png)\n\n [](https://baike.baidu.com/item//13882223?fr=aladdin)\n\n# Softmax  Loss  Log-Sum-Exp \n\n wiki\n Logistic  Loss  $\\log$ $\\exp$ \n\n\n\n $f(x) = \\log (\\sum \\alpha_{i} e^{x_{i}})$  Affine composition  Log-Sum-Exp \nAffine composition $f(x)$  $\\Leftrightarrow f(Ax+b)$ \n\n\n$$\n\\begin{aligned}\n\\nabla_{i} f & = e^{x_{i}}\\Big/\\sum_{k} e^{x_{k}}  & \\\\\n\\nabla_{i}^{2} f & = \\mathrm{diag}(z)-zz^{\\mathsf{T}}, & \\quad z_{i} = e^{x_{i}}\\Big/\\sum_{k} e^{x_{k}}\n\\end{aligned}\n$$\n$\\nabla_{i}^{2} f$ diagonally dominant matrix\n $A$  $A_{ii} \\ge \\sum_{i \\ne j} |A_{ij}|$ \n$\\nabla_{i}^{2} f$ positive semidefinite matrixp.s.d\n Log-Sum-Exp ","slug":"thinking-from-softmax-02","published":1,"updated":"2021-07-19T13:03:31.698Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrancna0001ig0vwlg1i0v57","content":"<h1 id=\"Logistic--Loss-\"><a href=\"#Logistic--Loss-\" class=\"headerlink\" title=\"Logistic  Loss \"></a>Logistic  Loss </h1><p>Logistic  Loss <br><br>$$<br>\\begin{aligned}<br>Loss(h_{\\theta}, \\, y) &amp;=\\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{2}[h_{\\theta}(x_{i})-y_{i}]^{2} \\<br>&amp; =\\frac{1}{2}\\left(\\frac{1}{1+e^{-\\theta^{\\mathsf{T}}x}}-y\\right)^{2}<br>\\end{aligned}<br>$$<br> $1/2$ <br>$n$ <br></p>\n<p> Loss <br>$$<br>Loss =<br>\\left { \\begin{aligned}<br>&amp; -\\log (h_{\\theta}(x)) &amp; \\qquad \\text{if }y=1 \\<br>&amp; -\\log (1-h_{\\theta}(x)) &amp; \\qquad \\text{if }y=0<br>\\end{aligned} \\right.<br>$$<br></p>\n<p><strong></strong><br> Loss  $-\\log$ </p>\n<h1 id=\"--\"><a href=\"#--\" class=\"headerlink\" title=\"  \"></a>  </h1><p><br>convex optimization</p>\n<p>SVM  Perceptron<br><br></p>\n<p>gradient descentGD<br>$f(y) \\ge f(x) + \\nabla f(x)^{\\mathsf{T}}(y-x)$  $f$ <br>$\\nabla^{2}f(x) \\ge 0$ </p>\n<p><br> $y=|x|$ <br>neural netNNactivation FunctionReLU</p>\n<p>subgradient<br> $g$ $f(y) \\ge f(x) + g^{\\mathsf{T}}(y-x)$ <br><br> $y=|x|$  0ReLU $1/2$ <br><br><img src=\"/.me//subgradient.png\" alt=\"Subgradient\"></p>\n<p> <a href=\"https://baike.baidu.com/item//13882223?fr=aladdin\" target=\"_blank\" rel=\"noopener\"></a></p>\n<h1 id=\"Softmax--Loss--Log-Sum-Exp-\"><a href=\"#Softmax--Loss--Log-Sum-Exp-\" class=\"headerlink\" title=\"Softmax  Loss  Log-Sum-Exp \"></a>Softmax  Loss  Log-Sum-Exp </h1><p> wiki<br> Logistic  Loss  $\\log$ $\\exp$ </p>\n<p></p>\n<p> $f(x) = \\log (\\sum \\alpha_{i} e^{x_{i}})$  Affine composition  Log-Sum-Exp <br>Affine composition $f(x)$  $\\Leftrightarrow f(Ax+b)$ </p>\n<p><br>$$<br>\\begin{aligned}<br>\\nabla_{i} f &amp; = e^{x_{i}}\\Big/\\sum_{k} e^{x_{k}}  &amp; \\<br>\\nabla_{i}^{2} f &amp; = \\mathrm{diag}(z)-zz^{\\mathsf{T}}, &amp; \\quad z_{i} = e^{x_{i}}\\Big/\\sum_{k} e^{x_{k}}<br>\\end{aligned}<br>$$<br>$\\nabla_{i}^{2} f$ diagonally dominant matrix<br> $A$  $A_{ii} \\ge \\sum_{i \\ne j} |A_{ij}|$ <br>$\\nabla_{i}^{2} f$ positive semidefinite matrixp.s.d<br> Log-Sum-Exp </p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Logistic--Loss-\"><a href=\"#Logistic--Loss-\" class=\"headerlink\" title=\"Logistic  Loss \"></a>Logistic  Loss </h1><p>Logistic  Loss <br><br>$$<br>\\begin{aligned}<br>Loss(h_{\\theta}, \\, y) &amp;=\\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{2}[h_{\\theta}(x_{i})-y_{i}]^{2} \\<br>&amp; =\\frac{1}{2}\\left(\\frac{1}{1+e^{-\\theta^{\\mathsf{T}}x}}-y\\right)^{2}<br>\\end{aligned}<br>$$<br> $1/2$ <br>$n$ <br></p>\n<p> Loss <br>$$<br>Loss =<br>\\left { \\begin{aligned}<br>&amp; -\\log (h_{\\theta}(x)) &amp; \\qquad \\text{if }y=1 \\<br>&amp; -\\log (1-h_{\\theta}(x)) &amp; \\qquad \\text{if }y=0<br>\\end{aligned} \\right.<br>$$<br></p>\n<p><strong></strong><br> Loss  $-\\log$ </p>\n<h1 id=\"--\"><a href=\"#--\" class=\"headerlink\" title=\"  \"></a>  </h1><p><br>convex optimization</p>\n<p>SVM  Perceptron<br><br></p>\n<p>gradient descentGD<br>$f(y) \\ge f(x) + \\nabla f(x)^{\\mathsf{T}}(y-x)$  $f$ <br>$\\nabla^{2}f(x) \\ge 0$ </p>\n<p><br> $y=|x|$ <br>neural netNNactivation FunctionReLU</p>\n<p>subgradient<br> $g$ $f(y) \\ge f(x) + g^{\\mathsf{T}}(y-x)$ <br><br> $y=|x|$  0ReLU $1/2$ <br><br><img src=\"/.me//subgradient.png\" alt=\"Subgradient\"></p>\n<p> <a href=\"https://baike.baidu.com/item//13882223?fr=aladdin\" target=\"_blank\" rel=\"noopener\"></a></p>\n<h1 id=\"Softmax--Loss--Log-Sum-Exp-\"><a href=\"#Softmax--Loss--Log-Sum-Exp-\" class=\"headerlink\" title=\"Softmax  Loss  Log-Sum-Exp \"></a>Softmax  Loss  Log-Sum-Exp </h1><p> wiki<br> Logistic  Loss  $\\log$ $\\exp$ </p>\n<p></p>\n<p> $f(x) = \\log (\\sum \\alpha_{i} e^{x_{i}})$  Affine composition  Log-Sum-Exp <br>Affine composition $f(x)$  $\\Leftrightarrow f(Ax+b)$ </p>\n<p><br>$$<br>\\begin{aligned}<br>\\nabla_{i} f &amp; = e^{x_{i}}\\Big/\\sum_{k} e^{x_{k}}  &amp; \\<br>\\nabla_{i}^{2} f &amp; = \\mathrm{diag}(z)-zz^{\\mathsf{T}}, &amp; \\quad z_{i} = e^{x_{i}}\\Big/\\sum_{k} e^{x_{k}}<br>\\end{aligned}<br>$$<br>$\\nabla_{i}^{2} f$ diagonally dominant matrix<br> $A$  $A_{ii} \\ge \\sum_{i \\ne j} |A_{ij}|$ <br>$\\nabla_{i}^{2} f$ positive semidefinite matrixp.s.d<br> Log-Sum-Exp </p>\n"},{"title":" Softmax   Sigmoid  ReLU","date":"2017-08-11T11:40:01.000Z","description":"   Softmax ","_content":"\n Logistic  Sigmoid \n Sigmoid  S  Sigmoid  Logistic \n $\\sigma(x)$ \n$$\n\\sigma(x) = \\frac{1}{1+e^{-x}}\n$$\n\n\n- \n-  $[-\\infty, +\\infty]$ \n-  $(0, 1)$  $(0, 1)$ \n\n\n\n\n\n# \n\n\n\nneural nets\nactivation function\n\n\n\n- \n- \n- learning rate\n\n $f(x) = x$  Sigmoid \n\n# Sigmoid \n\n Sigmoid saturation 0 kill gradient\n\n$$\n\\sigma(x)' = \\sigma(x)(1 - \\sigma(x))\n$$\nSigmoid  $1/4$  0 \n\nSigmoid  0  1\nbatch kill gradient \n\n\nfeature\n Dropout random forest Bagging \n\n# \n\n- tanh\n\n   0  Sigmoid \n\n- ReLU\n\n  $f(x) = \\max(0, x)$ \n\n  ReLU\n\n  1. \n  2. \n  3. 0  1\n  4.  ReLU die 0\n\n- Leaky-ReLUP-ReLUR-ReLUMaxout\n\n   wiki ReLU \n  dying ReLU\n  \n  No free lunch theoren\n\n  ****\n\n  Maxout  Maxout  Maxout  ReLU  ReLU  Dropout \n\n\n\n# \n\n\n\n ReLU \ndying ReLU Leaky ReLUPReLU  Maxout\n\n Sigmoid tanh\n\n","source":"_posts/thinking-from-softmax-03.md","raw":"---\ntitle:  Softmax   Sigmoid  ReLU\ndate: 2017-08-11 19:40:01\ncategories: ML\ntags:\n     - Deep learning\n     - Activation function\ndescription:    Softmax \n---\n\n Logistic  Sigmoid \n Sigmoid  S  Sigmoid  Logistic \n $\\sigma(x)$ \n$$\n\\sigma(x) = \\frac{1}{1+e^{-x}}\n$$\n\n\n- \n-  $[-\\infty, +\\infty]$ \n-  $(0, 1)$  $(0, 1)$ \n\n\n\n\n\n# \n\n\n\nneural nets\nactivation function\n\n\n\n- \n- \n- learning rate\n\n $f(x) = x$  Sigmoid \n\n# Sigmoid \n\n Sigmoid saturation 0 kill gradient\n\n$$\n\\sigma(x)' = \\sigma(x)(1 - \\sigma(x))\n$$\nSigmoid  $1/4$  0 \n\nSigmoid  0  1\nbatch kill gradient \n\n\nfeature\n Dropout random forest Bagging \n\n# \n\n- tanh\n\n   0  Sigmoid \n\n- ReLU\n\n  $f(x) = \\max(0, x)$ \n\n  ReLU\n\n  1. \n  2. \n  3. 0  1\n  4.  ReLU die 0\n\n- Leaky-ReLUP-ReLUR-ReLUMaxout\n\n   wiki ReLU \n  dying ReLU\n  \n  No free lunch theoren\n\n  ****\n\n  Maxout  Maxout  Maxout  ReLU  ReLU  Dropout \n\n\n\n# \n\n\n\n ReLU \ndying ReLU Leaky ReLUPReLU  Maxout\n\n Sigmoid tanh\n\n","slug":"thinking-from-softmax-03","published":1,"updated":"2021-07-19T13:03:31.699Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrancna1001lg0vwcod2qams","content":"<p> Logistic  Sigmoid <br> Sigmoid  S  Sigmoid  Logistic <br> $\\sigma(x)$ <br>$$<br>\\sigma(x) = \\frac{1}{1+e^{-x}}<br>$$<br></p>\n<ul>\n<li></li>\n<li> $[-\\infty, +\\infty]$ </li>\n<li> $(0, 1)$  $(0, 1)$ </li>\n</ul>\n<p></p>\n<p></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p><br><br>neural nets<br>activation function</p>\n<p></p>\n<ul>\n<li></li>\n<li></li>\n<li>learning rate</li>\n</ul>\n<p> $f(x) = x$  Sigmoid </p>\n<h1 id=\"Sigmoid-\"><a href=\"#Sigmoid-\" class=\"headerlink\" title=\"Sigmoid \"></a>Sigmoid </h1><p> Sigmoid saturation 0 kill gradient<br><br>$$<br>\\sigma(x) = \\sigma(x)(1 - \\sigma(x))<br>$$<br>Sigmoid  $1/4$  0 </p>\n<p>Sigmoid  0  1<br>batch kill gradient </p>\n<p><br>feature<br> Dropout random forest Bagging </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><ul>\n<li><p>tanh</p>\n<p> 0  Sigmoid </p>\n</li>\n<li><p>ReLU</p>\n<p>$f(x) = \\max(0, x)$ </p>\n<p>ReLU</p>\n<ol>\n<li></li>\n<li></li>\n<li>0  1</li>\n<li> ReLU die 0</li>\n</ol>\n</li>\n<li><p>Leaky-ReLUP-ReLUR-ReLUMaxout</p>\n<p> wiki ReLU <br>dying ReLU<br><br>No free lunch theoren</p>\n<p><strong></strong></p>\n<p>Maxout  Maxout  Maxout  ReLU  ReLU  Dropout </p>\n</li>\n</ul>\n<p></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n<p> ReLU <br>dying ReLU Leaky ReLUPReLU  Maxout</p>\n<p> Sigmoid tanh</p>\n<p></p>\n","site":{"data":{}},"excerpt":"","more":"<p> Logistic  Sigmoid <br> Sigmoid  S  Sigmoid  Logistic <br> $\\sigma(x)$ <br>$$<br>\\sigma(x) = \\frac{1}{1+e^{-x}}<br>$$<br></p>\n<ul>\n<li></li>\n<li> $[-\\infty, +\\infty]$ </li>\n<li> $(0, 1)$  $(0, 1)$ </li>\n</ul>\n<p></p>\n<p></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p><br><br>neural nets<br>activation function</p>\n<p></p>\n<ul>\n<li></li>\n<li></li>\n<li>learning rate</li>\n</ul>\n<p> $f(x) = x$  Sigmoid </p>\n<h1 id=\"Sigmoid-\"><a href=\"#Sigmoid-\" class=\"headerlink\" title=\"Sigmoid \"></a>Sigmoid </h1><p> Sigmoid saturation 0 kill gradient<br><br>$$<br>\\sigma(x) = \\sigma(x)(1 - \\sigma(x))<br>$$<br>Sigmoid  $1/4$  0 </p>\n<p>Sigmoid  0  1<br>batch kill gradient </p>\n<p><br>feature<br> Dropout random forest Bagging </p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><ul>\n<li><p>tanh</p>\n<p> 0  Sigmoid </p>\n</li>\n<li><p>ReLU</p>\n<p>$f(x) = \\max(0, x)$ </p>\n<p>ReLU</p>\n<ol>\n<li></li>\n<li></li>\n<li>0  1</li>\n<li> ReLU die 0</li>\n</ol>\n</li>\n<li><p>Leaky-ReLUP-ReLUR-ReLUMaxout</p>\n<p> wiki ReLU <br>dying ReLU<br><br>No free lunch theoren</p>\n<p><strong></strong></p>\n<p>Maxout  Maxout  Maxout  ReLU  ReLU  Dropout </p>\n</li>\n</ul>\n<p></p>\n<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p></p>\n<p> ReLU <br>dying ReLU Leaky ReLUPReLU  Maxout</p>\n<p> Sigmoid tanh</p>\n<p></p>\n"}],"PostAsset":[{"_id":"source/_posts/about-kernel-02/linear_space.png","post":"ckrancn9e000bg0vw2nlae14l","slug":"linear_space.png","modified":1,"renderable":1},{"_id":"source/_posts/thinking-from-softmax-02/subgradient.png","post":"ckrancna0001ig0vwlg1i0v57","slug":"subgradient.png","modified":1,"renderable":1}],"PostCategory":[{"post_id":"ckrancn900000g0vwtw0cs3mu","category_id":"ckrancn990003g0vwura362bx","_id":"ckrancn9h000dg0vwq2fndxb8"},{"post_id":"ckrancn9e000bg0vw2nlae14l","category_id":"ckrancn9d0008g0vwe20xrm6l","_id":"ckrancn9l000jg0vwureqzybl"},{"post_id":"ckrancn970002g0vwyu6y20ry","category_id":"ckrancn9d0008g0vwe20xrm6l","_id":"ckrancn9m000ng0vwik8qahtd"},{"post_id":"ckrancn9f000cg0vwz6oud3ca","category_id":"ckrancn990003g0vwura362bx","_id":"ckrancn9o000qg0vw8xgsmqox"},{"post_id":"ckrancn9j000gg0vwliaw3abq","category_id":"ckrancn9d0008g0vwe20xrm6l","_id":"ckrancn9p000vg0vwlchj1lk3"},{"post_id":"ckrancn9a0005g0vwizrjyp4n","category_id":"ckrancn9d0008g0vwe20xrm6l","_id":"ckrancn9s000xg0vwa20a52vc"},{"post_id":"ckrancn9k000ig0vwnlj7hffx","category_id":"ckrancn9d0008g0vwe20xrm6l","_id":"ckrancn9t0010g0vw7jotkqnl"},{"post_id":"ckrancn9m000mg0vwiujvh1ml","category_id":"ckrancn9d0008g0vwe20xrm6l","_id":"ckrancn9u0013g0vwlna7cul1"},{"post_id":"ckrancn9c0006g0vwr4t86q7p","category_id":"ckrancn9d0008g0vwe20xrm6l","_id":"ckrancn9w0017g0vwswg4k1qz"},{"post_id":"ckrancn9n000pg0vwrhf1e5th","category_id":"ckrancn990003g0vwura362bx","_id":"ckrancn9x0019g0vwvg7acuw0"},{"post_id":"ckrancn9p000ug0vwafj50q6v","category_id":"ckrancn9d0008g0vwe20xrm6l","_id":"ckrancn9y001bg0vwbvt555t1"},{"post_id":"ckrancn9c0007g0vwlhdivyyh","category_id":"ckrancn9d0008g0vwe20xrm6l","_id":"ckrancn9z001fg0vwnr9bmpie"},{"post_id":"ckrancn9p000wg0vw1sxmglt5","category_id":"ckrancn9d0008g0vwe20xrm6l","_id":"ckrancna1001jg0vw6x4h459e"},{"post_id":"ckrancn9t000zg0vwtlnykhka","category_id":"ckrancn9d0008g0vwe20xrm6l","_id":"ckrancna2001mg0vwsburcbh0"},{"post_id":"ckrancn9u0012g0vw5ff1wqyh","category_id":"ckrancn9d0008g0vwe20xrm6l","_id":"ckrancna2001ng0vwet26ktjx"},{"post_id":"ckrancn9v0016g0vwzeh0hl1v","category_id":"ckrancn9d0008g0vwe20xrm6l","_id":"ckrancna3001pg0vw7gjupexf"},{"post_id":"ckrancn9x0018g0vw5uzijjsg","category_id":"ckrancn9d0008g0vwe20xrm6l","_id":"ckrancna3001qg0vwwpls73h1"},{"post_id":"ckrancn9z001eg0vw65av20cz","category_id":"ckrancn9d0008g0vwe20xrm6l","_id":"ckrancna3001sg0vw8ubko28m"},{"post_id":"ckrancna0001ig0vwlg1i0v57","category_id":"ckrancn9d0008g0vwe20xrm6l","_id":"ckrancna4001ug0vwhgc6duyi"},{"post_id":"ckrancna1001lg0vwcod2qams","category_id":"ckrancn9d0008g0vwe20xrm6l","_id":"ckrancna4001xg0vwoqbstq5z"},{"post_id":"ckrancn9y001ag0vwa6z7y3xd","category_id":"ckrancna0001gg0vwdpmw7qwd","_id":"ckrancna4001zg0vwov5xxvq5"}],"PostTag":[{"post_id":"ckrancn900000g0vwtw0cs3mu","tag_id":"ckrancn9a0004g0vw1xzcw2ei","_id":"ckrancn9e000ag0vww0m9mg29"},{"post_id":"ckrancn9f000cg0vwz6oud3ca","tag_id":"ckrancn9a0004g0vw1xzcw2ei","_id":"ckrancn9k000hg0vwpdc5ivgw"},{"post_id":"ckrancn970002g0vwyu6y20ry","tag_id":"ckrancn9e0009g0vwugm61uev","_id":"ckrancn9n000og0vw3bzl8445"},{"post_id":"ckrancn970002g0vwyu6y20ry","tag_id":"ckrancn9h000fg0vw1wzwtmw6","_id":"ckrancn9o000sg0vwqozit239"},{"post_id":"ckrancn9a0005g0vwizrjyp4n","tag_id":"ckrancn9e0009g0vwugm61uev","_id":"ckrancn9u0011g0vwj1rc4fwt"},{"post_id":"ckrancn9a0005g0vwizrjyp4n","tag_id":"ckrancn9h000fg0vw1wzwtmw6","_id":"ckrancn9v0014g0vw4vbmfxq4"},{"post_id":"ckrancn9c0006g0vwr4t86q7p","tag_id":"ckrancn9s000yg0vwe58avcy0","_id":"ckrancn9z001dg0vw71tr0455"},{"post_id":"ckrancn9c0006g0vwr4t86q7p","tag_id":"ckrancn9v0015g0vwc4jh8d58","_id":"ckrancna0001hg0vw4ajebnyn"},{"post_id":"ckrancn9c0007g0vwlhdivyyh","tag_id":"ckrancn9y001cg0vwm8o6ew99","_id":"ckrancna4001tg0vwal5gvjs5"},{"post_id":"ckrancn9c0007g0vwlhdivyyh","tag_id":"ckrancna1001kg0vw3x926hn2","_id":"ckrancna4001vg0vwsrfnuj0j"},{"post_id":"ckrancn9c0007g0vwlhdivyyh","tag_id":"ckrancna3001og0vw1umevovn","_id":"ckrancna4001yg0vwy4kjleyr"},{"post_id":"ckrancn9e000bg0vw2nlae14l","tag_id":"ckrancn9y001cg0vwm8o6ew99","_id":"ckrancna50022g0vwt2waranh"},{"post_id":"ckrancn9e000bg0vw2nlae14l","tag_id":"ckrancna3001og0vw1umevovn","_id":"ckrancna50023g0vweig2snem"},{"post_id":"ckrancn9e000bg0vw2nlae14l","tag_id":"ckrancna40020g0vwa4sbn4yr","_id":"ckrancna60025g0vwdcuzevlq"},{"post_id":"ckrancn9j000gg0vwliaw3abq","tag_id":"ckrancn9e0009g0vwugm61uev","_id":"ckrancna60026g0vw60ldi1me"},{"post_id":"ckrancn9j000gg0vwliaw3abq","tag_id":"ckrancna50021g0vw04ronbw1","_id":"ckrancna60028g0vw3xvn7z2y"},{"post_id":"ckrancn9k000ig0vwnlj7hffx","tag_id":"ckrancna50024g0vw0qmea349","_id":"ckrancna6002ag0vwx5rcoyk4"},{"post_id":"ckrancn9k000ig0vwnlj7hffx","tag_id":"ckrancna60027g0vws0zajszi","_id":"ckrancna7002bg0vw18ltlnek"},{"post_id":"ckrancn9m000mg0vwiujvh1ml","tag_id":"ckrancna60029g0vw3dkr7nc8","_id":"ckrancna7002eg0vwxa0ux9oj"},{"post_id":"ckrancn9m000mg0vwiujvh1ml","tag_id":"ckrancna7002cg0vwh4w4rl2j","_id":"ckrancna7002fg0vw4re7wlho"},{"post_id":"ckrancn9p000ug0vwafj50q6v","tag_id":"ckrancna7002dg0vw7eeyom13","_id":"ckrancna8002jg0vwaukvo444"},{"post_id":"ckrancn9p000ug0vwafj50q6v","tag_id":"ckrancna7002gg0vwscpliq6n","_id":"ckrancna9002kg0vw5tjr8myy"},{"post_id":"ckrancn9p000ug0vwafj50q6v","tag_id":"ckrancna8002hg0vwutj6t0gg","_id":"ckrancna9002mg0vwewnmgeky"},{"post_id":"ckrancn9p000wg0vw1sxmglt5","tag_id":"ckrancna8002ig0vwbxp4src5","_id":"ckrancnaa002og0vwldi7ekh0"},{"post_id":"ckrancn9p000wg0vw1sxmglt5","tag_id":"ckrancna9002lg0vw2chq5i8j","_id":"ckrancnaa002pg0vwtm8vgn8i"},{"post_id":"ckrancn9t000zg0vwtlnykhka","tag_id":"ckrancna9002ng0vw779en002","_id":"ckrancnab002tg0vw6uae3uiw"},{"post_id":"ckrancn9t000zg0vwtlnykhka","tag_id":"ckrancnaa002qg0vw5s7fr6lv","_id":"ckrancnab002ug0vwow4whb7s"},{"post_id":"ckrancn9t000zg0vwtlnykhka","tag_id":"ckrancnaa002rg0vwxncjve4x","_id":"ckrancnac002wg0vwuz29c17v"},{"post_id":"ckrancn9u0012g0vw5ff1wqyh","tag_id":"ckrancna8002ig0vwbxp4src5","_id":"ckrancnad002zg0vwx8z975pk"},{"post_id":"ckrancn9u0012g0vw5ff1wqyh","tag_id":"ckrancna9002lg0vw2chq5i8j","_id":"ckrancnad0030g0vwrljnhojs"},{"post_id":"ckrancn9u0012g0vw5ff1wqyh","tag_id":"ckrancnac002xg0vwm9xk43nv","_id":"ckrancnae0032g0vwd8c7c2zm"},{"post_id":"ckrancn9v0016g0vwzeh0hl1v","tag_id":"ckrancna8002ig0vwbxp4src5","_id":"ckrancnae0034g0vwmrvbpuj5"},{"post_id":"ckrancn9v0016g0vwzeh0hl1v","tag_id":"ckrancnac002xg0vwm9xk43nv","_id":"ckrancnaf0035g0vwk03tkko9"},{"post_id":"ckrancn9x0018g0vw5uzijjsg","tag_id":"ckrancnae0033g0vw7bjcd84x","_id":"ckrancnaf0037g0vw78h21fag"},{"post_id":"ckrancn9y001ag0vwa6z7y3xd","tag_id":"ckrancnaf0036g0vw0ho85vfu","_id":"ckrancnaf0039g0vwhr50nokq"},{"post_id":"ckrancn9z001eg0vw65av20cz","tag_id":"ckrancnaf0038g0vw6ay9ttca","_id":"ckrancnai003eg0vwn3wh4rh3"},{"post_id":"ckrancn9z001eg0vw65av20cz","tag_id":"ckrancnag003ag0vwy0keps64","_id":"ckrancnai003fg0vwv56d4sef"},{"post_id":"ckrancn9z001eg0vw65av20cz","tag_id":"ckrancnag003bg0vwuql42p39","_id":"ckrancnai003hg0vwrm12c17k"},{"post_id":"ckrancn9z001eg0vw65av20cz","tag_id":"ckrancnah003cg0vwx6aa0cpw","_id":"ckrancnaj003ig0vwsoyxjukn"},{"post_id":"ckrancna0001ig0vwlg1i0v57","tag_id":"ckrancnag003bg0vwuql42p39","_id":"ckrancnal003ng0vw7i7y5qc6"},{"post_id":"ckrancna0001ig0vwlg1i0v57","tag_id":"ckrancnah003cg0vwx6aa0cpw","_id":"ckrancnal003og0vw67cbyvqz"},{"post_id":"ckrancna0001ig0vwlg1i0v57","tag_id":"ckrancna9002lg0vw2chq5i8j","_id":"ckrancnam003pg0vwv1jjqogv"},{"post_id":"ckrancna0001ig0vwlg1i0v57","tag_id":"ckrancnak003kg0vw98ojn22v","_id":"ckrancnam003qg0vwim9bsdmj"},{"post_id":"ckrancna0001ig0vwlg1i0v57","tag_id":"ckrancnak003lg0vwa1ebnzo1","_id":"ckrancnam003rg0vwier4mj4q"},{"post_id":"ckrancna1001lg0vwcod2qams","tag_id":"ckrancn9e0009g0vwugm61uev","_id":"ckrancnam003sg0vwcw4ro5c9"},{"post_id":"ckrancna1001lg0vwcod2qams","tag_id":"ckrancnal003mg0vw7bipaegw","_id":"ckrancnam003tg0vwtzdocvyi"}],"Tag":[{"name":"Tips","_id":"ckrancn9a0004g0vw1xzcw2ei"},{"name":"Deep learning","_id":"ckrancn9e0009g0vwugm61uev"},{"name":"GAN","_id":"ckrancn9h000fg0vw1wzwtmw6"},{"name":"GD","_id":"ckrancn9s000yg0vwe58avcy0"},{"name":"Momentum","_id":"ckrancn9v0015g0vwc4jh8d58"},{"name":"Kernel","_id":"ckrancn9y001cg0vwm8o6ew99"},{"name":"SVM","_id":"ckrancna1001kg0vw3x926hn2"},{"name":"RKHS","_id":"ckrancna3001og0vw1umevovn"},{"name":"Functional analysis","_id":"ckrancna40020g0vwa4sbn4yr"},{"name":"Loss function","_id":"ckrancna50021g0vw04ronbw1"},{"name":"PCA","_id":"ckrancna50024g0vw0qmea349"},{"name":"LFM","_id":"ckrancna60027g0vws0zajszi"},{"name":"Discriminative model","_id":"ckrancna60029g0vw3dkr7nc8"},{"name":"Generative model","_id":"ckrancna7002cg0vwh4w4rl2j"},{"name":"Ensemble","_id":"ckrancna7002dg0vw7eeyom13"},{"name":"Boost","_id":"ckrancna7002gg0vwscpliq6n"},{"name":"GBDT/GBRT","_id":"ckrancna8002hg0vwutj6t0gg"},{"name":"Norm regularization","_id":"ckrancna8002ig0vwbxp4src5"},{"name":"Convex optimization","_id":"ckrancna9002lg0vw2chq5i8j"},{"name":"K-means","_id":"ckrancna9002ng0vw779en002"},{"name":"GMM","_id":"ckrancnaa002qg0vw5s7fr6lv"},{"name":"EM Algorithm","_id":"ckrancnaa002rg0vwxncjve4x"},{"name":"Matrix theory","_id":"ckrancnac002xg0vwm9xk43nv"},{"name":"Rating","_id":"ckrancnae0033g0vw7bjcd84x"},{"name":"Statistics","_id":"ckrancnaf0036g0vw0ho85vfu"},{"name":"Generalized Linear Model","_id":"ckrancnaf0038g0vw6ay9ttca"},{"name":"Exponential family distribution","_id":"ckrancnag003ag0vwy0keps64"},{"name":"Logistic","_id":"ckrancnag003bg0vwuql42p39"},{"name":"Softmax","_id":"ckrancnah003cg0vwx6aa0cpw"},{"name":"Gradient","_id":"ckrancnak003kg0vw98ojn22v"},{"name":"Subgradient","_id":"ckrancnak003lg0vwa1ebnzo1"},{"name":"Activation function","_id":"ckrancnal003mg0vw7bipaegw"}]}}